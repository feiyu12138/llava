/home/jchen293/.conda/envs/llava_git/bin/python
[2024-03-23 16:50:55,536] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-23 16:51:00,130] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7: setting --include=localhost:0,1,2,3,4,5,6,7
[2024-03-23 16:51:00,130] [INFO] [runner.py:571:main] cmd = /home/jchen293/.conda/envs/llava_git/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --deepspeed ./scripts/zero2.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version plain --data_path /datasets/jchen293/data/llava_datasets/LLaVA-Pretrain/blip_laion_cc_sbu_558k.json --image_folder /datasets/jchen293/data/llava_datasets/LLaVA-Pretrain/images --vision_tower openai/clip-vit-large-patch14-336 --mm_projector_type mlp2x_gelu --tune_mm_mlp_adapter True --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --bf16 True --output_dir /datasets/jchen293/weights/llava/checkpoint/llava-v1.5-7b-pretrain-stride-16-layer-16-grouping-avgpool1d --num_train_epochs 1 --per_device_train_batch_size 32 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 24000 --save_total_limit 1 --learning_rate 1e-3 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb --stride 16 --layer 16 --grouping avgpool1d
[2024-03-23 16:51:02,645] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-23 16:51:04,136] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-03-23 16:51:04,136] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-03-23 16:51:04,136] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-03-23 16:51:04,136] [INFO] [launch.py:163:main] dist_world_size=8
[2024-03-23 16:51:04,136] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2024-03-23 16:51:19,870] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-23 16:51:19,885] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-23 16:51:19,893] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-23 16:51:19,902] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-23 16:51:19,917] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-23 16:51:19,919] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-23 16:51:19,959] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-23 16:51:19,970] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-23 16:51:25,293] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-23 16:51:25,295] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-23 16:51:25,297] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-23 16:51:25,297] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-03-23 16:51:25,297] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-23 16:51:25,298] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-23 16:51:25,298] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
[2024-03-23 16:51:30,638] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-23 16:51:31,332] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:35<00:35, 35.16s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:35<00:35, 36.00s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:35<00:35, 35.89s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:47<00:00, 21.79s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:47<00:00, 23.80s/it]
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
Loading checkpoint shards:  50%|█████     | 1/2 [00:35<00:35, 35.95s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:35<00:35, 35.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:47<00:00, 21.67s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:47<00:00, 23.82s/it]
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
Loading checkpoint shards:  50%|█████     | 1/2 [00:34<00:34, 34.96s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:47<00:00, 21.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:47<00:00, 23.55s/it]
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
Loading checkpoint shards:  50%|█████     | 1/2 [00:34<00:34, 34.52s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:34<00:34, 34.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:46<00:00, 21.25s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:46<00:00, 23.45s/it]
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
Loading checkpoint shards: 100%|██████████| 2/2 [00:45<00:00, 20.68s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:45<00:00, 22.84s/it]
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
Loading checkpoint shards: 100%|██████████| 2/2 [00:45<00:00, 20.57s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:45<00:00, 22.74s/it]
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
Loading checkpoint shards: 100%|██████████| 2/2 [00:45<00:00, 20.48s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:45<00:00, 22.59s/it]
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 20.21s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.34s/it]
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Formatting inputs...Skip in lazy mode
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
wandb: Currently logged in as: jienengchen01. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /home/jchen293/code/llava_git/llava/wandb/run-20240323_165333-7qz44ogc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run grateful-star-74
wandb: ⭐️ View project at https://wandb.ai/jienengchen01/huggingface
wandb: 🚀 View run at https://wandb.ai/jienengchen01/huggingface/runs/7qz44ogc
  0%|          | 0/2181 [00:00<?, ?it/s]/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
  0%|          | 1/2181 [00:47<28:46:06, 47.51s/it]                                                   {'loss': 7.7652, 'learning_rate': 1.5151515151515153e-05, 'epoch': 0.0}
  0%|          | 1/2181 [00:47<28:46:06, 47.51s/it]  0%|          | 2/2181 [00:54<14:25:51, 23.84s/it]                                                   {'loss': 7.6467, 'learning_rate': 3.0303030303030306e-05, 'epoch': 0.0}
  0%|          | 2/2181 [00:54<14:25:51, 23.84s/it]  0%|          | 3/2181 [01:01<9:44:20, 16.10s/it]                                                   {'loss': 7.1693, 'learning_rate': 4.545454545454546e-05, 'epoch': 0.0}
  0%|          | 3/2181 [01:01<9:44:20, 16.10s/it]  0%|          | 4/2181 [01:08<7:35:26, 12.55s/it]                                                  {'loss': 5.9196, 'learning_rate': 6.060606060606061e-05, 'epoch': 0.0}
  0%|          | 4/2181 [01:08<7:35:26, 12.55s/it]  0%|          | 5/2181 [01:15<6:21:01, 10.51s/it]                                                  {'loss': 5.437, 'learning_rate': 7.575757575757576e-05, 'epoch': 0.0}
  0%|          | 5/2181 [01:15<6:21:01, 10.51s/it]  0%|          | 6/2181 [01:22<5:33:11,  9.19s/it]                                                  {'loss': 5.2919, 'learning_rate': 9.090909090909092e-05, 'epoch': 0.0}
  0%|          | 6/2181 [01:22<5:33:11,  9.19s/it]  0%|          | 7/2181 [01:29<5:05:56,  8.44s/it]                                                  {'loss': 5.0744, 'learning_rate': 0.00010606060606060606, 'epoch': 0.0}
  0%|          | 7/2181 [01:29<5:05:56,  8.44s/it]  0%|          | 8/2181 [01:36<4:48:39,  7.97s/it]                                                  {'loss': 4.7853, 'learning_rate': 0.00012121212121212122, 'epoch': 0.0}
  0%|          | 8/2181 [01:36<4:48:39,  7.97s/it]  0%|          | 9/2181 [01:42<4:34:14,  7.58s/it]                                                  {'loss': 4.6404, 'learning_rate': 0.00013636363636363637, 'epoch': 0.0}
  0%|          | 9/2181 [01:42<4:34:14,  7.58s/it]  0%|          | 10/2181 [01:49<4:22:18,  7.25s/it]                                                   {'loss': 4.4591, 'learning_rate': 0.00015151515151515152, 'epoch': 0.0}
  0%|          | 10/2181 [01:49<4:22:18,  7.25s/it]  1%|          | 11/2181 [01:56<4:17:10,  7.11s/it]                                                   {'loss': 4.3353, 'learning_rate': 0.00016666666666666666, 'epoch': 0.01}
  1%|          | 11/2181 [01:56<4:17:10,  7.11s/it]  1%|          | 12/2181 [02:02<4:11:13,  6.95s/it]                                                   {'loss': 4.2102, 'learning_rate': 0.00018181818181818183, 'epoch': 0.01}
  1%|          | 12/2181 [02:02<4:11:13,  6.95s/it]  1%|          | 13/2181 [02:09<4:06:34,  6.82s/it]                                                   {'loss': 4.1774, 'learning_rate': 0.00019696969696969695, 'epoch': 0.01}
  1%|          | 13/2181 [02:09<4:06:34,  6.82s/it]  1%|          | 14/2181 [02:15<4:04:14,  6.76s/it]                                                   {'loss': 4.067, 'learning_rate': 0.00021212121212121213, 'epoch': 0.01}
  1%|          | 14/2181 [02:15<4:04:14,  6.76s/it]  1%|          | 15/2181 [02:22<4:05:08,  6.79s/it]                                                   {'loss': 4.0505, 'learning_rate': 0.00022727272727272727, 'epoch': 0.01}
  1%|          | 15/2181 [02:22<4:05:08,  6.79s/it]  1%|          | 16/2181 [02:29<4:06:53,  6.84s/it]                                                   {'loss': 3.9037, 'learning_rate': 0.00024242424242424245, 'epoch': 0.01}
  1%|          | 16/2181 [02:29<4:06:53,  6.84s/it]  1%|          | 17/2181 [02:36<4:03:54,  6.76s/it]                                                   {'loss': 4.0184, 'learning_rate': 0.00025757575757575756, 'epoch': 0.01}
  1%|          | 17/2181 [02:36<4:03:54,  6.76s/it]  1%|          | 18/2181 [02:42<4:01:12,  6.69s/it]                                                   {'loss': 3.9258, 'learning_rate': 0.00027272727272727274, 'epoch': 0.01}
  1%|          | 18/2181 [02:42<4:01:12,  6.69s/it]  1%|          | 19/2181 [02:49<3:59:33,  6.65s/it]                                                   {'loss': 3.8886, 'learning_rate': 0.0002878787878787879, 'epoch': 0.01}
  1%|          | 19/2181 [02:49<3:59:33,  6.65s/it]  1%|          | 20/2181 [02:56<4:00:25,  6.68s/it]                                                   {'loss': 3.6994, 'learning_rate': 0.00030303030303030303, 'epoch': 0.01}
  1%|          | 20/2181 [02:56<4:00:25,  6.68s/it]  1%|          | 21/2181 [03:02<3:59:22,  6.65s/it]                                                   {'loss': 3.7655, 'learning_rate': 0.0003181818181818182, 'epoch': 0.01}
  1%|          | 21/2181 [03:02<3:59:22,  6.65s/it]  1%|          | 22/2181 [03:10<4:13:02,  7.03s/it]                                                   {'loss': 3.6058, 'learning_rate': 0.0003333333333333333, 'epoch': 0.01}
  1%|          | 22/2181 [03:10<4:13:02,  7.03s/it]  1%|          | 23/2181 [03:17<4:10:32,  6.97s/it]                                                   {'loss': 3.5283, 'learning_rate': 0.0003484848484848485, 'epoch': 0.01}
  1%|          | 23/2181 [03:17<4:10:32,  6.97s/it]  1%|          | 24/2181 [03:23<4:05:26,  6.83s/it]                                                   {'loss': 3.6975, 'learning_rate': 0.00036363636363636367, 'epoch': 0.01}
  1%|          | 24/2181 [03:24<4:05:26,  6.83s/it]  1%|          | 25/2181 [03:30<4:03:04,  6.76s/it]                                                   {'loss': 3.5786, 'learning_rate': 0.0003787878787878788, 'epoch': 0.01}
  1%|          | 25/2181 [03:30<4:03:04,  6.76s/it]  1%|          | 26/2181 [03:37<3:59:41,  6.67s/it]                                                   {'loss': 3.4395, 'learning_rate': 0.0003939393939393939, 'epoch': 0.01}
  1%|          | 26/2181 [03:37<3:59:41,  6.67s/it]  1%|          | 27/2181 [03:43<3:59:07,  6.66s/it]                                                   {'loss': 3.5981, 'learning_rate': 0.00040909090909090913, 'epoch': 0.01}
  1%|          | 27/2181 [03:43<3:59:07,  6.66s/it]  1%|▏         | 28/2181 [03:50<3:57:17,  6.61s/it]                                                   {'loss': 3.5372, 'learning_rate': 0.00042424242424242425, 'epoch': 0.01}
  1%|▏         | 28/2181 [03:50<3:57:17,  6.61s/it]  1%|▏         | 29/2181 [03:56<3:58:11,  6.64s/it]                                                   {'loss': 3.4273, 'learning_rate': 0.0004393939393939394, 'epoch': 0.01}
  1%|▏         | 29/2181 [03:56<3:58:11,  6.64s/it]  1%|▏         | 30/2181 [04:03<3:58:06,  6.64s/it]                                                   {'loss': 3.5371, 'learning_rate': 0.00045454545454545455, 'epoch': 0.01}
  1%|▏         | 30/2181 [04:03<3:58:06,  6.64s/it]  1%|▏         | 31/2181 [04:10<3:58:47,  6.66s/it]                                                   {'loss': 3.4373, 'learning_rate': 0.0004696969696969697, 'epoch': 0.01}
  1%|▏         | 31/2181 [04:10<3:58:47,  6.66s/it]  1%|▏         | 32/2181 [04:16<3:57:06,  6.62s/it]                                                   {'loss': 3.4249, 'learning_rate': 0.0004848484848484849, 'epoch': 0.01}
  1%|▏         | 32/2181 [04:16<3:57:06,  6.62s/it]  2%|▏         | 33/2181 [04:23<3:55:22,  6.57s/it]                                                   {'loss': 3.3612, 'learning_rate': 0.0005, 'epoch': 0.02}
  2%|▏         | 33/2181 [04:23<3:55:22,  6.57s/it]  2%|▏         | 34/2181 [04:29<3:54:26,  6.55s/it]                                                   {'loss': 3.4405, 'learning_rate': 0.0005151515151515151, 'epoch': 0.02}
  2%|▏         | 34/2181 [04:29<3:54:26,  6.55s/it]  2%|▏         | 35/2181 [04:36<3:53:35,  6.53s/it]                                                   {'loss': 3.3616, 'learning_rate': 0.0005303030303030302, 'epoch': 0.02}
  2%|▏         | 35/2181 [04:36<3:53:35,  6.53s/it]  2%|▏         | 36/2181 [04:42<3:53:51,  6.54s/it]                                                   {'loss': 3.3407, 'learning_rate': 0.0005454545454545455, 'epoch': 0.02}
  2%|▏         | 36/2181 [04:42<3:53:51,  6.54s/it]  2%|▏         | 37/2181 [04:49<3:56:25,  6.62s/it]                                                   {'loss': 3.4144, 'learning_rate': 0.0005606060606060606, 'epoch': 0.02}
  2%|▏         | 37/2181 [04:49<3:56:25,  6.62s/it]  2%|▏         | 38/2181 [04:56<3:57:36,  6.65s/it]                                                   {'loss': 3.441, 'learning_rate': 0.0005757575757575758, 'epoch': 0.02}
  2%|▏         | 38/2181 [04:56<3:57:36,  6.65s/it]  2%|▏         | 39/2181 [05:02<3:55:49,  6.61s/it]                                                   {'loss': 3.4771, 'learning_rate': 0.0005909090909090909, 'epoch': 0.02}
  2%|▏         | 39/2181 [05:02<3:55:49,  6.61s/it]  2%|▏         | 40/2181 [05:09<3:55:05,  6.59s/it]                                                   {'loss': 3.4283, 'learning_rate': 0.0006060606060606061, 'epoch': 0.02}
  2%|▏         | 40/2181 [05:09<3:55:05,  6.59s/it]  2%|▏         | 41/2181 [05:15<3:54:04,  6.56s/it]                                                   {'loss': 3.2827, 'learning_rate': 0.0006212121212121212, 'epoch': 0.02}
  2%|▏         | 41/2181 [05:15<3:54:04,  6.56s/it]  2%|▏         | 42/2181 [05:22<3:53:22,  6.55s/it]                                                   {'loss': 3.3954, 'learning_rate': 0.0006363636363636364, 'epoch': 0.02}
  2%|▏         | 42/2181 [05:22<3:53:22,  6.55s/it]  2%|▏         | 43/2181 [05:28<3:52:37,  6.53s/it]                                                   {'loss': 3.3582, 'learning_rate': 0.0006515151515151515, 'epoch': 0.02}
  2%|▏         | 43/2181 [05:28<3:52:37,  6.53s/it]  2%|▏         | 44/2181 [05:35<3:54:22,  6.58s/it]                                                   {'loss': 3.2681, 'learning_rate': 0.0006666666666666666, 'epoch': 0.02}
  2%|▏         | 44/2181 [05:35<3:54:22,  6.58s/it]  2%|▏         | 45/2181 [05:42<3:54:41,  6.59s/it]                                                   {'loss': 3.1908, 'learning_rate': 0.0006818181818181818, 'epoch': 0.02}
  2%|▏         | 45/2181 [05:42<3:54:41,  6.59s/it]  2%|▏         | 46/2181 [05:48<3:56:32,  6.65s/it]                                                   {'loss': 3.3151, 'learning_rate': 0.000696969696969697, 'epoch': 0.02}
  2%|▏         | 46/2181 [05:48<3:56:32,  6.65s/it]  2%|▏         | 47/2181 [05:55<3:54:46,  6.60s/it]                                                   {'loss': 3.3529, 'learning_rate': 0.0007121212121212122, 'epoch': 0.02}
  2%|▏         | 47/2181 [05:55<3:54:46,  6.60s/it]  2%|▏         | 48/2181 [06:01<3:53:04,  6.56s/it]                                                   {'loss': 3.3107, 'learning_rate': 0.0007272727272727273, 'epoch': 0.02}
  2%|▏         | 48/2181 [06:01<3:53:04,  6.56s/it]  2%|▏         | 49/2181 [06:08<3:52:41,  6.55s/it]                                                   {'loss': 3.3106, 'learning_rate': 0.0007424242424242425, 'epoch': 0.02}
  2%|▏         | 49/2181 [06:08<3:52:41,  6.55s/it]  2%|▏         | 50/2181 [06:14<3:51:19,  6.51s/it]                                                   {'loss': 3.3661, 'learning_rate': 0.0007575757575757576, 'epoch': 0.02}
  2%|▏         | 50/2181 [06:14<3:51:19,  6.51s/it]  2%|▏         | 51/2181 [06:21<3:51:03,  6.51s/it]                                                   {'loss': 3.2742, 'learning_rate': 0.0007727272727272727, 'epoch': 0.02}
  2%|▏         | 51/2181 [06:21<3:51:03,  6.51s/it]  2%|▏         | 52/2181 [06:28<3:53:54,  6.59s/it]                                                   {'loss': 3.3087, 'learning_rate': 0.0007878787878787878, 'epoch': 0.02}
  2%|▏         | 52/2181 [06:28<3:53:54,  6.59s/it]  2%|▏         | 53/2181 [06:34<3:55:39,  6.64s/it]                                                   {'loss': 3.2488, 'learning_rate': 0.000803030303030303, 'epoch': 0.02}
  2%|▏         | 53/2181 [06:35<3:55:39,  6.64s/it]  2%|▏         | 54/2181 [06:41<3:55:29,  6.64s/it]                                                   {'loss': 3.3037, 'learning_rate': 0.0008181818181818183, 'epoch': 0.02}
  2%|▏         | 54/2181 [06:41<3:55:29,  6.64s/it]  3%|▎         | 55/2181 [06:48<3:53:35,  6.59s/it]                                                   {'loss': 3.3086, 'learning_rate': 0.0008333333333333334, 'epoch': 0.03}
  3%|▎         | 55/2181 [06:48<3:53:35,  6.59s/it]  3%|▎         | 56/2181 [06:54<3:52:32,  6.57s/it]                                                   {'loss': 3.3724, 'learning_rate': 0.0008484848484848485, 'epoch': 0.03}
  3%|▎         | 56/2181 [06:54<3:52:32,  6.57s/it]  3%|▎         | 57/2181 [07:01<3:56:29,  6.68s/it]                                                   {'loss': 3.2469, 'learning_rate': 0.0008636363636363636, 'epoch': 0.03}
  3%|▎         | 57/2181 [07:01<3:56:29,  6.68s/it]  3%|▎         | 58/2181 [07:07<3:54:09,  6.62s/it]                                                   {'loss': 3.29, 'learning_rate': 0.0008787878787878789, 'epoch': 0.03}
  3%|▎         | 58/2181 [07:07<3:54:09,  6.62s/it]  3%|▎         | 59/2181 [07:14<3:53:26,  6.60s/it]                                                   {'loss': 3.2053, 'learning_rate': 0.000893939393939394, 'epoch': 0.03}
  3%|▎         | 59/2181 [07:14<3:53:26,  6.60s/it]  3%|▎         | 60/2181 [07:21<3:57:41,  6.72s/it]                                                   {'loss': 3.2662, 'learning_rate': 0.0009090909090909091, 'epoch': 0.03}
  3%|▎         | 60/2181 [07:21<3:57:41,  6.72s/it]  3%|▎         | 61/2181 [07:28<3:57:06,  6.71s/it]                                                   {'loss': 3.2198, 'learning_rate': 0.0009242424242424242, 'epoch': 0.03}
  3%|▎         | 61/2181 [07:28<3:57:06,  6.71s/it]  3%|▎         | 62/2181 [07:34<3:55:39,  6.67s/it]                                                   {'loss': 3.1872, 'learning_rate': 0.0009393939393939394, 'epoch': 0.03}
  3%|▎         | 62/2181 [07:34<3:55:39,  6.67s/it]  3%|▎         | 63/2181 [07:41<3:54:09,  6.63s/it]                                                   {'loss': 3.2232, 'learning_rate': 0.0009545454545454546, 'epoch': 0.03}
  3%|▎         | 63/2181 [07:41<3:54:09,  6.63s/it]  3%|▎         | 64/2181 [07:47<3:53:10,  6.61s/it]                                                   {'loss': 3.1379, 'learning_rate': 0.0009696969696969698, 'epoch': 0.03}
  3%|▎         | 64/2181 [07:47<3:53:10,  6.61s/it]  3%|▎         | 65/2181 [07:54<3:51:39,  6.57s/it]                                                   {'loss': 3.1622, 'learning_rate': 0.000984848484848485, 'epoch': 0.03}
  3%|▎         | 65/2181 [07:54<3:51:39,  6.57s/it]  3%|▎         | 66/2181 [08:00<3:51:39,  6.57s/it]                                                   {'loss': 3.0859, 'learning_rate': 0.001, 'epoch': 0.03}
  3%|▎         | 66/2181 [08:01<3:51:39,  6.57s/it]  3%|▎         | 67/2181 [08:07<3:54:07,  6.65s/it]                                                   {'loss': 3.0975, 'learning_rate': 0.0009999994484067654, 'epoch': 0.03}
  3%|▎         | 67/2181 [08:07<3:54:07,  6.65s/it]  3%|▎         | 68/2181 [08:14<3:54:24,  6.66s/it]                                                   {'loss': 3.1139, 'learning_rate': 0.0009999977936282788, 'epoch': 0.03}
  3%|▎         | 68/2181 [08:14<3:54:24,  6.66s/it]  3%|▎         | 69/2181 [08:20<3:52:20,  6.60s/it]                                                   {'loss': 3.0661, 'learning_rate': 0.0009999950356681913, 'epoch': 0.03}
  3%|▎         | 69/2181 [08:20<3:52:20,  6.60s/it]  3%|▎         | 70/2181 [08:27<3:52:33,  6.61s/it]                                                   {'loss': 3.1291, 'learning_rate': 0.0009999911745325876, 'epoch': 0.03}
  3%|▎         | 70/2181 [08:27<3:52:33,  6.61s/it]  3%|▎         | 71/2181 [08:34<3:51:15,  6.58s/it]                                                   {'loss': 3.0192, 'learning_rate': 0.0009999862102299873, 'epoch': 0.03}
  3%|▎         | 71/2181 [08:34<3:51:15,  6.58s/it]  3%|▎         | 72/2181 [08:40<3:50:10,  6.55s/it]                                                   {'loss': 3.0821, 'learning_rate': 0.0009999801427713433, 'epoch': 0.03}
  3%|▎         | 72/2181 [08:40<3:50:10,  6.55s/it]  3%|▎         | 73/2181 [08:47<3:49:54,  6.54s/it]                                                   {'loss': 2.957, 'learning_rate': 0.0009999729721700424, 'epoch': 0.03}
  3%|▎         | 73/2181 [08:47<3:49:54,  6.54s/it]  3%|▎         | 74/2181 [08:53<3:51:51,  6.60s/it]                                                   {'loss': 3.0929, 'learning_rate': 0.000999964698441906, 'epoch': 0.03}
  3%|▎         | 74/2181 [08:53<3:51:51,  6.60s/it]  3%|▎         | 75/2181 [09:00<3:53:44,  6.66s/it]                                                   {'loss': 2.9841, 'learning_rate': 0.0009999553216051892, 'epoch': 0.03}
  3%|▎         | 75/2181 [09:00<3:53:44,  6.66s/it]  3%|▎         | 76/2181 [09:07<3:53:00,  6.64s/it]                                                   {'loss': 3.0211, 'learning_rate': 0.00099994484168058, 'epoch': 0.03}
  3%|▎         | 76/2181 [09:07<3:53:00,  6.64s/it]  4%|▎         | 77/2181 [09:13<3:51:35,  6.60s/it]                                                   {'loss': 3.09, 'learning_rate': 0.0009999332586912019, 'epoch': 0.04}
  4%|▎         | 77/2181 [09:13<3:51:35,  6.60s/it]  4%|▎         | 78/2181 [09:20<3:50:08,  6.57s/it]                                                   {'loss': 2.8828, 'learning_rate': 0.0009999205726626108, 'epoch': 0.04}
  4%|▎         | 78/2181 [09:20<3:50:08,  6.57s/it]  4%|▎         | 79/2181 [09:26<3:52:10,  6.63s/it]                                                   {'loss': 2.8994, 'learning_rate': 0.000999906783622797, 'epoch': 0.04}
  4%|▎         | 79/2181 [09:27<3:52:10,  6.63s/it]  4%|▎         | 80/2181 [09:33<3:50:00,  6.57s/it]                                                   {'loss': 3.0165, 'learning_rate': 0.0009998918916021842, 'epoch': 0.04}
  4%|▎         | 80/2181 [09:33<3:50:00,  6.57s/it]  4%|▎         | 81/2181 [09:40<3:50:58,  6.60s/it]                                                   {'loss': 3.0527, 'learning_rate': 0.0009998758966336297, 'epoch': 0.04}
  4%|▎         | 81/2181 [09:40<3:50:58,  6.60s/it]  4%|▍         | 82/2181 [09:46<3:52:43,  6.65s/it]                                                   {'loss': 2.9931, 'learning_rate': 0.0009998587987524242, 'epoch': 0.04}
  4%|▍         | 82/2181 [09:46<3:52:43,  6.65s/it]  4%|▍         | 83/2181 [09:53<3:54:06,  6.70s/it]                                                   {'loss': 2.9057, 'learning_rate': 0.0009998405979962926, 'epoch': 0.04}
  4%|▍         | 83/2181 [09:53<3:54:06,  6.70s/it]  4%|▍         | 84/2181 [10:00<3:51:51,  6.63s/it]                                                   {'loss': 2.8299, 'learning_rate': 0.000999821294405392, 'epoch': 0.04}
  4%|▍         | 84/2181 [10:00<3:51:51,  6.63s/it]  4%|▍         | 85/2181 [10:06<3:50:21,  6.59s/it]                                                   {'loss': 2.974, 'learning_rate': 0.0009998008880223134, 'epoch': 0.04}
  4%|▍         | 85/2181 [10:06<3:50:21,  6.59s/it]  4%|▍         | 86/2181 [10:13<3:49:21,  6.57s/it]                                                   {'loss': 2.8082, 'learning_rate': 0.000999779378892081, 'epoch': 0.04}
  4%|▍         | 86/2181 [10:13<3:49:21,  6.57s/it]  4%|▍         | 87/2181 [10:19<3:49:14,  6.57s/it]                                                   {'loss': 2.9065, 'learning_rate': 0.0009997567670621522, 'epoch': 0.04}
  4%|▍         | 87/2181 [10:19<3:49:14,  6.57s/it]  4%|▍         | 88/2181 [10:26<3:49:38,  6.58s/it]                                                   {'loss': 2.8319, 'learning_rate': 0.0009997330525824165, 'epoch': 0.04}
  4%|▍         | 88/2181 [10:26<3:49:38,  6.58s/it]  4%|▍         | 89/2181 [10:32<3:50:23,  6.61s/it]                                                   {'loss': 2.8639, 'learning_rate': 0.0009997082355051976, 'epoch': 0.04}
  4%|▍         | 89/2181 [10:33<3:50:23,  6.61s/it]  4%|▍         | 90/2181 [10:39<3:51:57,  6.66s/it]                                                   {'loss': 2.8811, 'learning_rate': 0.000999682315885251, 'epoch': 0.04}
  4%|▍         | 90/2181 [10:39<3:51:57,  6.66s/it]  4%|▍         | 91/2181 [10:46<3:53:01,  6.69s/it]                                                   {'loss': 2.8247, 'learning_rate': 0.0009996552937797645, 'epoch': 0.04}
  4%|▍         | 91/2181 [10:46<3:53:01,  6.69s/it]  4%|▍         | 92/2181 [10:53<3:51:28,  6.65s/it]                                                   {'loss': 2.8139, 'learning_rate': 0.0009996271692483596, 'epoch': 0.04}
  4%|▍         | 92/2181 [10:53<3:51:28,  6.65s/it]  4%|▍         | 93/2181 [10:59<3:49:43,  6.60s/it]                                                   {'loss': 2.7841, 'learning_rate': 0.0009995979423530893, 'epoch': 0.04}
  4%|▍         | 93/2181 [10:59<3:49:43,  6.60s/it]  4%|▍         | 94/2181 [11:06<3:48:55,  6.58s/it]                                                   {'loss': 2.7684, 'learning_rate': 0.000999567613158439, 'epoch': 0.04}
  4%|▍         | 94/2181 [11:06<3:48:55,  6.58s/it]  4%|▍         | 95/2181 [11:12<3:48:25,  6.57s/it]                                                   {'loss': 2.7775, 'learning_rate': 0.0009995361817313263, 'epoch': 0.04}
  4%|▍         | 95/2181 [11:12<3:48:25,  6.57s/it]  4%|▍         | 96/2181 [11:19<3:48:16,  6.57s/it]                                                   {'loss': 2.7661, 'learning_rate': 0.0009995036481411004, 'epoch': 0.04}
  4%|▍         | 96/2181 [11:19<3:48:16,  6.57s/it]  4%|▍         | 97/2181 [11:25<3:49:45,  6.61s/it]                                                   {'loss': 2.9121, 'learning_rate': 0.0009994700124595429, 'epoch': 0.04}
  4%|▍         | 97/2181 [11:25<3:49:45,  6.61s/it]  4%|▍         | 98/2181 [11:32<3:51:31,  6.67s/it]                                                   {'loss': 2.7467, 'learning_rate': 0.0009994352747608663, 'epoch': 0.04}
  4%|▍         | 98/2181 [11:32<3:51:31,  6.67s/it]  5%|▍         | 99/2181 [11:39<3:49:47,  6.62s/it]                                                   {'loss': 2.8463, 'learning_rate': 0.0009993994351217151, 'epoch': 0.05}
  5%|▍         | 99/2181 [11:39<3:49:47,  6.62s/it]  5%|▍         | 100/2181 [11:45<3:48:17,  6.58s/it]                                                    {'loss': 2.7755, 'learning_rate': 0.000999362493621165, 'epoch': 0.05}
  5%|▍         | 100/2181 [11:45<3:48:17,  6.58s/it]  5%|▍         | 101/2181 [11:52<3:47:53,  6.57s/it]                                                    {'loss': 2.856, 'learning_rate': 0.0009993244503407226, 'epoch': 0.05}
  5%|▍         | 101/2181 [11:52<3:47:53,  6.57s/it]  5%|▍         | 102/2181 [11:58<3:46:11,  6.53s/it]                                                    {'loss': 2.7556, 'learning_rate': 0.0009992853053643258, 'epoch': 0.05}
  5%|▍         | 102/2181 [11:58<3:46:11,  6.53s/it]  5%|▍         | 103/2181 [12:05<3:45:55,  6.52s/it]                                                    {'loss': 2.7678, 'learning_rate': 0.0009992450587783426, 'epoch': 0.05}
  5%|▍         | 103/2181 [12:05<3:45:55,  6.52s/it]  5%|▍         | 104/2181 [12:12<3:49:43,  6.64s/it]                                                    {'loss': 2.6668, 'learning_rate': 0.000999203710671572, 'epoch': 0.05}
  5%|▍         | 104/2181 [12:12<3:49:43,  6.64s/it]  5%|▍         | 105/2181 [12:18<3:50:03,  6.65s/it]                                                    {'loss': 2.7212, 'learning_rate': 0.0009991612611352438, 'epoch': 0.05}
  5%|▍         | 105/2181 [12:18<3:50:03,  6.65s/it]  5%|▍         | 106/2181 [12:25<3:49:58,  6.65s/it]                                                    {'loss': 2.7229, 'learning_rate': 0.0009991177102630173, 'epoch': 0.05}
  5%|▍         | 106/2181 [12:25<3:49:58,  6.65s/it]  5%|▍         | 107/2181 [12:31<3:48:14,  6.60s/it]                                                    {'loss': 2.6978, 'learning_rate': 0.0009990730581509817, 'epoch': 0.05}
  5%|▍         | 107/2181 [12:31<3:48:14,  6.60s/it]  5%|▍         | 108/2181 [12:38<3:47:05,  6.57s/it]                                                    {'loss': 2.706, 'learning_rate': 0.0009990273048976566, 'epoch': 0.05}
  5%|▍         | 108/2181 [12:38<3:47:05,  6.57s/it]  5%|▍         | 109/2181 [12:44<3:46:45,  6.57s/it]                                                    {'loss': 2.7042, 'learning_rate': 0.0009989804506039905, 'epoch': 0.05}
  5%|▍         | 109/2181 [12:45<3:46:45,  6.57s/it]  5%|▌         | 110/2181 [12:51<3:46:22,  6.56s/it]                                                    {'loss': 2.7045, 'learning_rate': 0.0009989324953733614, 'epoch': 0.05}
  5%|▌         | 110/2181 [12:51<3:46:22,  6.56s/it]  5%|▌         | 111/2181 [12:57<3:45:32,  6.54s/it]                                                    {'loss': 2.6327, 'learning_rate': 0.0009988834393115766, 'epoch': 0.05}
  5%|▌         | 111/2181 [12:58<3:45:32,  6.54s/it]  5%|▌         | 112/2181 [13:04<3:48:01,  6.61s/it]                                                    {'loss': 2.7335, 'learning_rate': 0.000998833282526872, 'epoch': 0.05}
  5%|▌         | 112/2181 [13:04<3:48:01,  6.61s/it]  5%|▌         | 113/2181 [13:11<3:51:48,  6.73s/it]                                                    {'loss': 2.6905, 'learning_rate': 0.0009987820251299122, 'epoch': 0.05}
  5%|▌         | 113/2181 [13:11<3:51:48,  6.73s/it]  5%|▌         | 114/2181 [13:18<3:50:58,  6.70s/it]                                                    {'loss': 2.8134, 'learning_rate': 0.00099872966723379, 'epoch': 0.05}
  5%|▌         | 114/2181 [13:18<3:50:58,  6.70s/it]  5%|▌         | 115/2181 [13:25<3:52:35,  6.75s/it]                                                    {'loss': 2.6315, 'learning_rate': 0.0009986762089540266, 'epoch': 0.05}
  5%|▌         | 115/2181 [13:25<3:52:35,  6.75s/it]  5%|▌         | 116/2181 [13:31<3:49:55,  6.68s/it]                                                    {'loss': 2.6253, 'learning_rate': 0.0009986216504085709, 'epoch': 0.05}
  5%|▌         | 116/2181 [13:31<3:49:55,  6.68s/it]  5%|▌         | 117/2181 [13:38<3:47:34,  6.62s/it]                                                    {'loss': 2.6552, 'learning_rate': 0.0009985659917177991, 'epoch': 0.05}
  5%|▌         | 117/2181 [13:38<3:47:34,  6.62s/it]  5%|▌         | 118/2181 [13:44<3:46:06,  6.58s/it]                                                    {'loss': 2.7214, 'learning_rate': 0.0009985092330045155, 'epoch': 0.05}
  5%|▌         | 118/2181 [13:44<3:46:06,  6.58s/it]  5%|▌         | 119/2181 [13:51<3:46:56,  6.60s/it]                                                    {'loss': 2.7095, 'learning_rate': 0.0009984513743939508, 'epoch': 0.05}
  5%|▌         | 119/2181 [13:51<3:46:56,  6.60s/it]  6%|▌         | 120/2181 [13:58<3:48:10,  6.64s/it]                                                    {'loss': 2.6914, 'learning_rate': 0.0009983924160137626, 'epoch': 0.06}
  6%|▌         | 120/2181 [13:58<3:48:10,  6.64s/it]  6%|▌         | 121/2181 [14:04<3:49:15,  6.68s/it]                                                    {'loss': 2.6614, 'learning_rate': 0.000998332357994035, 'epoch': 0.06}
  6%|▌         | 121/2181 [14:04<3:49:15,  6.68s/it]  6%|▌         | 122/2181 [14:11<3:46:15,  6.59s/it]                                                    {'loss': 2.7214, 'learning_rate': 0.0009982712004672786, 'epoch': 0.06}
  6%|▌         | 122/2181 [14:11<3:46:15,  6.59s/it]  6%|▌         | 123/2181 [14:17<3:44:36,  6.55s/it]                                                    {'loss': 2.7331, 'learning_rate': 0.0009982089435684295, 'epoch': 0.06}
  6%|▌         | 123/2181 [14:17<3:44:36,  6.55s/it]  6%|▌         | 124/2181 [14:24<3:44:24,  6.55s/it]                                                    {'loss': 2.663, 'learning_rate': 0.0009981455874348499, 'epoch': 0.06}
  6%|▌         | 124/2181 [14:24<3:44:24,  6.55s/it]  6%|▌         | 125/2181 [14:30<3:44:12,  6.54s/it]                                                    {'loss': 2.6128, 'learning_rate': 0.0009980811322063269, 'epoch': 0.06}
  6%|▌         | 125/2181 [14:30<3:44:12,  6.54s/it]  6%|▌         | 126/2181 [14:37<3:44:16,  6.55s/it]                                                    {'loss': 2.6485, 'learning_rate': 0.0009980155780250728, 'epoch': 0.06}
  6%|▌         | 126/2181 [14:37<3:44:16,  6.55s/it]  6%|▌         | 127/2181 [14:44<3:47:07,  6.63s/it]                                                    {'loss': 2.6435, 'learning_rate': 0.0009979489250357243, 'epoch': 0.06}
  6%|▌         | 127/2181 [14:44<3:47:07,  6.63s/it]  6%|▌         | 128/2181 [14:50<3:47:48,  6.66s/it]                                                    {'loss': 2.6791, 'learning_rate': 0.0009978811733853431, 'epoch': 0.06}
  6%|▌         | 128/2181 [14:50<3:47:48,  6.66s/it]  6%|▌         | 129/2181 [14:57<3:47:53,  6.66s/it]                                                    {'loss': 2.6355, 'learning_rate': 0.0009978123232234147, 'epoch': 0.06}
  6%|▌         | 129/2181 [14:57<3:47:53,  6.66s/it]  6%|▌         | 130/2181 [15:04<3:45:51,  6.61s/it]                                                    {'loss': 2.6136, 'learning_rate': 0.000997742374701848, 'epoch': 0.06}
  6%|▌         | 130/2181 [15:04<3:45:51,  6.61s/it]  6%|▌         | 131/2181 [15:10<3:45:21,  6.60s/it]                                                    {'loss': 2.6551, 'learning_rate': 0.0009976713279749754, 'epoch': 0.06}
  6%|▌         | 131/2181 [15:10<3:45:21,  6.60s/it]  6%|▌         | 132/2181 [15:17<3:45:11,  6.59s/it]                                                    {'loss': 2.6079, 'learning_rate': 0.0009975991831995528, 'epoch': 0.06}
  6%|▌         | 132/2181 [15:17<3:45:11,  6.59s/it]  6%|▌         | 133/2181 [15:23<3:44:53,  6.59s/it]                                                    {'loss': 2.6057, 'learning_rate': 0.0009975259405347581, 'epoch': 0.06}
  6%|▌         | 133/2181 [15:23<3:44:53,  6.59s/it]  6%|▌         | 134/2181 [15:30<3:46:05,  6.63s/it]                                                    {'loss': 2.6151, 'learning_rate': 0.0009974516001421926, 'epoch': 0.06}
  6%|▌         | 134/2181 [15:30<3:46:05,  6.63s/it]  6%|▌         | 135/2181 [15:37<3:47:53,  6.68s/it]                                                    {'loss': 2.635, 'learning_rate': 0.000997376162185878, 'epoch': 0.06}
  6%|▌         | 135/2181 [15:37<3:47:53,  6.68s/it]  6%|▌         | 136/2181 [15:44<3:47:59,  6.69s/it]                                                    {'loss': 2.6644, 'learning_rate': 0.0009972996268322594, 'epoch': 0.06}
  6%|▌         | 136/2181 [15:44<3:47:59,  6.69s/it]  6%|▋         | 137/2181 [15:50<3:46:13,  6.64s/it]                                                    {'loss': 2.596, 'learning_rate': 0.0009972219942502017, 'epoch': 0.06}
  6%|▋         | 137/2181 [15:50<3:46:13,  6.64s/it]  6%|▋         | 138/2181 [15:57<3:44:05,  6.58s/it]                                                    {'loss': 2.544, 'learning_rate': 0.0009971432646109918, 'epoch': 0.06}
  6%|▋         | 138/2181 [15:57<3:44:05,  6.58s/it]  6%|▋         | 139/2181 [16:03<3:42:56,  6.55s/it]                                                    {'loss': 2.6179, 'learning_rate': 0.0009970634380883365, 'epoch': 0.06}
  6%|▋         | 139/2181 [16:03<3:42:56,  6.55s/it]  6%|▋         | 140/2181 [16:09<3:41:57,  6.53s/it]                                                    {'loss': 2.5899, 'learning_rate': 0.0009969825148583627, 'epoch': 0.06}
  6%|▋         | 140/2181 [16:10<3:41:57,  6.53s/it]  6%|▋         | 141/2181 [16:16<3:41:25,  6.51s/it]                                                    {'loss': 2.6336, 'learning_rate': 0.0009969004950996173, 'epoch': 0.06}
  6%|▋         | 141/2181 [16:16<3:41:25,  6.51s/it]  7%|▋         | 142/2181 [16:23<3:44:07,  6.60s/it]                                                    {'loss': 2.6509, 'learning_rate': 0.0009968173789930668, 'epoch': 0.07}
  7%|▋         | 142/2181 [16:23<3:44:07,  6.60s/it]  7%|▋         | 143/2181 [16:29<3:45:10,  6.63s/it]                                                    {'loss': 2.5763, 'learning_rate': 0.0009967331667220958, 'epoch': 0.07}
  7%|▋         | 143/2181 [16:30<3:45:10,  6.63s/it]  7%|▋         | 144/2181 [16:36<3:42:58,  6.57s/it]                                                    {'loss': 2.6693, 'learning_rate': 0.0009966478584725086, 'epoch': 0.07}
  7%|▋         | 144/2181 [16:36<3:42:58,  6.57s/it]  7%|▋         | 145/2181 [16:42<3:41:36,  6.53s/it]                                                    {'loss': 2.6073, 'learning_rate': 0.0009965614544325263, 'epoch': 0.07}
  7%|▋         | 145/2181 [16:42<3:41:36,  6.53s/it]  7%|▋         | 146/2181 [16:49<3:42:03,  6.55s/it]                                                    {'loss': 2.6118, 'learning_rate': 0.000996473954792789, 'epoch': 0.07}
  7%|▋         | 146/2181 [16:49<3:42:03,  6.55s/it]  7%|▋         | 147/2181 [16:56<3:48:23,  6.74s/it]                                                    {'loss': 2.5515, 'learning_rate': 0.0009963853597463532, 'epoch': 0.07}
  7%|▋         | 147/2181 [16:56<3:48:23,  6.74s/it]  7%|▋         | 148/2181 [17:03<3:46:19,  6.68s/it]                                                    {'loss': 2.6552, 'learning_rate': 0.000996295669488693, 'epoch': 0.07}
  7%|▋         | 148/2181 [17:03<3:46:19,  6.68s/it]  7%|▋         | 149/2181 [17:09<3:45:45,  6.67s/it]                                                    {'loss': 2.5838, 'learning_rate': 0.0009962048842176979, 'epoch': 0.07}
  7%|▋         | 149/2181 [17:09<3:45:45,  6.67s/it]  7%|▋         | 150/2181 [17:16<3:45:41,  6.67s/it]                                                    {'loss': 2.6025, 'learning_rate': 0.0009961130041336748, 'epoch': 0.07}
  7%|▋         | 150/2181 [17:16<3:45:41,  6.67s/it]  7%|▋         | 151/2181 [17:23<3:51:17,  6.84s/it]                                                    {'loss': 2.5253, 'learning_rate': 0.0009960200294393449, 'epoch': 0.07}
  7%|▋         | 151/2181 [17:23<3:51:17,  6.84s/it]  7%|▋         | 152/2181 [17:30<3:47:52,  6.74s/it]                                                    {'loss': 2.5798, 'learning_rate': 0.0009959259603398453, 'epoch': 0.07}
  7%|▋         | 152/2181 [17:30<3:47:52,  6.74s/it]  7%|▋         | 153/2181 [17:36<3:44:16,  6.64s/it]                                                    {'loss': 2.6421, 'learning_rate': 0.0009958307970427275, 'epoch': 0.07}
  7%|▋         | 153/2181 [17:36<3:44:16,  6.64s/it]  7%|▋         | 154/2181 [17:43<3:42:35,  6.59s/it]                                                    {'loss': 2.5841, 'learning_rate': 0.0009957345397579572, 'epoch': 0.07}
  7%|▋         | 154/2181 [17:43<3:42:35,  6.59s/it]  7%|▋         | 155/2181 [17:49<3:41:55,  6.57s/it]                                                    {'loss': 2.6017, 'learning_rate': 0.0009956371886979138, 'epoch': 0.07}
  7%|▋         | 155/2181 [17:49<3:41:55,  6.57s/it]  7%|▋         | 156/2181 [17:56<3:42:56,  6.61s/it]                                                    {'loss': 2.6225, 'learning_rate': 0.00099553874407739, 'epoch': 0.07}
  7%|▋         | 156/2181 [17:56<3:42:56,  6.61s/it]  7%|▋         | 157/2181 [18:02<3:43:53,  6.64s/it]                                                    {'loss': 2.5965, 'learning_rate': 0.0009954392061135916, 'epoch': 0.07}
  7%|▋         | 157/2181 [18:03<3:43:53,  6.64s/it]  7%|▋         | 158/2181 [18:09<3:43:50,  6.64s/it]                                                    {'loss': 2.5158, 'learning_rate': 0.0009953385750261364, 'epoch': 0.07}
  7%|▋         | 158/2181 [18:09<3:43:50,  6.64s/it]  7%|▋         | 159/2181 [18:16<3:41:53,  6.58s/it]                                                    {'loss': 2.5048, 'learning_rate': 0.0009952368510370538, 'epoch': 0.07}
  7%|▋         | 159/2181 [18:16<3:41:53,  6.58s/it]  7%|▋         | 160/2181 [18:23<3:49:58,  6.83s/it]                                                    {'loss': 2.5353, 'learning_rate': 0.0009951340343707852, 'epoch': 0.07}
  7%|▋         | 160/2181 [18:23<3:49:58,  6.83s/it]  7%|▋         | 161/2181 [18:30<3:46:53,  6.74s/it]                                                    {'loss': 2.4836, 'learning_rate': 0.0009950301252541823, 'epoch': 0.07}
  7%|▋         | 161/2181 [18:30<3:46:53,  6.74s/it]  7%|▋         | 162/2181 [18:36<3:44:04,  6.66s/it]                                                    {'loss': 2.4894, 'learning_rate': 0.0009949251239165075, 'epoch': 0.07}
  7%|▋         | 162/2181 [18:36<3:44:04,  6.66s/it]  7%|▋         | 163/2181 [18:43<3:42:59,  6.63s/it]                                                    {'loss': 2.4383, 'learning_rate': 0.000994819030589433, 'epoch': 0.07}
  7%|▋         | 163/2181 [18:43<3:42:59,  6.63s/it]  8%|▊         | 164/2181 [18:49<3:44:24,  6.68s/it]                                                    {'loss': 2.4759, 'learning_rate': 0.00099471184550704, 'epoch': 0.08}
  8%|▊         | 164/2181 [18:49<3:44:24,  6.68s/it]  8%|▊         | 165/2181 [18:56<3:45:42,  6.72s/it]                                                    {'loss': 2.469, 'learning_rate': 0.0009946035689058189, 'epoch': 0.08}
  8%|▊         | 165/2181 [18:56<3:45:42,  6.72s/it]  8%|▊         | 166/2181 [19:03<3:45:33,  6.72s/it]                                                    {'loss': 2.5339, 'learning_rate': 0.0009944942010246681, 'epoch': 0.08}
  8%|▊         | 166/2181 [19:03<3:45:33,  6.72s/it]  8%|▊         | 167/2181 [19:09<3:42:49,  6.64s/it]                                                    {'loss': 2.5728, 'learning_rate': 0.0009943837421048942, 'epoch': 0.08}
  8%|▊         | 167/2181 [19:09<3:42:49,  6.64s/it]  8%|▊         | 168/2181 [19:16<3:40:55,  6.58s/it]                                                    {'loss': 2.5311, 'learning_rate': 0.0009942721923902106, 'epoch': 0.08}
  8%|▊         | 168/2181 [19:16<3:40:55,  6.58s/it]  8%|▊         | 169/2181 [19:22<3:40:20,  6.57s/it]                                                    {'loss': 2.5687, 'learning_rate': 0.0009941595521267377, 'epoch': 0.08}
  8%|▊         | 169/2181 [19:22<3:40:20,  6.57s/it]  8%|▊         | 170/2181 [19:29<3:39:35,  6.55s/it]                                                    {'loss': 2.5062, 'learning_rate': 0.0009940458215630017, 'epoch': 0.08}
  8%|▊         | 170/2181 [19:29<3:39:35,  6.55s/it]  8%|▊         | 171/2181 [19:36<3:40:51,  6.59s/it]                                                    {'loss': 2.5008, 'learning_rate': 0.0009939310009499348, 'epoch': 0.08}
  8%|▊         | 171/2181 [19:36<3:40:51,  6.59s/it]  8%|▊         | 172/2181 [19:42<3:41:54,  6.63s/it]                                                    {'loss': 2.5466, 'learning_rate': 0.000993815090540874, 'epoch': 0.08}
  8%|▊         | 172/2181 [19:42<3:41:54,  6.63s/it]  8%|▊         | 173/2181 [19:49<3:42:48,  6.66s/it]                                                    {'loss': 2.5049, 'learning_rate': 0.000993698090591561, 'epoch': 0.08}
  8%|▊         | 173/2181 [19:49<3:42:48,  6.66s/it]  8%|▊         | 174/2181 [19:55<3:40:21,  6.59s/it]                                                    {'loss': 2.4908, 'learning_rate': 0.0009935800013601416, 'epoch': 0.08}
  8%|▊         | 174/2181 [19:55<3:40:21,  6.59s/it]  8%|▊         | 175/2181 [20:02<3:39:24,  6.56s/it]                                                    {'loss': 2.5828, 'learning_rate': 0.000993460823107164, 'epoch': 0.08}
  8%|▊         | 175/2181 [20:02<3:39:24,  6.56s/it]  8%|▊         | 176/2181 [20:08<3:37:38,  6.51s/it]                                                    {'loss': 2.5894, 'learning_rate': 0.0009933405560955803, 'epoch': 0.08}
  8%|▊         | 176/2181 [20:08<3:37:38,  6.51s/it]  8%|▊         | 177/2181 [20:15<3:37:47,  6.52s/it]                                                    {'loss': 2.473, 'learning_rate': 0.0009932192005907446, 'epoch': 0.08}
  8%|▊         | 177/2181 [20:15<3:37:47,  6.52s/it]  8%|▊         | 178/2181 [20:21<3:36:59,  6.50s/it]                                                    {'loss': 2.556, 'learning_rate': 0.0009930967568604118, 'epoch': 0.08}
  8%|▊         | 178/2181 [20:21<3:36:59,  6.50s/it]  8%|▊         | 179/2181 [20:28<3:38:40,  6.55s/it]                                                    {'loss': 2.5332, 'learning_rate': 0.000992973225174739, 'epoch': 0.08}
  8%|▊         | 179/2181 [20:28<3:38:40,  6.55s/it]  8%|▊         | 180/2181 [20:35<3:40:14,  6.60s/it]                                                    {'loss': 2.5142, 'learning_rate': 0.0009928486058062827, 'epoch': 0.08}
  8%|▊         | 180/2181 [20:35<3:40:14,  6.60s/it]  8%|▊         | 181/2181 [20:41<3:38:43,  6.56s/it]                                                    {'loss': 2.4812, 'learning_rate': 0.0009927228990299999, 'epoch': 0.08}
  8%|▊         | 181/2181 [20:41<3:38:43,  6.56s/it]  8%|▊         | 182/2181 [20:48<3:41:28,  6.65s/it]                                                    {'loss': 2.5547, 'learning_rate': 0.0009925961051232468, 'epoch': 0.08}
  8%|▊         | 182/2181 [20:48<3:41:28,  6.65s/it]  8%|▊         | 183/2181 [20:54<3:39:13,  6.58s/it]                                                    {'loss': 2.5473, 'learning_rate': 0.000992468224365778, 'epoch': 0.08}
  8%|▊         | 183/2181 [20:54<3:39:13,  6.58s/it]  8%|▊         | 184/2181 [21:01<3:38:24,  6.56s/it]                                                    {'loss': 2.491, 'learning_rate': 0.000992339257039746, 'epoch': 0.08}
  8%|▊         | 184/2181 [21:01<3:38:24,  6.56s/it]  8%|▊         | 185/2181 [21:07<3:37:08,  6.53s/it]                                                    {'loss': 2.493, 'learning_rate': 0.0009922092034297006, 'epoch': 0.08}
  8%|▊         | 185/2181 [21:07<3:37:08,  6.53s/it]  9%|▊         | 186/2181 [21:14<3:39:32,  6.60s/it]                                                    {'loss': 2.4899, 'learning_rate': 0.0009920780638225891, 'epoch': 0.09}
  9%|▊         | 186/2181 [21:14<3:39:32,  6.60s/it]  9%|▊         | 187/2181 [21:21<3:41:23,  6.66s/it]                                                    {'loss': 2.5022, 'learning_rate': 0.0009919458385077538, 'epoch': 0.09}
  9%|▊         | 187/2181 [21:21<3:41:23,  6.66s/it]  9%|▊         | 188/2181 [21:28<3:42:08,  6.69s/it]                                                    {'loss': 2.449, 'learning_rate': 0.0009918125277769336, 'epoch': 0.09}
  9%|▊         | 188/2181 [21:28<3:42:08,  6.69s/it]  9%|▊         | 189/2181 [21:34<3:39:11,  6.60s/it]                                                    {'loss': 2.6187, 'learning_rate': 0.0009916781319242614, 'epoch': 0.09}
  9%|▊         | 189/2181 [21:34<3:39:11,  6.60s/it]  9%|▊         | 190/2181 [21:41<3:38:44,  6.59s/it]                                                    {'loss': 2.4355, 'learning_rate': 0.0009915426512462646, 'epoch': 0.09}
  9%|▊         | 190/2181 [21:41<3:38:44,  6.59s/it]  9%|▉         | 191/2181 [21:47<3:37:33,  6.56s/it]                                                    {'loss': 2.3906, 'learning_rate': 0.0009914060860418644, 'epoch': 0.09}
  9%|▉         | 191/2181 [21:47<3:37:33,  6.56s/it]  9%|▉         | 192/2181 [21:53<3:35:20,  6.50s/it]                                                    {'loss': 2.4935, 'learning_rate': 0.000991268436612374, 'epoch': 0.09}
  9%|▉         | 192/2181 [21:54<3:35:20,  6.50s/it]  9%|▉         | 193/2181 [22:00<3:35:50,  6.51s/it]                                                    {'loss': 2.4456, 'learning_rate': 0.0009911297032614997, 'epoch': 0.09}
  9%|▉         | 193/2181 [22:00<3:35:50,  6.51s/it]  9%|▉         | 194/2181 [22:07<3:37:23,  6.56s/it]                                                    {'loss': 2.5669, 'learning_rate': 0.000990989886295339, 'epoch': 0.09}
  9%|▉         | 194/2181 [22:07<3:37:23,  6.56s/it]  9%|▉         | 195/2181 [22:13<3:38:43,  6.61s/it]                                                    {'loss': 2.4762, 'learning_rate': 0.0009908489860223804, 'epoch': 0.09}
  9%|▉         | 195/2181 [22:14<3:38:43,  6.61s/it]  9%|▉         | 196/2181 [22:20<3:37:31,  6.57s/it]                                                    {'loss': 2.5088, 'learning_rate': 0.000990707002753502, 'epoch': 0.09}
  9%|▉         | 196/2181 [22:20<3:37:31,  6.57s/it]  9%|▉         | 197/2181 [22:26<3:36:56,  6.56s/it]                                                    {'loss': 2.4446, 'learning_rate': 0.0009905639368019724, 'epoch': 0.09}
  9%|▉         | 197/2181 [22:27<3:36:56,  6.56s/it]  9%|▉         | 198/2181 [22:33<3:35:10,  6.51s/it]                                                    {'loss': 2.4682, 'learning_rate': 0.0009904197884834482, 'epoch': 0.09}
  9%|▉         | 198/2181 [22:33<3:35:10,  6.51s/it]  9%|▉         | 199/2181 [22:39<3:35:14,  6.52s/it]                                                    {'loss': 2.3891, 'learning_rate': 0.0009902745581159742, 'epoch': 0.09}
  9%|▉         | 199/2181 [22:39<3:35:14,  6.52s/it]  9%|▉         | 200/2181 [22:46<3:34:11,  6.49s/it]                                                    {'loss': 2.4966, 'learning_rate': 0.0009901282460199829, 'epoch': 0.09}
  9%|▉         | 200/2181 [22:46<3:34:11,  6.49s/it]  9%|▉         | 201/2181 [22:53<3:38:14,  6.61s/it]                                                    {'loss': 2.5197, 'learning_rate': 0.0009899808525182935, 'epoch': 0.09}
  9%|▉         | 201/2181 [22:53<3:38:14,  6.61s/it]  9%|▉         | 202/2181 [22:59<3:38:35,  6.63s/it]                                                    {'loss': 2.4131, 'learning_rate': 0.0009898323779361107, 'epoch': 0.09}
  9%|▉         | 202/2181 [22:59<3:38:35,  6.63s/it]  9%|▉         | 203/2181 [23:06<3:38:57,  6.64s/it]                                                    {'loss': 2.4097, 'learning_rate': 0.000989682822601025, 'epoch': 0.09}
  9%|▉         | 203/2181 [23:06<3:38:57,  6.64s/it]  9%|▉         | 204/2181 [23:13<3:37:26,  6.60s/it]                                                    {'loss': 2.3794, 'learning_rate': 0.0009895321868430113, 'epoch': 0.09}
  9%|▉         | 204/2181 [23:13<3:37:26,  6.60s/it]  9%|▉         | 205/2181 [23:19<3:36:07,  6.56s/it]                                                    {'loss': 2.5574, 'learning_rate': 0.0009893804709944281, 'epoch': 0.09}
  9%|▉         | 205/2181 [23:19<3:36:07,  6.56s/it]  9%|▉         | 206/2181 [23:25<3:35:07,  6.54s/it]                                                    {'loss': 2.4885, 'learning_rate': 0.0009892276753900174, 'epoch': 0.09}
  9%|▉         | 206/2181 [23:26<3:35:07,  6.54s/it]  9%|▉         | 207/2181 [23:32<3:34:27,  6.52s/it]                                                    {'loss': 2.4333, 'learning_rate': 0.0009890738003669028, 'epoch': 0.09}
  9%|▉         | 207/2181 [23:32<3:34:27,  6.52s/it] 10%|▉         | 208/2181 [23:39<3:35:04,  6.54s/it]                                                    {'loss': 2.477, 'learning_rate': 0.0009889188462645904, 'epoch': 0.1}
 10%|▉         | 208/2181 [23:39<3:35:04,  6.54s/it] 10%|▉         | 209/2181 [23:45<3:37:24,  6.61s/it]                                                    {'loss': 2.4382, 'learning_rate': 0.0009887628134249667, 'epoch': 0.1}
 10%|▉         | 209/2181 [23:45<3:37:24,  6.61s/it] 10%|▉         | 210/2181 [23:52<3:37:28,  6.62s/it]                                                    {'loss': 2.5264, 'learning_rate': 0.0009886057021922983, 'epoch': 0.1}
 10%|▉         | 210/2181 [23:52<3:37:28,  6.62s/it] 10%|▉         | 211/2181 [23:58<3:35:55,  6.58s/it]                                                    {'loss': 2.4829, 'learning_rate': 0.0009884475129132311, 'epoch': 0.1}
 10%|▉         | 211/2181 [23:59<3:35:55,  6.58s/it] 10%|▉         | 212/2181 [24:05<3:34:55,  6.55s/it]                                                    {'loss': 2.4388, 'learning_rate': 0.0009882882459367897, 'epoch': 0.1}
 10%|▉         | 212/2181 [24:05<3:34:55,  6.55s/it] 10%|▉         | 213/2181 [24:11<3:34:21,  6.54s/it]                                                    {'loss': 2.4442, 'learning_rate': 0.0009881279016143766, 'epoch': 0.1}
 10%|▉         | 213/2181 [24:12<3:34:21,  6.54s/it] 10%|▉         | 214/2181 [24:18<3:33:11,  6.50s/it]                                                    {'loss': 2.5234, 'learning_rate': 0.0009879664802997707, 'epoch': 0.1}
 10%|▉         | 214/2181 [24:18<3:33:11,  6.50s/it] 10%|▉         | 215/2181 [24:24<3:33:17,  6.51s/it]                                                    {'loss': 2.4247, 'learning_rate': 0.000987803982349128, 'epoch': 0.1}
 10%|▉         | 215/2181 [24:24<3:33:17,  6.51s/it] 10%|▉         | 216/2181 [24:31<3:36:09,  6.60s/it]                                                    {'loss': 2.4558, 'learning_rate': 0.0009876404081209796, 'epoch': 0.1}
 10%|▉         | 216/2181 [24:31<3:36:09,  6.60s/it] 10%|▉         | 217/2181 [24:38<3:36:52,  6.63s/it]                                                    {'loss': 2.4151, 'learning_rate': 0.000987475757976231, 'epoch': 0.1}
 10%|▉         | 217/2181 [24:38<3:36:52,  6.63s/it] 10%|▉         | 218/2181 [24:45<3:37:08,  6.64s/it]                                                    {'loss': 2.499, 'learning_rate': 0.000987310032278162, 'epoch': 0.1}
 10%|▉         | 218/2181 [24:45<3:37:08,  6.64s/it] 10%|█         | 219/2181 [24:51<3:35:03,  6.58s/it]                                                    {'loss': 2.4876, 'learning_rate': 0.0009871432313924254, 'epoch': 0.1}
 10%|█         | 219/2181 [24:51<3:35:03,  6.58s/it] 10%|█         | 220/2181 [24:58<3:35:34,  6.60s/it]                                                    {'loss': 2.4419, 'learning_rate': 0.000986975355687046, 'epoch': 0.1}
 10%|█         | 220/2181 [24:58<3:35:34,  6.60s/it] 10%|█         | 221/2181 [25:04<3:34:42,  6.57s/it]                                                    {'loss': 2.4149, 'learning_rate': 0.0009868064055324204, 'epoch': 0.1}
 10%|█         | 221/2181 [25:04<3:34:42,  6.57s/it] 10%|█         | 222/2181 [25:11<3:32:46,  6.52s/it]                                                    {'loss': 2.449, 'learning_rate': 0.0009866363813013153, 'epoch': 0.1}
 10%|█         | 222/2181 [25:11<3:32:46,  6.52s/it] 10%|█         | 223/2181 [25:17<3:33:18,  6.54s/it]                                                    {'loss': 2.4418, 'learning_rate': 0.0009864652833688676, 'epoch': 0.1}
 10%|█         | 223/2181 [25:17<3:33:18,  6.54s/it] 10%|█         | 224/2181 [25:24<3:35:19,  6.60s/it]                                                    {'loss': 2.4852, 'learning_rate': 0.0009862931121125836, 'epoch': 0.1}
 10%|█         | 224/2181 [25:24<3:35:19,  6.60s/it] 10%|█         | 225/2181 [25:31<3:35:27,  6.61s/it]                                                    {'loss': 2.4093, 'learning_rate': 0.000986119867912337, 'epoch': 0.1}
 10%|█         | 225/2181 [25:31<3:35:27,  6.61s/it] 10%|█         | 226/2181 [25:37<3:34:28,  6.58s/it]                                                    {'loss': 2.3507, 'learning_rate': 0.000985945551150369, 'epoch': 0.1}
 10%|█         | 226/2181 [25:37<3:34:28,  6.58s/it] 10%|█         | 227/2181 [25:44<3:33:27,  6.55s/it]                                                    {'loss': 2.5155, 'learning_rate': 0.0009857701622112876, 'epoch': 0.1}
 10%|█         | 227/2181 [25:44<3:33:27,  6.55s/it] 10%|█         | 228/2181 [25:50<3:32:35,  6.53s/it]                                                    {'loss': 2.4124, 'learning_rate': 0.000985593701482066, 'epoch': 0.1}
 10%|█         | 228/2181 [25:50<3:32:35,  6.53s/it] 10%|█         | 229/2181 [25:56<3:31:57,  6.52s/it]                                                    {'loss': 2.4794, 'learning_rate': 0.0009854161693520424, 'epoch': 0.1}
 10%|█         | 229/2181 [25:57<3:31:57,  6.52s/it] 11%|█         | 230/2181 [26:03<3:31:11,  6.50s/it]                                                    {'loss': 2.3456, 'learning_rate': 0.0009852375662129194, 'epoch': 0.11}
 11%|█         | 230/2181 [26:03<3:31:11,  6.50s/it] 11%|█         | 231/2181 [26:10<3:32:50,  6.55s/it]                                                    {'loss': 2.5348, 'learning_rate': 0.0009850578924587613, 'epoch': 0.11}
 11%|█         | 231/2181 [26:10<3:32:50,  6.55s/it] 11%|█         | 232/2181 [26:16<3:33:54,  6.58s/it]                                                    {'loss': 2.4332, 'learning_rate': 0.000984877148485996, 'epoch': 0.11}
 11%|█         | 232/2181 [26:16<3:33:54,  6.58s/it] 11%|█         | 233/2181 [26:23<3:34:40,  6.61s/it]                                                    {'loss': 2.366, 'learning_rate': 0.000984695334693412, 'epoch': 0.11}
 11%|█         | 233/2181 [26:23<3:34:40,  6.61s/it] 11%|█         | 234/2181 [26:29<3:32:50,  6.56s/it]                                                    {'loss': 2.3898, 'learning_rate': 0.000984512451482158, 'epoch': 0.11}
 11%|█         | 234/2181 [26:29<3:32:50,  6.56s/it] 11%|█         | 235/2181 [26:36<3:32:00,  6.54s/it]                                                    {'loss': 2.3715, 'learning_rate': 0.0009843284992557431, 'epoch': 0.11}
 11%|█         | 235/2181 [26:36<3:32:00,  6.54s/it] 11%|█         | 236/2181 [26:42<3:31:01,  6.51s/it]                                                    {'loss': 2.4255, 'learning_rate': 0.000984143478420034, 'epoch': 0.11}
 11%|█         | 236/2181 [26:42<3:31:01,  6.51s/it] 11%|█         | 237/2181 [26:49<3:31:46,  6.54s/it]                                                    {'loss': 2.4345, 'learning_rate': 0.0009839573893832563, 'epoch': 0.11}
 11%|█         | 237/2181 [26:49<3:31:46,  6.54s/it] 11%|█         | 238/2181 [26:56<3:32:19,  6.56s/it]                                                    {'loss': 2.4077, 'learning_rate': 0.000983770232555991, 'epoch': 0.11}
 11%|█         | 238/2181 [26:56<3:32:19,  6.56s/it] 11%|█         | 239/2181 [27:02<3:34:31,  6.63s/it]                                                    {'loss': 2.419, 'learning_rate': 0.0009835820083511765, 'epoch': 0.11}
 11%|█         | 239/2181 [27:02<3:34:31,  6.63s/it] 11%|█         | 240/2181 [27:09<3:34:46,  6.64s/it]                                                    {'loss': 2.3929, 'learning_rate': 0.0009833927171841055, 'epoch': 0.11}
 11%|█         | 240/2181 [27:09<3:34:46,  6.64s/it] 11%|█         | 241/2181 [27:15<3:32:30,  6.57s/it]                                                    {'loss': 2.3543, 'learning_rate': 0.0009832023594724246, 'epoch': 0.11}
 11%|█         | 241/2181 [27:15<3:32:30,  6.57s/it] 11%|█         | 242/2181 [27:22<3:31:44,  6.55s/it]                                                    {'loss': 2.4419, 'learning_rate': 0.0009830109356361344, 'epoch': 0.11}
 11%|█         | 242/2181 [27:22<3:31:44,  6.55s/it] 11%|█         | 243/2181 [27:28<3:30:43,  6.52s/it]                                                    {'loss': 2.3954, 'learning_rate': 0.0009828184460975867, 'epoch': 0.11}
 11%|█         | 243/2181 [27:28<3:30:43,  6.52s/it] 11%|█         | 244/2181 [27:35<3:29:54,  6.50s/it]                                                    {'loss': 2.4348, 'learning_rate': 0.0009826248912814855, 'epoch': 0.11}
 11%|█         | 244/2181 [27:35<3:29:54,  6.50s/it] 11%|█         | 245/2181 [27:41<3:29:05,  6.48s/it]                                                    {'loss': 2.4452, 'learning_rate': 0.0009824302716148847, 'epoch': 0.11}
 11%|█         | 245/2181 [27:41<3:29:05,  6.48s/it] 11%|█▏        | 246/2181 [27:48<3:32:34,  6.59s/it]                                                    {'loss': 2.364, 'learning_rate': 0.0009822345875271884, 'epoch': 0.11}
 11%|█▏        | 246/2181 [27:48<3:32:34,  6.59s/it] 11%|█▏        | 247/2181 [27:55<3:33:53,  6.64s/it]                                                    {'loss': 2.4691, 'learning_rate': 0.0009820378394501481, 'epoch': 0.11}
 11%|█▏        | 247/2181 [27:55<3:33:53,  6.64s/it] 11%|█▏        | 248/2181 [28:01<3:33:43,  6.63s/it]                                                    {'loss': 2.3866, 'learning_rate': 0.0009818400278178636, 'epoch': 0.11}
 11%|█▏        | 248/2181 [28:01<3:33:43,  6.63s/it] 11%|█▏        | 249/2181 [28:08<3:32:24,  6.60s/it]                                                    {'loss': 2.4199, 'learning_rate': 0.0009816411530667814, 'epoch': 0.11}
 11%|█▏        | 249/2181 [28:08<3:32:24,  6.60s/it] 11%|█▏        | 250/2181 [28:14<3:30:34,  6.54s/it]                                                    {'loss': 2.329, 'learning_rate': 0.000981441215635693, 'epoch': 0.11}
 11%|█▏        | 250/2181 [28:14<3:30:34,  6.54s/it] 12%|█▏        | 251/2181 [28:21<3:30:01,  6.53s/it]                                                    {'loss': 2.4106, 'learning_rate': 0.0009812402159657353, 'epoch': 0.12}
 12%|█▏        | 251/2181 [28:21<3:30:01,  6.53s/it] 12%|█▏        | 252/2181 [28:28<3:31:04,  6.57s/it]                                                    {'loss': 2.3069, 'learning_rate': 0.000981038154500388, 'epoch': 0.12}
 12%|█▏        | 252/2181 [28:28<3:31:04,  6.57s/it] 12%|█▏        | 253/2181 [28:34<3:31:40,  6.59s/it]                                                    {'loss': 2.3342, 'learning_rate': 0.0009808350316854746, 'epoch': 0.12}
 12%|█▏        | 253/2181 [28:34<3:31:40,  6.59s/it] 12%|█▏        | 254/2181 [28:41<3:33:07,  6.64s/it]                                                    {'loss': 2.3258, 'learning_rate': 0.0009806308479691594, 'epoch': 0.12}
 12%|█▏        | 254/2181 [28:41<3:33:07,  6.64s/it] 12%|█▏        | 255/2181 [28:48<3:33:59,  6.67s/it]                                                    {'loss': 2.3883, 'learning_rate': 0.0009804256038019482, 'epoch': 0.12}
 12%|█▏        | 255/2181 [28:48<3:33:59,  6.67s/it] 12%|█▏        | 256/2181 [28:54<3:32:45,  6.63s/it]                                                    {'loss': 2.3711, 'learning_rate': 0.0009802192996366857, 'epoch': 0.12}
 12%|█▏        | 256/2181 [28:54<3:32:45,  6.63s/it] 12%|█▏        | 257/2181 [29:01<3:31:54,  6.61s/it]                                                    {'loss': 2.4115, 'learning_rate': 0.0009800119359285563, 'epoch': 0.12}
 12%|█▏        | 257/2181 [29:01<3:31:54,  6.61s/it] 12%|█▏        | 258/2181 [29:07<3:30:47,  6.58s/it]                                                    {'loss': 2.3176, 'learning_rate': 0.0009798035131350813, 'epoch': 0.12}
 12%|█▏        | 258/2181 [29:07<3:30:47,  6.58s/it] 12%|█▏        | 259/2181 [29:14<3:29:42,  6.55s/it]                                                    {'loss': 2.3858, 'learning_rate': 0.0009795940317161194, 'epoch': 0.12}
 12%|█▏        | 259/2181 [29:14<3:29:42,  6.55s/it] 12%|█▏        | 260/2181 [29:20<3:29:05,  6.53s/it]                                                    {'loss': 2.294, 'learning_rate': 0.0009793834921338646, 'epoch': 0.12}
 12%|█▏        | 260/2181 [29:20<3:29:05,  6.53s/it] 12%|█▏        | 261/2181 [29:27<3:30:47,  6.59s/it]                                                    {'loss': 2.4204, 'learning_rate': 0.0009791718948528457, 'epoch': 0.12}
 12%|█▏        | 261/2181 [29:27<3:30:47,  6.59s/it] 12%|█▏        | 262/2181 [29:34<3:32:01,  6.63s/it]                                                    {'loss': 2.3496, 'learning_rate': 0.0009789592403399252, 'epoch': 0.12}
 12%|█▏        | 262/2181 [29:34<3:32:01,  6.63s/it] 12%|█▏        | 263/2181 [29:40<3:31:18,  6.61s/it]                                                    {'loss': 2.3711, 'learning_rate': 0.0009787455290642985, 'epoch': 0.12}
 12%|█▏        | 263/2181 [29:40<3:31:18,  6.61s/it] 12%|█▏        | 264/2181 [29:47<3:29:50,  6.57s/it]                                                    {'loss': 2.4803, 'learning_rate': 0.000978530761497492, 'epoch': 0.12}
 12%|█▏        | 264/2181 [29:47<3:29:50,  6.57s/it] 12%|█▏        | 265/2181 [29:53<3:28:03,  6.52s/it]                                                    {'loss': 2.4044, 'learning_rate': 0.0009783149381133633, 'epoch': 0.12}
 12%|█▏        | 265/2181 [29:53<3:28:03,  6.52s/it] 12%|█▏        | 266/2181 [30:00<3:27:44,  6.51s/it]                                                    {'loss': 2.3629, 'learning_rate': 0.0009780980593880992, 'epoch': 0.12}
 12%|█▏        | 266/2181 [30:00<3:27:44,  6.51s/it] 12%|█▏        | 267/2181 [30:06<3:27:48,  6.51s/it]                                                    {'loss': 2.3176, 'learning_rate': 0.0009778801258002153, 'epoch': 0.12}
 12%|█▏        | 267/2181 [30:06<3:27:48,  6.51s/it] 12%|█▏        | 268/2181 [30:13<3:28:12,  6.53s/it]                                                    {'loss': 2.3702, 'learning_rate': 0.000977661137830554, 'epoch': 0.12}
 12%|█▏        | 268/2181 [30:13<3:28:12,  6.53s/it] 12%|█▏        | 269/2181 [30:19<3:29:58,  6.59s/it]                                                    {'loss': 2.3463, 'learning_rate': 0.0009774410959622845, 'epoch': 0.12}
 12%|█▏        | 269/2181 [30:20<3:29:58,  6.59s/it] 12%|█▏        | 270/2181 [30:26<3:31:43,  6.65s/it]                                                    {'loss': 2.3179, 'learning_rate': 0.000977220000680901, 'epoch': 0.12}
 12%|█▏        | 270/2181 [30:26<3:31:43,  6.65s/it] 12%|█▏        | 271/2181 [30:33<3:29:44,  6.59s/it]                                                    {'loss': 2.3363, 'learning_rate': 0.000976997852474223, 'epoch': 0.12}
 12%|█▏        | 271/2181 [30:33<3:29:44,  6.59s/it] 12%|█▏        | 272/2181 [30:39<3:28:52,  6.56s/it]                                                    {'loss': 2.365, 'learning_rate': 0.0009767746518323914, 'epoch': 0.12}
 12%|█▏        | 272/2181 [30:39<3:28:52,  6.56s/it] 13%|█▎        | 273/2181 [30:46<3:27:15,  6.52s/it]                                                    {'loss': 2.2963, 'learning_rate': 0.0009765503992478704, 'epoch': 0.13}
 13%|█▎        | 273/2181 [30:46<3:27:15,  6.52s/it] 13%|█▎        | 274/2181 [30:52<3:26:32,  6.50s/it]                                                    {'loss': 2.4041, 'learning_rate': 0.0009763250952154449, 'epoch': 0.13}
 13%|█▎        | 274/2181 [30:52<3:26:32,  6.50s/it] 13%|█▎        | 275/2181 [30:59<3:26:51,  6.51s/it]                                                    {'loss': 2.3083, 'learning_rate': 0.0009760987402322195, 'epoch': 0.13}
 13%|█▎        | 275/2181 [30:59<3:26:51,  6.51s/it] 13%|█▎        | 276/2181 [31:05<3:27:55,  6.55s/it]                                                    {'loss': 2.3439, 'learning_rate': 0.0009758713347976178, 'epoch': 0.13}
 13%|█▎        | 276/2181 [31:05<3:27:55,  6.55s/it] 13%|█▎        | 277/2181 [31:12<3:29:58,  6.62s/it]                                                    {'loss': 2.3971, 'learning_rate': 0.000975642879413381, 'epoch': 0.13}
 13%|█▎        | 277/2181 [31:12<3:29:58,  6.62s/it] 13%|█▎        | 278/2181 [31:19<3:30:03,  6.62s/it]                                                    {'loss': 2.399, 'learning_rate': 0.0009754133745835665, 'epoch': 0.13}
 13%|█▎        | 278/2181 [31:19<3:30:03,  6.62s/it] 13%|█▎        | 279/2181 [31:25<3:29:14,  6.60s/it]                                                    {'loss': 2.3904, 'learning_rate': 0.0009751828208145482, 'epoch': 0.13}
 13%|█▎        | 279/2181 [31:25<3:29:14,  6.60s/it] 13%|█▎        | 280/2181 [31:32<3:27:24,  6.55s/it]                                                    {'loss': 2.3768, 'learning_rate': 0.0009749512186150131, 'epoch': 0.13}
 13%|█▎        | 280/2181 [31:32<3:27:24,  6.55s/it] 13%|█▎        | 281/2181 [31:38<3:27:16,  6.55s/it]                                                    {'loss': 2.3367, 'learning_rate': 0.0009747185684959625, 'epoch': 0.13}
 13%|█▎        | 281/2181 [31:38<3:27:16,  6.55s/it] 13%|█▎        | 282/2181 [31:45<3:26:00,  6.51s/it]                                                    {'loss': 2.2978, 'learning_rate': 0.000974484870970709, 'epoch': 0.13}
 13%|█▎        | 282/2181 [31:45<3:26:00,  6.51s/it] 13%|█▎        | 283/2181 [31:51<3:26:10,  6.52s/it]                                                    {'loss': 2.3131, 'learning_rate': 0.0009742501265548767, 'epoch': 0.13}
 13%|█▎        | 283/2181 [31:51<3:26:10,  6.52s/it] 13%|█▎        | 284/2181 [31:58<3:27:48,  6.57s/it]                                                    {'loss': 2.3343, 'learning_rate': 0.0009740143357663993, 'epoch': 0.13}
 13%|█▎        | 284/2181 [31:58<3:27:48,  6.57s/it] 13%|█▎        | 285/2181 [32:05<3:29:12,  6.62s/it]                                                    {'loss': 2.3029, 'learning_rate': 0.000973777499125519, 'epoch': 0.13}
 13%|█▎        | 285/2181 [32:05<3:29:12,  6.62s/it] 13%|█▎        | 286/2181 [32:11<3:28:17,  6.59s/it]                                                    {'loss': 2.3644, 'learning_rate': 0.0009735396171547859, 'epoch': 0.13}
 13%|█▎        | 286/2181 [32:11<3:28:17,  6.59s/it] 13%|█▎        | 287/2181 [32:17<3:26:29,  6.54s/it]                                                    {'loss': 2.3465, 'learning_rate': 0.0009733006903790564, 'epoch': 0.13}
 13%|█▎        | 287/2181 [32:18<3:26:29,  6.54s/it] 13%|█▎        | 288/2181 [32:24<3:27:46,  6.59s/it]                                                    {'loss': 2.3968, 'learning_rate': 0.0009730607193254922, 'epoch': 0.13}
 13%|█▎        | 288/2181 [32:24<3:27:46,  6.59s/it] 13%|█▎        | 289/2181 [32:31<3:27:03,  6.57s/it]                                                    {'loss': 2.2818, 'learning_rate': 0.0009728197045235585, 'epoch': 0.13}
 13%|█▎        | 289/2181 [32:31<3:27:03,  6.57s/it] 13%|█▎        | 290/2181 [32:37<3:25:53,  6.53s/it]                                                    {'loss': 2.3516, 'learning_rate': 0.0009725776465050242, 'epoch': 0.13}
 13%|█▎        | 290/2181 [32:37<3:25:53,  6.53s/it] 13%|█▎        | 291/2181 [32:44<3:27:02,  6.57s/it]                                                    {'loss': 2.3263, 'learning_rate': 0.0009723345458039594, 'epoch': 0.13}
 13%|█▎        | 291/2181 [32:44<3:27:02,  6.57s/it] 13%|█▎        | 292/2181 [32:51<3:28:22,  6.62s/it]                                                    {'loss': 2.3104, 'learning_rate': 0.000972090402956735, 'epoch': 0.13}
 13%|█▎        | 292/2181 [32:51<3:28:22,  6.62s/it] 13%|█▎        | 293/2181 [32:57<3:29:47,  6.67s/it]                                                    {'loss': 2.2677, 'learning_rate': 0.0009718452185020212, 'epoch': 0.13}
 13%|█▎        | 293/2181 [32:57<3:29:47,  6.67s/it] 13%|█▎        | 294/2181 [33:04<3:27:36,  6.60s/it]                                                    {'loss': 2.3473, 'learning_rate': 0.0009715989929807862, 'epoch': 0.13}
 13%|█▎        | 294/2181 [33:04<3:27:36,  6.60s/it] 14%|█▎        | 295/2181 [33:10<3:27:02,  6.59s/it]                                                    {'loss': 2.3112, 'learning_rate': 0.0009713517269362955, 'epoch': 0.14}
 14%|█▎        | 295/2181 [33:10<3:27:02,  6.59s/it] 14%|█▎        | 296/2181 [33:17<3:26:17,  6.57s/it]                                                    {'loss': 2.3174, 'learning_rate': 0.0009711034209141101, 'epoch': 0.14}
 14%|█▎        | 296/2181 [33:17<3:26:17,  6.57s/it] 14%|█▎        | 297/2181 [33:23<3:25:27,  6.54s/it]                                                    {'loss': 2.3455, 'learning_rate': 0.0009708540754620856, 'epoch': 0.14}
 14%|█▎        | 297/2181 [33:23<3:25:27,  6.54s/it] 14%|█▎        | 298/2181 [33:30<3:24:43,  6.52s/it]                                                    {'loss': 2.3609, 'learning_rate': 0.0009706036911303713, 'epoch': 0.14}
 14%|█▎        | 298/2181 [33:30<3:24:43,  6.52s/it] 14%|█▎        | 299/2181 [33:37<3:26:47,  6.59s/it]                                                    {'loss': 2.3032, 'learning_rate': 0.0009703522684714083, 'epoch': 0.14}
 14%|█▎        | 299/2181 [33:37<3:26:47,  6.59s/it] 14%|█▍        | 300/2181 [33:43<3:28:42,  6.66s/it]                                                    {'loss': 2.2706, 'learning_rate': 0.0009700998080399286, 'epoch': 0.14}
 14%|█▍        | 300/2181 [33:43<3:28:42,  6.66s/it] 14%|█▍        | 301/2181 [33:50<3:27:29,  6.62s/it]                                                    {'loss': 2.2739, 'learning_rate': 0.0009698463103929542, 'epoch': 0.14}
 14%|█▍        | 301/2181 [33:50<3:27:29,  6.62s/it] 14%|█▍        | 302/2181 [33:56<3:25:37,  6.57s/it]                                                    {'loss': 2.2972, 'learning_rate': 0.0009695917760897954, 'epoch': 0.14}
 14%|█▍        | 302/2181 [33:56<3:25:37,  6.57s/it] 14%|█▍        | 303/2181 [34:03<3:24:58,  6.55s/it]                                                    {'loss': 2.2955, 'learning_rate': 0.0009693362056920501, 'epoch': 0.14}
 14%|█▍        | 303/2181 [34:03<3:24:58,  6.55s/it] 14%|█▍        | 304/2181 [34:09<3:24:38,  6.54s/it]                                                    {'loss': 2.3075, 'learning_rate': 0.0009690795997636015, 'epoch': 0.14}
 14%|█▍        | 304/2181 [34:09<3:24:38,  6.54s/it] 14%|█▍        | 305/2181 [34:16<3:23:04,  6.50s/it]                                                    {'loss': 2.3301, 'learning_rate': 0.0009688219588706179, 'epoch': 0.14}
 14%|█▍        | 305/2181 [34:16<3:23:04,  6.50s/it] 14%|█▍        | 306/2181 [34:22<3:24:20,  6.54s/it]                                                    {'loss': 2.2403, 'learning_rate': 0.0009685632835815518, 'epoch': 0.14}
 14%|█▍        | 306/2181 [34:22<3:24:20,  6.54s/it] 14%|█▍        | 307/2181 [34:29<3:26:02,  6.60s/it]                                                    {'loss': 2.3016, 'learning_rate': 0.0009683035744671367, 'epoch': 0.14}
 14%|█▍        | 307/2181 [34:29<3:26:02,  6.60s/it] 14%|█▍        | 308/2181 [34:36<3:29:08,  6.70s/it]                                                    {'loss': 2.3104, 'learning_rate': 0.0009680428321003883, 'epoch': 0.14}
 14%|█▍        | 308/2181 [34:36<3:29:08,  6.70s/it] 14%|█▍        | 309/2181 [34:42<3:26:15,  6.61s/it]                                                    {'loss': 2.3042, 'learning_rate': 0.000967781057056601, 'epoch': 0.14}
 14%|█▍        | 309/2181 [34:43<3:26:15,  6.61s/it] 14%|█▍        | 310/2181 [34:49<3:25:04,  6.58s/it]                                                    {'loss': 2.2941, 'learning_rate': 0.0009675182499133485, 'epoch': 0.14}
 14%|█▍        | 310/2181 [34:49<3:25:04,  6.58s/it] 14%|█▍        | 311/2181 [34:55<3:23:50,  6.54s/it]                                                    {'loss': 2.2765, 'learning_rate': 0.0009672544112504813, 'epoch': 0.14}
 14%|█▍        | 311/2181 [34:56<3:23:50,  6.54s/it] 14%|█▍        | 312/2181 [35:02<3:24:11,  6.55s/it]                                                    {'loss': 2.2743, 'learning_rate': 0.0009669895416501257, 'epoch': 0.14}
 14%|█▍        | 312/2181 [35:02<3:24:11,  6.55s/it] 14%|█▍        | 313/2181 [35:09<3:23:39,  6.54s/it]                                                    {'loss': 2.2807, 'learning_rate': 0.0009667236416966833, 'epoch': 0.14}
 14%|█▍        | 313/2181 [35:09<3:23:39,  6.54s/it] 14%|█▍        | 314/2181 [35:15<3:24:59,  6.59s/it]                                                    {'loss': 2.2821, 'learning_rate': 0.0009664567119768281, 'epoch': 0.14}
 14%|█▍        | 314/2181 [35:15<3:24:59,  6.59s/it] 14%|█▍        | 315/2181 [35:22<3:26:50,  6.65s/it]                                                    {'loss': 2.2915, 'learning_rate': 0.0009661887530795067, 'epoch': 0.14}
 14%|█▍        | 315/2181 [35:22<3:26:50,  6.65s/it] 14%|█▍        | 316/2181 [35:29<3:25:12,  6.60s/it]                                                    {'loss': 2.2535, 'learning_rate': 0.0009659197655959365, 'epoch': 0.14}
 14%|█▍        | 316/2181 [35:29<3:25:12,  6.60s/it] 15%|█▍        | 317/2181 [35:35<3:23:26,  6.55s/it]                                                    {'loss': 2.2818, 'learning_rate': 0.000965649750119604, 'epoch': 0.15}
 15%|█▍        | 317/2181 [35:35<3:23:26,  6.55s/it] 15%|█▍        | 318/2181 [35:41<3:22:05,  6.51s/it]                                                    {'loss': 2.3242, 'learning_rate': 0.0009653787072462643, 'epoch': 0.15}
 15%|█▍        | 318/2181 [35:41<3:22:05,  6.51s/it] 15%|█▍        | 319/2181 [35:48<3:21:33,  6.49s/it]                                                    {'loss': 2.3604, 'learning_rate': 0.0009651066375739388, 'epoch': 0.15}
 15%|█▍        | 319/2181 [35:48<3:21:33,  6.49s/it] 15%|█▍        | 320/2181 [35:54<3:21:33,  6.50s/it]                                                    {'loss': 2.2821, 'learning_rate': 0.000964833541702915, 'epoch': 0.15}
 15%|█▍        | 320/2181 [35:54<3:21:33,  6.50s/it] 15%|█▍        | 321/2181 [36:01<3:24:03,  6.58s/it]                                                    {'loss': 2.1933, 'learning_rate': 0.0009645594202357438, 'epoch': 0.15}
 15%|█▍        | 321/2181 [36:01<3:24:03,  6.58s/it] 15%|█▍        | 322/2181 [36:08<3:24:37,  6.60s/it]                                                    {'loss': 2.2407, 'learning_rate': 0.0009642842737772397, 'epoch': 0.15}
 15%|█▍        | 322/2181 [36:08<3:24:37,  6.60s/it] 15%|█▍        | 323/2181 [36:14<3:25:03,  6.62s/it]                                                    {'loss': 2.3135, 'learning_rate': 0.0009640081029344782, 'epoch': 0.15}
 15%|█▍        | 323/2181 [36:14<3:25:03,  6.62s/it] 15%|█▍        | 324/2181 [36:21<3:24:21,  6.60s/it]                                                    {'loss': 2.2614, 'learning_rate': 0.0009637309083167956, 'epoch': 0.15}
 15%|█▍        | 324/2181 [36:21<3:24:21,  6.60s/it] 15%|█▍        | 325/2181 [36:28<3:24:08,  6.60s/it]                                                    {'loss': 2.3739, 'learning_rate': 0.0009634526905357859, 'epoch': 0.15}
 15%|█▍        | 325/2181 [36:28<3:24:08,  6.60s/it] 15%|█▍        | 326/2181 [36:34<3:22:32,  6.55s/it]                                                    {'loss': 2.3155, 'learning_rate': 0.000963173450205302, 'epoch': 0.15}
 15%|█▍        | 326/2181 [36:34<3:22:32,  6.55s/it] 15%|█▍        | 327/2181 [36:41<3:22:21,  6.55s/it]                                                    {'loss': 2.3081, 'learning_rate': 0.0009628931879414517, 'epoch': 0.15}
 15%|█▍        | 327/2181 [36:41<3:22:21,  6.55s/it] 15%|█▌        | 328/2181 [36:47<3:21:53,  6.54s/it]                                                    {'loss': 2.352, 'learning_rate': 0.0009626119043625983, 'epoch': 0.15}
 15%|█▌        | 328/2181 [36:47<3:21:53,  6.54s/it] 15%|█▌        | 329/2181 [36:54<3:22:57,  6.58s/it]                                                    {'loss': 2.2754, 'learning_rate': 0.0009623296000893582, 'epoch': 0.15}
 15%|█▌        | 329/2181 [36:54<3:22:57,  6.58s/it] 15%|█▌        | 330/2181 [37:00<3:23:48,  6.61s/it]                                                    {'loss': 2.3845, 'learning_rate': 0.0009620462757446, 'epoch': 0.15}
 15%|█▌        | 330/2181 [37:00<3:23:48,  6.61s/it] 15%|█▌        | 331/2181 [37:07<3:22:23,  6.56s/it]                                                    {'loss': 2.3643, 'learning_rate': 0.0009617619319534428, 'epoch': 0.15}
 15%|█▌        | 331/2181 [37:07<3:22:23,  6.56s/it] 15%|█▌        | 332/2181 [37:13<3:21:27,  6.54s/it]                                                    {'loss': 2.3474, 'learning_rate': 0.000961476569343255, 'epoch': 0.15}
 15%|█▌        | 332/2181 [37:13<3:21:27,  6.54s/it] 15%|█▌        | 333/2181 [37:20<3:21:53,  6.55s/it]                                                    {'loss': 2.2771, 'learning_rate': 0.0009611901885436529, 'epoch': 0.15}
 15%|█▌        | 333/2181 [37:20<3:21:53,  6.55s/it] 15%|█▌        | 334/2181 [37:26<3:21:17,  6.54s/it]                                                    {'loss': 2.3035, 'learning_rate': 0.0009609027901864996, 'epoch': 0.15}
 15%|█▌        | 334/2181 [37:26<3:21:17,  6.54s/it] 15%|█▌        | 335/2181 [37:33<3:22:53,  6.59s/it]                                                    {'loss': 2.3293, 'learning_rate': 0.0009606143749059029, 'epoch': 0.15}
 15%|█▌        | 335/2181 [37:33<3:22:53,  6.59s/it] 15%|█▌        | 336/2181 [37:40<3:24:04,  6.64s/it]                                                    {'loss': 2.2636, 'learning_rate': 0.0009603249433382144, 'epoch': 0.15}
 15%|█▌        | 336/2181 [37:40<3:24:04,  6.64s/it] 15%|█▌        | 337/2181 [37:47<3:25:11,  6.68s/it]                                                    {'loss': 2.2962, 'learning_rate': 0.0009600344961220282, 'epoch': 0.15}
 15%|█▌        | 337/2181 [37:47<3:25:11,  6.68s/it] 15%|█▌        | 338/2181 [37:53<3:24:41,  6.66s/it]                                                    {'loss': 2.2743, 'learning_rate': 0.0009597430338981791, 'epoch': 0.15}
 15%|█▌        | 338/2181 [37:53<3:24:41,  6.66s/it] 16%|█▌        | 339/2181 [38:00<3:23:23,  6.63s/it]                                                    {'loss': 2.2392, 'learning_rate': 0.0009594505573097414, 'epoch': 0.16}
 16%|█▌        | 339/2181 [38:00<3:23:23,  6.63s/it] 16%|█▌        | 340/2181 [38:06<3:21:10,  6.56s/it]                                                    {'loss': 2.3552, 'learning_rate': 0.0009591570670020277, 'epoch': 0.16}
 16%|█▌        | 340/2181 [38:06<3:21:10,  6.56s/it] 16%|█▌        | 341/2181 [38:13<3:20:06,  6.53s/it]                                                    {'loss': 2.2882, 'learning_rate': 0.0009588625636225871, 'epoch': 0.16}
 16%|█▌        | 341/2181 [38:13<3:20:06,  6.53s/it] 16%|█▌        | 342/2181 [38:19<3:20:31,  6.54s/it]                                                    {'loss': 2.2749, 'learning_rate': 0.0009585670478212036, 'epoch': 0.16}
 16%|█▌        | 342/2181 [38:19<3:20:31,  6.54s/it] 16%|█▌        | 343/2181 [38:26<3:20:46,  6.55s/it]                                                    {'loss': 2.3246, 'learning_rate': 0.0009582705202498956, 'epoch': 0.16}
 16%|█▌        | 343/2181 [38:26<3:20:46,  6.55s/it] 16%|█▌        | 344/2181 [38:33<3:22:01,  6.60s/it]                                                    {'loss': 2.2005, 'learning_rate': 0.0009579729815629133, 'epoch': 0.16}
 16%|█▌        | 344/2181 [38:33<3:22:01,  6.60s/it] 16%|█▌        | 345/2181 [38:39<3:23:14,  6.64s/it]                                                    {'loss': 2.3645, 'learning_rate': 0.0009576744324167379, 'epoch': 0.16}
 16%|█▌        | 345/2181 [38:39<3:23:14,  6.64s/it] 16%|█▌        | 346/2181 [38:46<3:23:07,  6.64s/it]                                                    {'loss': 2.2883, 'learning_rate': 0.0009573748734700804, 'epoch': 0.16}
 16%|█▌        | 346/2181 [38:46<3:23:07,  6.64s/it] 16%|█▌        | 347/2181 [38:52<3:21:35,  6.60s/it]                                                    {'loss': 2.3767, 'learning_rate': 0.0009570743053838796, 'epoch': 0.16}
 16%|█▌        | 347/2181 [38:52<3:21:35,  6.60s/it] 16%|█▌        | 348/2181 [38:59<3:21:03,  6.58s/it]                                                    {'loss': 2.3053, 'learning_rate': 0.0009567727288213005, 'epoch': 0.16}
 16%|█▌        | 348/2181 [38:59<3:21:03,  6.58s/it] 16%|█▌        | 349/2181 [39:05<3:20:15,  6.56s/it]                                                    {'loss': 2.3251, 'learning_rate': 0.0009564701444477337, 'epoch': 0.16}
 16%|█▌        | 349/2181 [39:06<3:20:15,  6.56s/it] 16%|█▌        | 350/2181 [39:12<3:19:47,  6.55s/it]                                                    {'loss': 2.2734, 'learning_rate': 0.000956166552930793, 'epoch': 0.16}
 16%|█▌        | 350/2181 [39:12<3:19:47,  6.55s/it] 16%|█▌        | 351/2181 [39:19<3:20:02,  6.56s/it]                                                    {'loss': 2.2666, 'learning_rate': 0.0009558619549403147, 'epoch': 0.16}
 16%|█▌        | 351/2181 [39:19<3:20:02,  6.56s/it] 16%|█▌        | 352/2181 [39:25<3:22:10,  6.63s/it]                                                    {'loss': 2.2998, 'learning_rate': 0.0009555563511483555, 'epoch': 0.16}
 16%|█▌        | 352/2181 [39:25<3:22:10,  6.63s/it] 16%|█▌        | 353/2181 [39:32<3:22:07,  6.63s/it]                                                    {'loss': 2.2889, 'learning_rate': 0.0009552497422291912, 'epoch': 0.16}
 16%|█▌        | 353/2181 [39:32<3:22:07,  6.63s/it] 16%|█▌        | 354/2181 [39:39<3:21:10,  6.61s/it]                                                    {'loss': 2.3375, 'learning_rate': 0.0009549421288593157, 'epoch': 0.16}
 16%|█▌        | 354/2181 [39:39<3:21:10,  6.61s/it] 16%|█▋        | 355/2181 [39:45<3:20:30,  6.59s/it]                                                    {'loss': 2.2783, 'learning_rate': 0.0009546335117174385, 'epoch': 0.16}
 16%|█▋        | 355/2181 [39:45<3:20:30,  6.59s/it] 16%|█▋        | 356/2181 [39:52<3:19:46,  6.57s/it]                                                    {'loss': 2.2818, 'learning_rate': 0.0009543238914844843, 'epoch': 0.16}
 16%|█▋        | 356/2181 [39:52<3:19:46,  6.57s/it] 16%|█▋        | 357/2181 [39:58<3:19:07,  6.55s/it]                                                    {'loss': 2.1977, 'learning_rate': 0.0009540132688435907, 'epoch': 0.16}
 16%|█▋        | 357/2181 [39:58<3:19:07,  6.55s/it] 16%|█▋        | 358/2181 [40:05<3:18:06,  6.52s/it]                                                    {'loss': 2.3098, 'learning_rate': 0.0009537016444801074, 'epoch': 0.16}
 16%|█▋        | 358/2181 [40:05<3:18:06,  6.52s/it] 16%|█▋        | 359/2181 [40:11<3:19:34,  6.57s/it]                                                    {'loss': 2.2728, 'learning_rate': 0.0009533890190815935, 'epoch': 0.16}
 16%|█▋        | 359/2181 [40:11<3:19:34,  6.57s/it] 17%|█▋        | 360/2181 [40:18<3:19:54,  6.59s/it]                                                    {'loss': 2.2903, 'learning_rate': 0.0009530753933378173, 'epoch': 0.17}
 17%|█▋        | 360/2181 [40:18<3:19:54,  6.59s/it] 17%|█▋        | 361/2181 [40:25<3:20:09,  6.60s/it]                                                    {'loss': 2.2422, 'learning_rate': 0.0009527607679407545, 'epoch': 0.17}
 17%|█▋        | 361/2181 [40:25<3:20:09,  6.60s/it] 17%|█▋        | 362/2181 [40:31<3:19:24,  6.58s/it]                                                    {'loss': 2.2379, 'learning_rate': 0.0009524451435845857, 'epoch': 0.17}
 17%|█▋        | 362/2181 [40:31<3:19:24,  6.58s/it] 17%|█▋        | 363/2181 [40:38<3:18:26,  6.55s/it]                                                    {'loss': 2.2612, 'learning_rate': 0.0009521285209656963, 'epoch': 0.17}
 17%|█▋        | 363/2181 [40:38<3:18:26,  6.55s/it] 17%|█▋        | 364/2181 [40:44<3:17:44,  6.53s/it]                                                    {'loss': 2.2348, 'learning_rate': 0.0009518109007826734, 'epoch': 0.17}
 17%|█▋        | 364/2181 [40:44<3:17:44,  6.53s/it] 17%|█▋        | 365/2181 [40:50<3:16:55,  6.51s/it]                                                    {'loss': 2.283, 'learning_rate': 0.0009514922837363059, 'epoch': 0.17}
 17%|█▋        | 365/2181 [40:51<3:16:55,  6.51s/it] 17%|█▋        | 366/2181 [40:57<3:16:46,  6.51s/it]                                                    {'loss': 2.2948, 'learning_rate': 0.0009511726705295817, 'epoch': 0.17}
 17%|█▋        | 366/2181 [40:57<3:16:46,  6.51s/it] 17%|█▋        | 367/2181 [41:04<3:18:05,  6.55s/it]                                                    {'loss': 2.3053, 'learning_rate': 0.000950852061867687, 'epoch': 0.17}
 17%|█▋        | 367/2181 [41:04<3:18:05,  6.55s/it] 17%|█▋        | 368/2181 [41:10<3:19:12,  6.59s/it]                                                    {'loss': 2.1926, 'learning_rate': 0.0009505304584580038, 'epoch': 0.17}
 17%|█▋        | 368/2181 [41:10<3:19:12,  6.59s/it] 17%|█▋        | 369/2181 [41:17<3:18:04,  6.56s/it]                                                    {'loss': 2.3284, 'learning_rate': 0.0009502078610101092, 'epoch': 0.17}
 17%|█▋        | 369/2181 [41:17<3:18:04,  6.56s/it] 17%|█▋        | 370/2181 [41:23<3:17:32,  6.54s/it]                                                    {'loss': 2.2485, 'learning_rate': 0.0009498842702357736, 'epoch': 0.17}
 17%|█▋        | 370/2181 [41:23<3:17:32,  6.54s/it] 17%|█▋        | 371/2181 [41:30<3:16:39,  6.52s/it]                                                    {'loss': 2.2656, 'learning_rate': 0.0009495596868489587, 'epoch': 0.17}
 17%|█▋        | 371/2181 [41:30<3:16:39,  6.52s/it] 17%|█▋        | 372/2181 [41:36<3:16:01,  6.50s/it]                                                    {'loss': 2.2229, 'learning_rate': 0.0009492341115658167, 'epoch': 0.17}
 17%|█▋        | 372/2181 [41:36<3:16:01,  6.50s/it] 17%|█▋        | 373/2181 [41:43<3:15:50,  6.50s/it]                                                    {'loss': 2.2547, 'learning_rate': 0.0009489075451046879, 'epoch': 0.17}
 17%|█▋        | 373/2181 [41:43<3:15:50,  6.50s/it] 17%|█▋        | 374/2181 [41:50<3:18:21,  6.59s/it]                                                    {'loss': 2.2848, 'learning_rate': 0.0009485799881861, 'epoch': 0.17}
 17%|█▋        | 374/2181 [41:50<3:18:21,  6.59s/it] 17%|█▋        | 375/2181 [41:56<3:19:00,  6.61s/it]                                                    {'loss': 2.3796, 'learning_rate': 0.0009482514415327654, 'epoch': 0.17}
 17%|█▋        | 375/2181 [41:56<3:19:00,  6.61s/it] 17%|█▋        | 376/2181 [42:03<3:18:45,  6.61s/it]                                                    {'loss': 2.2395, 'learning_rate': 0.000947921905869581, 'epoch': 0.17}
 17%|█▋        | 376/2181 [42:03<3:18:45,  6.61s/it] 17%|█▋        | 377/2181 [42:09<3:18:43,  6.61s/it]                                                    {'loss': 2.2405, 'learning_rate': 0.0009475913819236248, 'epoch': 0.17}
 17%|█▋        | 377/2181 [42:09<3:18:43,  6.61s/it] 17%|█▋        | 378/2181 [42:16<3:17:14,  6.56s/it]                                                    {'loss': 2.275, 'learning_rate': 0.0009472598704241561, 'epoch': 0.17}
 17%|█▋        | 378/2181 [42:16<3:17:14,  6.56s/it] 17%|█▋        | 379/2181 [42:22<3:16:33,  6.54s/it]                                                    {'loss': 2.3033, 'learning_rate': 0.0009469273721026131, 'epoch': 0.17}
 17%|█▋        | 379/2181 [42:22<3:16:33,  6.54s/it] 17%|█▋        | 380/2181 [42:29<3:16:34,  6.55s/it]                                                    {'loss': 2.2101, 'learning_rate': 0.0009465938876926111, 'epoch': 0.17}
 17%|█▋        | 380/2181 [42:29<3:16:34,  6.55s/it] 17%|█▋        | 381/2181 [42:35<3:16:15,  6.54s/it]                                                    {'loss': 2.2038, 'learning_rate': 0.0009462594179299406, 'epoch': 0.17}
 17%|█▋        | 381/2181 [42:36<3:16:15,  6.54s/it] 18%|█▊        | 382/2181 [42:42<3:17:36,  6.59s/it]                                                    {'loss': 2.2385, 'learning_rate': 0.0009459239635525672, 'epoch': 0.18}
 18%|█▊        | 382/2181 [42:42<3:17:36,  6.59s/it] 18%|█▊        | 383/2181 [42:49<3:19:03,  6.64s/it]                                                    {'loss': 2.3556, 'learning_rate': 0.0009455875253006281, 'epoch': 0.18}
 18%|█▊        | 383/2181 [42:49<3:19:03,  6.64s/it] 18%|█▊        | 384/2181 [42:55<3:16:59,  6.58s/it]                                                    {'loss': 2.3315, 'learning_rate': 0.0009452501039164315, 'epoch': 0.18}
 18%|█▊        | 384/2181 [42:55<3:16:59,  6.58s/it] 18%|█▊        | 385/2181 [43:02<3:15:46,  6.54s/it]                                                    {'loss': 2.2748, 'learning_rate': 0.0009449117001444549, 'epoch': 0.18}
 18%|█▊        | 385/2181 [43:02<3:15:46,  6.54s/it] 18%|█▊        | 386/2181 [43:08<3:15:00,  6.52s/it]                                                    {'loss': 2.24, 'learning_rate': 0.0009445723147313433, 'epoch': 0.18}
 18%|█▊        | 386/2181 [43:08<3:15:00,  6.52s/it] 18%|█▊        | 387/2181 [43:15<3:14:56,  6.52s/it]                                                    {'loss': 2.1774, 'learning_rate': 0.0009442319484259074, 'epoch': 0.18}
 18%|█▊        | 387/2181 [43:15<3:14:56,  6.52s/it] 18%|█▊        | 388/2181 [43:21<3:14:47,  6.52s/it]                                                    {'loss': 2.2659, 'learning_rate': 0.0009438906019791222, 'epoch': 0.18}
 18%|█▊        | 388/2181 [43:21<3:14:47,  6.52s/it] 18%|█▊        | 389/2181 [43:28<3:17:26,  6.61s/it]                                                    {'loss': 2.2894, 'learning_rate': 0.0009435482761441251, 'epoch': 0.18}
 18%|█▊        | 389/2181 [43:28<3:17:26,  6.61s/it] 18%|█▊        | 390/2181 [43:35<3:17:44,  6.62s/it]                                                    {'loss': 2.2451, 'learning_rate': 0.000943204971676215, 'epoch': 0.18}
 18%|█▊        | 390/2181 [43:35<3:17:44,  6.62s/it] 18%|█▊        | 391/2181 [43:41<3:17:14,  6.61s/it]                                                    {'loss': 2.2051, 'learning_rate': 0.0009428606893328493, 'epoch': 0.18}
 18%|█▊        | 391/2181 [43:41<3:17:14,  6.61s/it] 18%|█▊        | 392/2181 [43:48<3:15:52,  6.57s/it]                                                    {'loss': 2.3124, 'learning_rate': 0.0009425154298736432, 'epoch': 0.18}
 18%|█▊        | 392/2181 [43:48<3:15:52,  6.57s/it] 18%|█▊        | 393/2181 [43:54<3:14:10,  6.52s/it]                                                    {'loss': 2.3619, 'learning_rate': 0.0009421691940603678, 'epoch': 0.18}
 18%|█▊        | 393/2181 [43:54<3:14:10,  6.52s/it] 18%|█▊        | 394/2181 [44:01<3:13:43,  6.50s/it]                                                    {'loss': 2.1767, 'learning_rate': 0.0009418219826569488, 'epoch': 0.18}
 18%|█▊        | 394/2181 [44:01<3:13:43,  6.50s/it] 18%|█▊        | 395/2181 [44:07<3:13:45,  6.51s/it]                                                    {'loss': 2.3151, 'learning_rate': 0.0009414737964294635, 'epoch': 0.18}
 18%|█▊        | 395/2181 [44:07<3:13:45,  6.51s/it] 18%|█▊        | 396/2181 [44:14<3:14:04,  6.52s/it]                                                    {'loss': 2.2561, 'learning_rate': 0.000941124636146141, 'epoch': 0.18}
 18%|█▊        | 396/2181 [44:14<3:14:04,  6.52s/it] 18%|█▊        | 397/2181 [44:20<3:15:05,  6.56s/it]                                                    {'loss': 2.1891, 'learning_rate': 0.0009407745025773589, 'epoch': 0.18}
 18%|█▊        | 397/2181 [44:21<3:15:05,  6.56s/it] 18%|█▊        | 398/2181 [44:27<3:16:38,  6.62s/it]                                                    {'loss': 2.312, 'learning_rate': 0.0009404233964956423, 'epoch': 0.18}
 18%|█▊        | 398/2181 [44:27<3:16:38,  6.62s/it] 18%|█▊        | 399/2181 [44:34<3:16:48,  6.63s/it]                                                    {'loss': 2.2374, 'learning_rate': 0.0009400713186756625, 'epoch': 0.18}
 18%|█▊        | 399/2181 [44:34<3:16:48,  6.63s/it] 18%|█▊        | 400/2181 [44:40<3:16:18,  6.61s/it]                                                    {'loss': 2.2515, 'learning_rate': 0.0009397182698942342, 'epoch': 0.18}
 18%|█▊        | 400/2181 [44:40<3:16:18,  6.61s/it] 18%|█▊        | 401/2181 [44:47<3:14:16,  6.55s/it]                                                    {'loss': 2.3798, 'learning_rate': 0.0009393642509303149, 'epoch': 0.18}
 18%|█▊        | 401/2181 [44:47<3:14:16,  6.55s/it] 18%|█▊        | 402/2181 [44:53<3:13:07,  6.51s/it]                                                    {'loss': 2.2554, 'learning_rate': 0.0009390092625650023, 'epoch': 0.18}
 18%|█▊        | 402/2181 [44:53<3:13:07,  6.51s/it] 18%|█▊        | 403/2181 [45:00<3:12:47,  6.51s/it]                                                    {'loss': 2.2102, 'learning_rate': 0.0009386533055815332, 'epoch': 0.18}
 18%|█▊        | 403/2181 [45:00<3:12:47,  6.51s/it] 19%|█▊        | 404/2181 [45:06<3:14:28,  6.57s/it]                                                    {'loss': 2.2565, 'learning_rate': 0.0009382963807652813, 'epoch': 0.19}
 19%|█▊        | 404/2181 [45:07<3:14:28,  6.57s/it] 19%|█▊        | 405/2181 [45:13<3:15:37,  6.61s/it]                                                    {'loss': 2.3108, 'learning_rate': 0.000937938488903756, 'epoch': 0.19}
 19%|█▊        | 405/2181 [45:13<3:15:37,  6.61s/it] 19%|█▊        | 406/2181 [45:20<3:14:48,  6.59s/it]                                                    {'loss': 2.2324, 'learning_rate': 0.0009375796307866003, 'epoch': 0.19}
 19%|█▊        | 406/2181 [45:20<3:14:48,  6.59s/it] 19%|█▊        | 407/2181 [45:26<3:13:24,  6.54s/it]                                                    {'loss': 2.2088, 'learning_rate': 0.0009372198072055888, 'epoch': 0.19}
 19%|█▊        | 407/2181 [45:26<3:13:24,  6.54s/it] 19%|█▊        | 408/2181 [45:33<3:12:22,  6.51s/it]                                                    {'loss': 2.3422, 'learning_rate': 0.0009368590189546268, 'epoch': 0.19}
 19%|█▊        | 408/2181 [45:33<3:12:22,  6.51s/it] 19%|█▉        | 409/2181 [45:39<3:12:08,  6.51s/it]                                                    {'loss': 2.2806, 'learning_rate': 0.0009364972668297474, 'epoch': 0.19}
 19%|█▉        | 409/2181 [45:39<3:12:08,  6.51s/it] 19%|█▉        | 410/2181 [45:46<3:12:30,  6.52s/it]                                                    {'loss': 2.242, 'learning_rate': 0.0009361345516291111, 'epoch': 0.19}
 19%|█▉        | 410/2181 [45:46<3:12:30,  6.52s/it] 19%|█▉        | 411/2181 [45:52<3:12:39,  6.53s/it]                                                    {'loss': 2.2459, 'learning_rate': 0.0009357708741530024, 'epoch': 0.19}
 19%|█▉        | 411/2181 [45:52<3:12:39,  6.53s/it] 19%|█▉        | 412/2181 [45:59<3:14:28,  6.60s/it]                                                    {'loss': 2.2249, 'learning_rate': 0.00093540623520383, 'epoch': 0.19}
 19%|█▉        | 412/2181 [45:59<3:14:28,  6.60s/it] 19%|█▉        | 413/2181 [46:06<3:15:38,  6.64s/it]                                                    {'loss': 2.2669, 'learning_rate': 0.000935040635586123, 'epoch': 0.19}
 19%|█▉        | 413/2181 [46:06<3:15:38,  6.64s/it] 19%|█▉        | 414/2181 [46:12<3:13:54,  6.58s/it]                                                    {'loss': 2.2566, 'learning_rate': 0.0009346740761065305, 'epoch': 0.19}
 19%|█▉        | 414/2181 [46:12<3:13:54,  6.58s/it] 19%|█▉        | 415/2181 [46:19<3:12:35,  6.54s/it]                                                    {'loss': 2.2286, 'learning_rate': 0.0009343065575738197, 'epoch': 0.19}
 19%|█▉        | 415/2181 [46:19<3:12:35,  6.54s/it] 19%|█▉        | 416/2181 [46:25<3:11:25,  6.51s/it]                                                    {'loss': 2.2615, 'learning_rate': 0.0009339380807988733, 'epoch': 0.19}
 19%|█▉        | 416/2181 [46:25<3:11:25,  6.51s/it] 19%|█▉        | 417/2181 [46:31<3:11:09,  6.50s/it]                                                    {'loss': 2.3701, 'learning_rate': 0.0009335686465946887, 'epoch': 0.19}
 19%|█▉        | 417/2181 [46:32<3:11:09,  6.50s/it] 19%|█▉        | 418/2181 [46:38<3:09:49,  6.46s/it]                                                    {'loss': 2.2032, 'learning_rate': 0.0009331982557763754, 'epoch': 0.19}
 19%|█▉        | 418/2181 [46:38<3:09:49,  6.46s/it] 19%|█▉        | 419/2181 [46:44<3:11:39,  6.53s/it]                                                    {'loss': 2.2908, 'learning_rate': 0.0009328269091611537, 'epoch': 0.19}
 19%|█▉        | 419/2181 [46:45<3:11:39,  6.53s/it] 19%|█▉        | 420/2181 [46:51<3:13:44,  6.60s/it]                                                    {'loss': 2.2701, 'learning_rate': 0.0009324546075683524, 'epoch': 0.19}
 19%|█▉        | 420/2181 [46:51<3:13:44,  6.60s/it] 19%|█▉        | 421/2181 [46:58<3:14:29,  6.63s/it]                                                    {'loss': 2.1663, 'learning_rate': 0.0009320813518194083, 'epoch': 0.19}
 19%|█▉        | 421/2181 [46:58<3:14:29,  6.63s/it] 19%|█▉        | 422/2181 [47:04<3:12:36,  6.57s/it]                                                    {'loss': 2.1962, 'learning_rate': 0.0009317071427378624, 'epoch': 0.19}
 19%|█▉        | 422/2181 [47:05<3:12:36,  6.57s/it] 19%|█▉        | 423/2181 [47:11<3:11:00,  6.52s/it]                                                    {'loss': 2.319, 'learning_rate': 0.0009313319811493594, 'epoch': 0.19}
 19%|█▉        | 423/2181 [47:11<3:11:00,  6.52s/it] 19%|█▉        | 424/2181 [47:17<3:11:05,  6.53s/it]                                                    {'loss': 2.2096, 'learning_rate': 0.000930955867881646, 'epoch': 0.19}
 19%|█▉        | 424/2181 [47:17<3:11:05,  6.53s/it] 19%|█▉        | 425/2181 [47:24<3:09:30,  6.48s/it]                                                    {'loss': 2.2817, 'learning_rate': 0.0009305788037645681, 'epoch': 0.19}
 19%|█▉        | 425/2181 [47:24<3:09:30,  6.48s/it] 20%|█▉        | 426/2181 [47:30<3:11:05,  6.53s/it]                                                    {'loss': 2.229, 'learning_rate': 0.0009302007896300697, 'epoch': 0.2}
 20%|█▉        | 426/2181 [47:30<3:11:05,  6.53s/it] 20%|█▉        | 427/2181 [47:37<3:12:07,  6.57s/it]                                                    {'loss': 2.2516, 'learning_rate': 0.0009298218263121911, 'epoch': 0.2}
 20%|█▉        | 427/2181 [47:37<3:12:07,  6.57s/it] 20%|█▉        | 428/2181 [47:44<3:13:32,  6.62s/it]                                                    {'loss': 2.2419, 'learning_rate': 0.0009294419146470668, 'epoch': 0.2}
 20%|█▉        | 428/2181 [47:44<3:13:32,  6.62s/it] 20%|█▉        | 429/2181 [47:50<3:12:14,  6.58s/it]                                                    {'loss': 2.2594, 'learning_rate': 0.0009290610554729234, 'epoch': 0.2}
 20%|█▉        | 429/2181 [47:50<3:12:14,  6.58s/it] 20%|█▉        | 430/2181 [47:57<3:11:35,  6.56s/it]                                                    {'loss': 2.2742, 'learning_rate': 0.0009286792496300784, 'epoch': 0.2}
 20%|█▉        | 430/2181 [47:57<3:11:35,  6.56s/it] 20%|█▉        | 431/2181 [48:03<3:11:09,  6.55s/it]                                                    {'loss': 2.3006, 'learning_rate': 0.0009282964979609379, 'epoch': 0.2}
 20%|█▉        | 431/2181 [48:03<3:11:09,  6.55s/it] 20%|█▉        | 432/2181 [48:10<3:10:16,  6.53s/it]                                                    {'loss': 2.2307, 'learning_rate': 0.0009279128013099947, 'epoch': 0.2}
 20%|█▉        | 432/2181 [48:10<3:10:16,  6.53s/it] 20%|█▉        | 433/2181 [48:16<3:09:55,  6.52s/it]                                                    {'loss': 2.263, 'learning_rate': 0.0009275281605238268, 'epoch': 0.2}
 20%|█▉        | 433/2181 [48:16<3:09:55,  6.52s/it] 20%|█▉        | 434/2181 [48:23<3:11:20,  6.57s/it]                                                    {'loss': 2.222, 'learning_rate': 0.0009271425764510953, 'epoch': 0.2}
 20%|█▉        | 434/2181 [48:23<3:11:20,  6.57s/it] 20%|█▉        | 435/2181 [48:30<3:13:14,  6.64s/it]                                                    {'loss': 2.2446, 'learning_rate': 0.0009267560499425423, 'epoch': 0.2}
 20%|█▉        | 435/2181 [48:30<3:13:14,  6.64s/it] 20%|█▉        | 436/2181 [48:36<3:12:50,  6.63s/it]                                                    {'loss': 2.2098, 'learning_rate': 0.0009263685818509895, 'epoch': 0.2}
 20%|█▉        | 436/2181 [48:36<3:12:50,  6.63s/it] 20%|██        | 437/2181 [48:43<3:11:14,  6.58s/it]                                                    {'loss': 2.2371, 'learning_rate': 0.000925980173031336, 'epoch': 0.2}
 20%|██        | 437/2181 [48:43<3:11:14,  6.58s/it] 20%|██        | 438/2181 [48:49<3:11:27,  6.59s/it]                                                    {'loss': 2.2068, 'learning_rate': 0.0009255908243405567, 'epoch': 0.2}
 20%|██        | 438/2181 [48:50<3:11:27,  6.59s/it] 20%|██        | 439/2181 [48:56<3:10:13,  6.55s/it]                                                    {'loss': 2.3119, 'learning_rate': 0.0009252005366376996, 'epoch': 0.2}
 20%|██        | 439/2181 [48:56<3:10:13,  6.55s/it] 20%|██        | 440/2181 [49:02<3:09:15,  6.52s/it]                                                    {'loss': 2.1997, 'learning_rate': 0.0009248093107838852, 'epoch': 0.2}
 20%|██        | 440/2181 [49:02<3:09:15,  6.52s/it] 20%|██        | 441/2181 [49:09<3:09:12,  6.52s/it]                                                    {'loss': 2.2744, 'learning_rate': 0.0009244171476423036, 'epoch': 0.2}
 20%|██        | 441/2181 [49:09<3:09:12,  6.52s/it] 20%|██        | 442/2181 [49:16<3:10:58,  6.59s/it]                                                    {'loss': 2.2304, 'learning_rate': 0.0009240240480782129, 'epoch': 0.2}
 20%|██        | 442/2181 [49:16<3:10:58,  6.59s/it] 20%|██        | 443/2181 [49:23<3:13:29,  6.68s/it]                                                    {'loss': 2.2814, 'learning_rate': 0.0009236300129589376, 'epoch': 0.2}
 20%|██        | 443/2181 [49:23<3:13:29,  6.68s/it] 20%|██        | 444/2181 [49:29<3:10:40,  6.59s/it]                                                    {'loss': 2.2548, 'learning_rate': 0.0009232350431538657, 'epoch': 0.2}
 20%|██        | 444/2181 [49:29<3:10:40,  6.59s/it] 20%|██        | 445/2181 [49:35<3:09:25,  6.55s/it]                                                    {'loss': 2.2923, 'learning_rate': 0.0009228391395344482, 'epoch': 0.2}
 20%|██        | 445/2181 [49:35<3:09:25,  6.55s/it] 20%|██        | 446/2181 [49:42<3:08:00,  6.50s/it]                                                    {'loss': 2.2741, 'learning_rate': 0.000922442302974196, 'epoch': 0.2}
 20%|██        | 446/2181 [49:42<3:08:00,  6.50s/it] 20%|██        | 447/2181 [49:48<3:08:42,  6.53s/it]                                                    {'loss': 2.3016, 'learning_rate': 0.0009220445343486785, 'epoch': 0.2}
 20%|██        | 447/2181 [49:48<3:08:42,  6.53s/it] 21%|██        | 448/2181 [49:55<3:07:39,  6.50s/it]                                                    {'loss': 2.2497, 'learning_rate': 0.0009216458345355217, 'epoch': 0.21}
 21%|██        | 448/2181 [49:55<3:07:39,  6.50s/it] 21%|██        | 449/2181 [50:02<3:10:28,  6.60s/it]                                                    {'loss': 2.1717, 'learning_rate': 0.0009212462044144061, 'epoch': 0.21}
 21%|██        | 449/2181 [50:02<3:10:28,  6.60s/it] 21%|██        | 450/2181 [50:08<3:12:28,  6.67s/it]                                                    {'loss': 2.1603, 'learning_rate': 0.0009208456448670648, 'epoch': 0.21}
 21%|██        | 450/2181 [50:09<3:12:28,  6.67s/it] 21%|██        | 451/2181 [50:15<3:10:44,  6.62s/it]                                                    {'loss': 2.2778, 'learning_rate': 0.0009204441567772816, 'epoch': 0.21}
 21%|██        | 451/2181 [50:15<3:10:44,  6.62s/it] 21%|██        | 452/2181 [50:21<3:09:44,  6.58s/it]                                                    {'loss': 2.2641, 'learning_rate': 0.0009200417410308888, 'epoch': 0.21}
 21%|██        | 452/2181 [50:22<3:09:44,  6.58s/it] 21%|██        | 453/2181 [50:28<3:08:55,  6.56s/it]                                                    {'loss': 2.235, 'learning_rate': 0.0009196383985157656, 'epoch': 0.21}
 21%|██        | 453/2181 [50:28<3:08:55,  6.56s/it] 21%|██        | 454/2181 [50:34<3:08:37,  6.55s/it]                                                    {'loss': 2.2577, 'learning_rate': 0.000919234130121836, 'epoch': 0.21}
 21%|██        | 454/2181 [50:35<3:08:37,  6.55s/it] 21%|██        | 455/2181 [50:41<3:07:56,  6.53s/it]                                                    {'loss': 2.1678, 'learning_rate': 0.0009188289367410672, 'epoch': 0.21}
 21%|██        | 455/2181 [50:41<3:07:56,  6.53s/it] 21%|██        | 456/2181 [50:48<3:09:32,  6.59s/it]                                                    {'loss': 2.1817, 'learning_rate': 0.0009184228192674666, 'epoch': 0.21}
 21%|██        | 456/2181 [50:48<3:09:32,  6.59s/it] 21%|██        | 457/2181 [50:55<3:12:33,  6.70s/it]                                                    {'loss': 2.257, 'learning_rate': 0.0009180157785970808, 'epoch': 0.21}
 21%|██        | 457/2181 [50:55<3:12:33,  6.70s/it] 21%|██        | 458/2181 [51:01<3:10:26,  6.63s/it]                                                    {'loss': 2.2475, 'learning_rate': 0.0009176078156279932, 'epoch': 0.21}
 21%|██        | 458/2181 [51:01<3:10:26,  6.63s/it] 21%|██        | 459/2181 [51:08<3:08:49,  6.58s/it]                                                    {'loss': 2.2653, 'learning_rate': 0.0009171989312603226, 'epoch': 0.21}
 21%|██        | 459/2181 [51:08<3:08:49,  6.58s/it] 21%|██        | 460/2181 [51:14<3:07:47,  6.55s/it]                                                    {'loss': 2.2012, 'learning_rate': 0.0009167891263962202, 'epoch': 0.21}
 21%|██        | 460/2181 [51:14<3:07:47,  6.55s/it] 21%|██        | 461/2181 [51:21<3:07:29,  6.54s/it]                                                    {'loss': 2.3452, 'learning_rate': 0.0009163784019398685, 'epoch': 0.21}
 21%|██        | 461/2181 [51:21<3:07:29,  6.54s/it] 21%|██        | 462/2181 [51:27<3:07:01,  6.53s/it]                                                    {'loss': 2.1763, 'learning_rate': 0.0009159667587974785, 'epoch': 0.21}
 21%|██        | 462/2181 [51:27<3:07:01,  6.53s/it] 21%|██        | 463/2181 [51:34<3:08:38,  6.59s/it]                                                    {'loss': 2.1774, 'learning_rate': 0.0009155541978772887, 'epoch': 0.21}
 21%|██        | 463/2181 [51:34<3:08:38,  6.59s/it] 21%|██▏       | 464/2181 [51:41<3:10:03,  6.64s/it]                                                    {'loss': 2.2315, 'learning_rate': 0.0009151407200895625, 'epoch': 0.21}
 21%|██▏       | 464/2181 [51:41<3:10:03,  6.64s/it] 21%|██▏       | 465/2181 [51:47<3:10:09,  6.65s/it]                                                    {'loss': 2.2371, 'learning_rate': 0.000914726326346586, 'epoch': 0.21}
 21%|██▏       | 465/2181 [51:47<3:10:09,  6.65s/it] 21%|██▏       | 466/2181 [51:54<3:08:38,  6.60s/it]                                                    {'loss': 2.2232, 'learning_rate': 0.0009143110175626661, 'epoch': 0.21}
 21%|██▏       | 466/2181 [51:54<3:08:38,  6.60s/it] 21%|██▏       | 467/2181 [52:00<3:06:46,  6.54s/it]                                                    {'loss': 2.2315, 'learning_rate': 0.0009138947946541291, 'epoch': 0.21}
 21%|██▏       | 467/2181 [52:00<3:06:46,  6.54s/it] 21%|██▏       | 468/2181 [52:07<3:06:08,  6.52s/it]                                                    {'loss': 2.1142, 'learning_rate': 0.0009134776585393181, 'epoch': 0.21}
 21%|██▏       | 468/2181 [52:07<3:06:08,  6.52s/it] 22%|██▏       | 469/2181 [52:13<3:05:20,  6.50s/it]                                                    {'loss': 2.3015, 'learning_rate': 0.0009130596101385906, 'epoch': 0.22}
 22%|██▏       | 469/2181 [52:13<3:05:20,  6.50s/it] 22%|██▏       | 470/2181 [52:20<3:05:44,  6.51s/it]                                                    {'loss': 2.2581, 'learning_rate': 0.0009126406503743174, 'epoch': 0.22}
 22%|██▏       | 470/2181 [52:20<3:05:44,  6.51s/it] 22%|██▏       | 471/2181 [52:27<3:09:06,  6.64s/it]                                                    {'loss': 2.1715, 'learning_rate': 0.0009122207801708802, 'epoch': 0.22}
 22%|██▏       | 471/2181 [52:27<3:09:06,  6.64s/it] 22%|██▏       | 472/2181 [52:34<3:12:32,  6.76s/it]                                                    {'loss': 2.1685, 'learning_rate': 0.0009118000004546689, 'epoch': 0.22}
 22%|██▏       | 472/2181 [52:34<3:12:32,  6.76s/it] 22%|██▏       | 473/2181 [52:40<3:09:22,  6.65s/it]                                                    {'loss': 2.2208, 'learning_rate': 0.0009113783121540807, 'epoch': 0.22}
 22%|██▏       | 473/2181 [52:40<3:09:22,  6.65s/it] 22%|██▏       | 474/2181 [52:46<3:07:51,  6.60s/it]                                                    {'loss': 2.2813, 'learning_rate': 0.0009109557161995172, 'epoch': 0.22}
 22%|██▏       | 474/2181 [52:47<3:07:51,  6.60s/it] 22%|██▏       | 475/2181 [52:53<3:06:21,  6.55s/it]                                                    {'loss': 2.1743, 'learning_rate': 0.0009105322135233828, 'epoch': 0.22}
 22%|██▏       | 475/2181 [52:53<3:06:21,  6.55s/it] 22%|██▏       | 476/2181 [52:59<3:05:32,  6.53s/it]                                                    {'loss': 2.2962, 'learning_rate': 0.0009101078050600821, 'epoch': 0.22}
 22%|██▏       | 476/2181 [52:59<3:05:32,  6.53s/it] 22%|██▏       | 477/2181 [53:06<3:04:41,  6.50s/it]                                                    {'loss': 2.2196, 'learning_rate': 0.0009096824917460186, 'epoch': 0.22}
 22%|██▏       | 477/2181 [53:06<3:04:41,  6.50s/it] 22%|██▏       | 478/2181 [53:12<3:05:54,  6.55s/it]                                                    {'loss': 2.2772, 'learning_rate': 0.0009092562745195921, 'epoch': 0.22}
 22%|██▏       | 478/2181 [53:13<3:05:54,  6.55s/it] 22%|██▏       | 479/2181 [53:19<3:07:49,  6.62s/it]                                                    {'loss': 2.1917, 'learning_rate': 0.0009088291543211967, 'epoch': 0.22}
 22%|██▏       | 479/2181 [53:19<3:07:49,  6.62s/it] 22%|██▏       | 480/2181 [53:26<3:08:01,  6.63s/it]                                                    {'loss': 2.1962, 'learning_rate': 0.0009084011320932188, 'epoch': 0.22}
 22%|██▏       | 480/2181 [53:26<3:08:01,  6.63s/it] 22%|██▏       | 481/2181 [53:32<3:06:25,  6.58s/it]                                                    {'loss': 2.1358, 'learning_rate': 0.0009079722087800352, 'epoch': 0.22}
 22%|██▏       | 481/2181 [53:32<3:06:25,  6.58s/it] 22%|██▏       | 482/2181 [53:39<3:05:43,  6.56s/it]                                                    {'loss': 2.2099, 'learning_rate': 0.0009075423853280106, 'epoch': 0.22}
 22%|██▏       | 482/2181 [53:39<3:05:43,  6.56s/it] 22%|██▏       | 483/2181 [53:45<3:04:14,  6.51s/it]                                                    {'loss': 2.3062, 'learning_rate': 0.0009071116626854958, 'epoch': 0.22}
 22%|██▏       | 483/2181 [53:45<3:04:14,  6.51s/it] 22%|██▏       | 484/2181 [53:52<3:04:18,  6.52s/it]                                                    {'loss': 2.1603, 'learning_rate': 0.0009066800418028256, 'epoch': 0.22}
 22%|██▏       | 484/2181 [53:52<3:04:18,  6.52s/it] 22%|██▏       | 485/2181 [53:58<3:03:39,  6.50s/it]                                                    {'loss': 2.1963, 'learning_rate': 0.0009062475236323168, 'epoch': 0.22}
 22%|██▏       | 485/2181 [53:58<3:03:39,  6.50s/it] 22%|██▏       | 486/2181 [54:05<3:05:40,  6.57s/it]                                                    {'loss': 2.1551, 'learning_rate': 0.0009058141091282656, 'epoch': 0.22}
 22%|██▏       | 486/2181 [54:05<3:05:40,  6.57s/it] 22%|██▏       | 487/2181 [54:12<3:06:28,  6.60s/it]                                                    {'loss': 2.3304, 'learning_rate': 0.0009053797992469461, 'epoch': 0.22}
 22%|██▏       | 487/2181 [54:12<3:06:28,  6.60s/it] 22%|██▏       | 488/2181 [54:18<3:04:54,  6.55s/it]                                                    {'loss': 2.2589, 'learning_rate': 0.0009049445949466078, 'epoch': 0.22}
 22%|██▏       | 488/2181 [54:18<3:04:54,  6.55s/it] 22%|██▏       | 489/2181 [54:25<3:04:06,  6.53s/it]                                                    {'loss': 2.1708, 'learning_rate': 0.0009045084971874737, 'epoch': 0.22}
 22%|██▏       | 489/2181 [54:25<3:04:06,  6.53s/it] 22%|██▏       | 490/2181 [54:31<3:04:13,  6.54s/it]                                                    {'loss': 2.1732, 'learning_rate': 0.0009040715069317382, 'epoch': 0.22}
 22%|██▏       | 490/2181 [54:31<3:04:13,  6.54s/it] 23%|██▎       | 491/2181 [54:38<3:03:46,  6.52s/it]                                                    {'loss': 2.2076, 'learning_rate': 0.0009036336251435648, 'epoch': 0.23}
 23%|██▎       | 491/2181 [54:38<3:03:46,  6.52s/it] 23%|██▎       | 492/2181 [54:44<3:02:54,  6.50s/it]                                                    {'loss': 2.3035, 'learning_rate': 0.0009031948527890839, 'epoch': 0.23}
 23%|██▎       | 492/2181 [54:44<3:02:54,  6.50s/it] 23%|██▎       | 493/2181 [54:51<3:05:10,  6.58s/it]                                                    {'loss': 2.221, 'learning_rate': 0.000902755190836391, 'epoch': 0.23}
 23%|██▎       | 493/2181 [54:51<3:05:10,  6.58s/it] 23%|██▎       | 494/2181 [54:58<3:05:37,  6.60s/it]                                                    {'loss': 2.3628, 'learning_rate': 0.0009023146402555442, 'epoch': 0.23}
 23%|██▎       | 494/2181 [54:58<3:05:37,  6.60s/it] 23%|██▎       | 495/2181 [55:04<3:05:47,  6.61s/it]                                                    {'loss': 2.2552, 'learning_rate': 0.0009018732020185624, 'epoch': 0.23}
 23%|██▎       | 495/2181 [55:04<3:05:47,  6.61s/it] 23%|██▎       | 496/2181 [55:11<3:04:43,  6.58s/it]                                                    {'loss': 2.156, 'learning_rate': 0.0009014308770994235, 'epoch': 0.23}
 23%|██▎       | 496/2181 [55:11<3:04:43,  6.58s/it] 23%|██▎       | 497/2181 [55:17<3:03:37,  6.54s/it]                                                    {'loss': 2.225, 'learning_rate': 0.0009009876664740605, 'epoch': 0.23}
 23%|██▎       | 497/2181 [55:17<3:03:37,  6.54s/it] 23%|██▎       | 498/2181 [55:24<3:03:11,  6.53s/it]                                                    {'loss': 2.1911, 'learning_rate': 0.0009005435711203618, 'epoch': 0.23}
 23%|██▎       | 498/2181 [55:24<3:03:11,  6.53s/it] 23%|██▎       | 499/2181 [55:30<3:02:37,  6.51s/it]                                                    {'loss': 2.1327, 'learning_rate': 0.000900098592018167, 'epoch': 0.23}
 23%|██▎       | 499/2181 [55:30<3:02:37,  6.51s/it] 23%|██▎       | 500/2181 [55:37<3:02:14,  6.50s/it]                                                    {'loss': 2.2376, 'learning_rate': 0.0008996527301492663, 'epoch': 0.23}
 23%|██▎       | 500/2181 [55:37<3:02:14,  6.50s/it] 23%|██▎       | 501/2181 [55:43<3:03:27,  6.55s/it]                                                    {'loss': 2.1486, 'learning_rate': 0.0008992059864973972, 'epoch': 0.23}
 23%|██▎       | 501/2181 [55:43<3:03:27,  6.55s/it] 23%|██▎       | 502/2181 [55:50<3:04:07,  6.58s/it]                                                    {'loss': 2.2387, 'learning_rate': 0.0008987583620482427, 'epoch': 0.23}
 23%|██▎       | 502/2181 [55:50<3:04:07,  6.58s/it] 23%|██▎       | 503/2181 [55:56<3:03:03,  6.55s/it]                                                    {'loss': 2.2365, 'learning_rate': 0.0008983098577894292, 'epoch': 0.23}
 23%|██▎       | 503/2181 [55:56<3:03:03,  6.55s/it] 23%|██▎       | 504/2181 [56:03<3:02:25,  6.53s/it]                                                    {'loss': 2.1898, 'learning_rate': 0.0008978604747105246, 'epoch': 0.23}
 23%|██▎       | 504/2181 [56:03<3:02:25,  6.53s/it] 23%|██▎       | 505/2181 [56:09<3:01:15,  6.49s/it]                                                    {'loss': 2.2017, 'learning_rate': 0.0008974102138030354, 'epoch': 0.23}
 23%|██▎       | 505/2181 [56:09<3:01:15,  6.49s/it] 23%|██▎       | 506/2181 [56:16<3:01:54,  6.52s/it]                                                    {'loss': 2.2445, 'learning_rate': 0.000896959076060405, 'epoch': 0.23}
 23%|██▎       | 506/2181 [56:16<3:01:54,  6.52s/it] 23%|██▎       | 507/2181 [56:22<3:01:57,  6.52s/it]                                                    {'loss': 2.1586, 'learning_rate': 0.0008965070624780116, 'epoch': 0.23}
 23%|██▎       | 507/2181 [56:22<3:01:57,  6.52s/it] 23%|██▎       | 508/2181 [56:29<3:03:12,  6.57s/it]                                                    {'loss': 2.2671, 'learning_rate': 0.0008960541740531658, 'epoch': 0.23}
 23%|██▎       | 508/2181 [56:29<3:03:12,  6.57s/it] 23%|██▎       | 509/2181 [56:36<3:03:38,  6.59s/it]                                                    {'loss': 2.2225, 'learning_rate': 0.0008956004117851083, 'epoch': 0.23}
 23%|██▎       | 509/2181 [56:36<3:03:38,  6.59s/it] 23%|██▎       | 510/2181 [56:42<3:04:05,  6.61s/it]                                                    {'loss': 2.2254, 'learning_rate': 0.0008951457766750079, 'epoch': 0.23}
 23%|██▎       | 510/2181 [56:42<3:04:05,  6.61s/it] 23%|██▎       | 511/2181 [56:49<3:02:16,  6.55s/it]                                                    {'loss': 2.1732, 'learning_rate': 0.0008946902697259593, 'epoch': 0.23}
 23%|██▎       | 511/2181 [56:49<3:02:16,  6.55s/it] 23%|██▎       | 512/2181 [56:55<3:01:32,  6.53s/it]                                                    {'loss': 2.23, 'learning_rate': 0.0008942338919429805, 'epoch': 0.23}
 23%|██▎       | 512/2181 [56:55<3:01:32,  6.53s/it] 24%|██▎       | 513/2181 [57:02<3:00:31,  6.49s/it]                                                    {'loss': 2.1682, 'learning_rate': 0.0008937766443330113, 'epoch': 0.24}
 24%|██▎       | 513/2181 [57:02<3:00:31,  6.49s/it] 24%|██▎       | 514/2181 [57:08<3:00:07,  6.48s/it]                                                    {'loss': 2.2979, 'learning_rate': 0.0008933185279049103, 'epoch': 0.24}
 24%|██▎       | 514/2181 [57:08<3:00:07,  6.48s/it] 24%|██▎       | 515/2181 [57:15<3:00:33,  6.50s/it]                                                    {'loss': 2.2743, 'learning_rate': 0.0008928595436694532, 'epoch': 0.24}
 24%|██▎       | 515/2181 [57:15<3:00:33,  6.50s/it] 24%|██▎       | 516/2181 [57:21<3:02:15,  6.57s/it]                                                    {'loss': 2.1255, 'learning_rate': 0.0008923996926393305, 'epoch': 0.24}
 24%|██▎       | 516/2181 [57:21<3:02:15,  6.57s/it] 24%|██▎       | 517/2181 [57:28<3:03:26,  6.61s/it]                                                    {'loss': 2.2422, 'learning_rate': 0.0008919389758291449, 'epoch': 0.24}
 24%|██▎       | 517/2181 [57:28<3:03:26,  6.61s/it] 24%|██▍       | 518/2181 [57:35<3:02:07,  6.57s/it]                                                    {'loss': 2.204, 'learning_rate': 0.0008914773942554098, 'epoch': 0.24}
 24%|██▍       | 518/2181 [57:35<3:02:07,  6.57s/it] 24%|██▍       | 519/2181 [57:41<3:01:11,  6.54s/it]                                                    {'loss': 2.3018, 'learning_rate': 0.000891014948936546, 'epoch': 0.24}
 24%|██▍       | 519/2181 [57:41<3:01:11,  6.54s/it] 24%|██▍       | 520/2181 [57:47<2:59:38,  6.49s/it]                                                    {'loss': 2.288, 'learning_rate': 0.0008905516408928804, 'epoch': 0.24}
 24%|██▍       | 520/2181 [57:47<2:59:38,  6.49s/it] 24%|██▍       | 521/2181 [57:54<2:58:49,  6.46s/it]                                                    {'loss': 2.2023, 'learning_rate': 0.0008900874711466434, 'epoch': 0.24}
 24%|██▍       | 521/2181 [57:54<2:58:49,  6.46s/it] 24%|██▍       | 522/2181 [58:00<2:59:01,  6.47s/it]                                                    {'loss': 2.2788, 'learning_rate': 0.0008896224407219666, 'epoch': 0.24}
 24%|██▍       | 522/2181 [58:00<2:59:01,  6.47s/it] 24%|██▍       | 523/2181 [58:07<3:01:03,  6.55s/it]                                                    {'loss': 2.2931, 'learning_rate': 0.0008891565506448804, 'epoch': 0.24}
 24%|██▍       | 523/2181 [58:07<3:01:03,  6.55s/it] 24%|██▍       | 524/2181 [58:14<3:01:53,  6.59s/it]                                                    {'loss': 2.1183, 'learning_rate': 0.0008886898019433122, 'epoch': 0.24}
 24%|██▍       | 524/2181 [58:14<3:01:53,  6.59s/it] 24%|██▍       | 525/2181 [58:20<3:02:04,  6.60s/it]                                                    {'loss': 2.2537, 'learning_rate': 0.0008882221956470836, 'epoch': 0.24}
 24%|██▍       | 525/2181 [58:20<3:02:04,  6.60s/it] 24%|██▍       | 526/2181 [58:27<3:01:48,  6.59s/it]                                                    {'loss': 2.2387, 'learning_rate': 0.0008877537327879086, 'epoch': 0.24}
 24%|██▍       | 526/2181 [58:27<3:01:48,  6.59s/it] 24%|██▍       | 527/2181 [58:33<3:00:25,  6.54s/it]                                                    {'loss': 2.1696, 'learning_rate': 0.0008872844143993908, 'epoch': 0.24}
 24%|██▍       | 527/2181 [58:33<3:00:25,  6.54s/it] 24%|██▍       | 528/2181 [58:40<2:59:54,  6.53s/it]                                                    {'loss': 2.2412, 'learning_rate': 0.0008868142415170218, 'epoch': 0.24}
 24%|██▍       | 528/2181 [58:40<2:59:54,  6.53s/it] 24%|██▍       | 529/2181 [58:46<2:59:26,  6.52s/it]                                                    {'loss': 2.2711, 'learning_rate': 0.0008863432151781781, 'epoch': 0.24}
 24%|██▍       | 529/2181 [58:46<2:59:26,  6.52s/it] 24%|██▍       | 530/2181 [58:53<2:58:37,  6.49s/it]                                                    {'loss': 2.1858, 'learning_rate': 0.0008858713364221195, 'epoch': 0.24}
 24%|██▍       | 530/2181 [58:53<2:58:37,  6.49s/it] 24%|██▍       | 531/2181 [59:00<3:01:04,  6.58s/it]                                                    {'loss': 2.1908, 'learning_rate': 0.0008853986062899868, 'epoch': 0.24}
 24%|██▍       | 531/2181 [59:00<3:01:04,  6.58s/it] 24%|██▍       | 532/2181 [59:06<3:01:47,  6.61s/it]                                                    {'loss': 2.2348, 'learning_rate': 0.0008849250258247986, 'epoch': 0.24}
 24%|██▍       | 532/2181 [59:06<3:01:47,  6.61s/it] 24%|██▍       | 533/2181 [59:13<3:01:31,  6.61s/it]                                                    {'loss': 2.1455, 'learning_rate': 0.0008844505960714503, 'epoch': 0.24}
 24%|██▍       | 533/2181 [59:13<3:01:31,  6.61s/it] 24%|██▍       | 534/2181 [59:19<3:00:13,  6.57s/it]                                                    {'loss': 2.1757, 'learning_rate': 0.0008839753180767108, 'epoch': 0.24}
 24%|██▍       | 534/2181 [59:19<3:00:13,  6.57s/it] 25%|██▍       | 535/2181 [59:26<3:00:29,  6.58s/it]                                                    {'loss': 2.3054, 'learning_rate': 0.0008834991928892204, 'epoch': 0.25}
 25%|██▍       | 535/2181 [59:26<3:00:29,  6.58s/it] 25%|██▍       | 536/2181 [59:32<3:00:07,  6.57s/it]                                                    {'loss': 2.2016, 'learning_rate': 0.000883022221559489, 'epoch': 0.25}
 25%|██▍       | 536/2181 [59:32<3:00:07,  6.57s/it] 25%|██▍       | 537/2181 [59:39<2:58:54,  6.53s/it]                                                    {'loss': 2.2217, 'learning_rate': 0.0008825444051398934, 'epoch': 0.25}
 25%|██▍       | 537/2181 [59:39<2:58:54,  6.53s/it] 25%|██▍       | 538/2181 [59:46<3:00:06,  6.58s/it]                                                    {'loss': 2.2459, 'learning_rate': 0.0008820657446846745, 'epoch': 0.25}
 25%|██▍       | 538/2181 [59:46<3:00:06,  6.58s/it] 25%|██▍       | 539/2181 [59:52<3:01:01,  6.61s/it]                                                    {'loss': 2.2535, 'learning_rate': 0.000881586241249936, 'epoch': 0.25}
 25%|██▍       | 539/2181 [59:52<3:01:01,  6.61s/it] 25%|██▍       | 540/2181 [59:59<3:01:45,  6.65s/it]                                                    {'loss': 2.2276, 'learning_rate': 0.0008811058958936411, 'epoch': 0.25}
 25%|██▍       | 540/2181 [59:59<3:01:45,  6.65s/it] 25%|██▍       | 541/2181 [1:00:05<2:59:49,  6.58s/it]                                                      {'loss': 2.2011, 'learning_rate': 0.000880624709675611, 'epoch': 0.25}
 25%|██▍       | 541/2181 [1:00:05<2:59:49,  6.58s/it] 25%|██▍       | 542/2181 [1:00:12<2:59:43,  6.58s/it]                                                      {'loss': 2.2158, 'learning_rate': 0.000880142683657522, 'epoch': 0.25}
 25%|██▍       | 542/2181 [1:00:12<2:59:43,  6.58s/it] 25%|██▍       | 543/2181 [1:00:18<2:58:47,  6.55s/it]                                                      {'loss': 2.1525, 'learning_rate': 0.0008796598189029029, 'epoch': 0.25}
 25%|██▍       | 543/2181 [1:00:19<2:58:47,  6.55s/it] 25%|██▍       | 544/2181 [1:00:25<2:58:09,  6.53s/it]                                                      {'loss': 2.1783, 'learning_rate': 0.0008791761164771338, 'epoch': 0.25}
 25%|██▍       | 544/2181 [1:00:25<2:58:09,  6.53s/it] 25%|██▍       | 545/2181 [1:00:32<2:58:45,  6.56s/it]                                                      {'loss': 2.1351, 'learning_rate': 0.0008786915774474424, 'epoch': 0.25}
 25%|██▍       | 545/2181 [1:00:32<2:58:45,  6.56s/it] 25%|██▌       | 546/2181 [1:00:38<3:00:53,  6.64s/it]                                                      {'loss': 2.1604, 'learning_rate': 0.0008782062028829027, 'epoch': 0.25}
 25%|██▌       | 546/2181 [1:00:38<3:00:53,  6.64s/it] 25%|██▌       | 547/2181 [1:00:45<3:01:02,  6.65s/it]                                                      {'loss': 2.192, 'learning_rate': 0.0008777199938544318, 'epoch': 0.25}
 25%|██▌       | 547/2181 [1:00:45<3:01:02,  6.65s/it] 25%|██▌       | 548/2181 [1:00:52<3:00:10,  6.62s/it]                                                      {'loss': 2.0957, 'learning_rate': 0.0008772329514347883, 'epoch': 0.25}
 25%|██▌       | 548/2181 [1:00:52<3:00:10,  6.62s/it] 25%|██▌       | 549/2181 [1:00:58<2:59:12,  6.59s/it]                                                      {'loss': 2.2366, 'learning_rate': 0.0008767450766985694, 'epoch': 0.25}
 25%|██▌       | 549/2181 [1:00:58<2:59:12,  6.59s/it] 25%|██▌       | 550/2181 [1:01:05<2:57:40,  6.54s/it]                                                      {'loss': 2.2361, 'learning_rate': 0.0008762563707222086, 'epoch': 0.25}
 25%|██▌       | 550/2181 [1:01:05<2:57:40,  6.54s/it] 25%|██▌       | 551/2181 [1:01:11<2:56:44,  6.51s/it]                                                      {'loss': 2.2687, 'learning_rate': 0.0008757668345839738, 'epoch': 0.25}
 25%|██▌       | 551/2181 [1:01:11<2:56:44,  6.51s/it] 25%|██▌       | 552/2181 [1:01:17<2:56:00,  6.48s/it]                                                      {'loss': 2.2774, 'learning_rate': 0.0008752764693639638, 'epoch': 0.25}
 25%|██▌       | 552/2181 [1:01:17<2:56:00,  6.48s/it] 25%|██▌       | 553/2181 [1:01:24<2:55:50,  6.48s/it]                                                      {'loss': 2.15, 'learning_rate': 0.0008747852761441078, 'epoch': 0.25}
 25%|██▌       | 553/2181 [1:01:24<2:55:50,  6.48s/it] 25%|██▌       | 554/2181 [1:01:31<2:57:40,  6.55s/it]                                                      {'loss': 2.1632, 'learning_rate': 0.0008742932560081607, 'epoch': 0.25}
 25%|██▌       | 554/2181 [1:01:31<2:57:40,  6.55s/it] 25%|██▌       | 555/2181 [1:01:37<2:58:23,  6.58s/it]                                                      {'loss': 2.1688, 'learning_rate': 0.0008738004100417025, 'epoch': 0.25}
 25%|██▌       | 555/2181 [1:01:37<2:58:23,  6.58s/it] 25%|██▌       | 556/2181 [1:01:44<2:58:46,  6.60s/it]                                                      {'loss': 2.2603, 'learning_rate': 0.0008733067393321355, 'epoch': 0.25}
 25%|██▌       | 556/2181 [1:01:44<2:58:46,  6.60s/it] 26%|██▌       | 557/2181 [1:01:50<2:57:34,  6.56s/it]                                                      {'loss': 2.2261, 'learning_rate': 0.000872812244968681, 'epoch': 0.26}
 26%|██▌       | 557/2181 [1:01:50<2:57:34,  6.56s/it] 26%|██▌       | 558/2181 [1:01:57<2:56:18,  6.52s/it]                                                      {'loss': 2.2872, 'learning_rate': 0.0008723169280423783, 'epoch': 0.26}
 26%|██▌       | 558/2181 [1:01:57<2:56:18,  6.52s/it] 26%|██▌       | 559/2181 [1:02:03<2:55:23,  6.49s/it]                                                      {'loss': 2.2437, 'learning_rate': 0.0008718207896460811, 'epoch': 0.26}
 26%|██▌       | 559/2181 [1:02:03<2:55:23,  6.49s/it] 26%|██▌       | 560/2181 [1:02:10<2:55:05,  6.48s/it]                                                      {'loss': 2.3073, 'learning_rate': 0.0008713238308744557, 'epoch': 0.26}
 26%|██▌       | 560/2181 [1:02:10<2:55:05,  6.48s/it] 26%|██▌       | 561/2181 [1:02:16<2:55:36,  6.50s/it]                                                      {'loss': 2.1501, 'learning_rate': 0.0008708260528239789, 'epoch': 0.26}
 26%|██▌       | 561/2181 [1:02:16<2:55:36,  6.50s/it] 26%|██▌       | 562/2181 [1:02:23<2:57:44,  6.59s/it]                                                      {'loss': 2.2254, 'learning_rate': 0.000870327456592934, 'epoch': 0.26}
 26%|██▌       | 562/2181 [1:02:23<2:57:44,  6.59s/it] 26%|██▌       | 563/2181 [1:02:30<2:58:23,  6.62s/it]                                                      {'loss': 2.2105, 'learning_rate': 0.0008698280432814107, 'epoch': 0.26}
 26%|██▌       | 563/2181 [1:02:30<2:58:23,  6.62s/it] 26%|██▌       | 564/2181 [1:02:36<2:57:08,  6.57s/it]                                                      {'loss': 2.2498, 'learning_rate': 0.000869327813991301, 'epoch': 0.26}
 26%|██▌       | 564/2181 [1:02:36<2:57:08,  6.57s/it] 26%|██▌       | 565/2181 [1:02:43<2:56:06,  6.54s/it]                                                      {'loss': 2.239, 'learning_rate': 0.0008688267698262971, 'epoch': 0.26}
 26%|██▌       | 565/2181 [1:02:43<2:56:06,  6.54s/it] 26%|██▌       | 566/2181 [1:02:49<2:54:37,  6.49s/it]                                                      {'loss': 2.1955, 'learning_rate': 0.0008683249118918894, 'epoch': 0.26}
 26%|██▌       | 566/2181 [1:02:49<2:54:37,  6.49s/it] 26%|██▌       | 567/2181 [1:02:55<2:54:17,  6.48s/it]                                                      {'loss': 2.2514, 'learning_rate': 0.0008678222412953637, 'epoch': 0.26}
 26%|██▌       | 567/2181 [1:02:55<2:54:17,  6.48s/it] 26%|██▌       | 568/2181 [1:03:02<2:53:59,  6.47s/it]                                                      {'loss': 2.3702, 'learning_rate': 0.0008673187591457987, 'epoch': 0.26}
 26%|██▌       | 568/2181 [1:03:02<2:53:59,  6.47s/it] 26%|██▌       | 569/2181 [1:03:09<2:55:44,  6.54s/it]                                                      {'loss': 2.1952, 'learning_rate': 0.0008668144665540639, 'epoch': 0.26}
 26%|██▌       | 569/2181 [1:03:09<2:55:44,  6.54s/it] 26%|██▌       | 570/2181 [1:03:15<2:57:56,  6.63s/it]                                                      {'loss': 2.1956, 'learning_rate': 0.0008663093646328167, 'epoch': 0.26}
 26%|██▌       | 570/2181 [1:03:16<2:57:56,  6.63s/it] 26%|██▌       | 571/2181 [1:03:22<2:57:55,  6.63s/it]                                                      {'loss': 2.1854, 'learning_rate': 0.0008658034544965003, 'epoch': 0.26}
 26%|██▌       | 571/2181 [1:03:22<2:57:55,  6.63s/it] 26%|██▌       | 572/2181 [1:03:29<2:56:45,  6.59s/it]                                                      {'loss': 2.2234, 'learning_rate': 0.0008652967372613412, 'epoch': 0.26}
 26%|██▌       | 572/2181 [1:03:29<2:56:45,  6.59s/it] 26%|██▋       | 573/2181 [1:03:35<2:55:45,  6.56s/it]                                                      {'loss': 2.2416, 'learning_rate': 0.0008647892140453466, 'epoch': 0.26}
 26%|██▋       | 573/2181 [1:03:35<2:55:45,  6.56s/it] 26%|██▋       | 574/2181 [1:03:42<2:55:06,  6.54s/it]                                                      {'loss': 2.1524, 'learning_rate': 0.0008642808859683021, 'epoch': 0.26}
 26%|██▋       | 574/2181 [1:03:42<2:55:06,  6.54s/it] 26%|██▋       | 575/2181 [1:03:48<2:54:37,  6.52s/it]                                                      {'loss': 2.2628, 'learning_rate': 0.0008637717541517689, 'epoch': 0.26}
 26%|██▋       | 575/2181 [1:03:48<2:54:37,  6.52s/it] 26%|██▋       | 576/2181 [1:03:55<2:54:12,  6.51s/it]                                                      {'loss': 2.2064, 'learning_rate': 0.0008632618197190816, 'epoch': 0.26}
 26%|██▋       | 576/2181 [1:03:55<2:54:12,  6.51s/it] 26%|██▋       | 577/2181 [1:04:01<2:57:01,  6.62s/it]                                                      {'loss': 2.2342, 'learning_rate': 0.0008627510837953458, 'epoch': 0.26}
 26%|██▋       | 577/2181 [1:04:01<2:57:01,  6.62s/it] 27%|██▋       | 578/2181 [1:04:08<2:57:31,  6.64s/it]                                                      {'loss': 2.1942, 'learning_rate': 0.0008622395475074355, 'epoch': 0.27}
 27%|██▋       | 578/2181 [1:04:08<2:57:31,  6.64s/it] 27%|██▋       | 579/2181 [1:04:15<2:55:58,  6.59s/it]                                                      {'loss': 2.2179, 'learning_rate': 0.0008617272119839903, 'epoch': 0.27}
 27%|██▋       | 579/2181 [1:04:15<2:55:58,  6.59s/it] 27%|██▋       | 580/2181 [1:04:21<2:54:16,  6.53s/it]                                                      {'loss': 2.1792, 'learning_rate': 0.0008612140783554136, 'epoch': 0.27}
 27%|██▋       | 580/2181 [1:04:21<2:54:16,  6.53s/it] 27%|██▋       | 581/2181 [1:04:27<2:53:40,  6.51s/it]                                                      {'loss': 2.1939, 'learning_rate': 0.0008607001477538696, 'epoch': 0.27}
 27%|██▋       | 581/2181 [1:04:28<2:53:40,  6.51s/it] 27%|██▋       | 582/2181 [1:04:34<2:53:28,  6.51s/it]                                                      {'loss': 2.3048, 'learning_rate': 0.0008601854213132807, 'epoch': 0.27}
 27%|██▋       | 582/2181 [1:04:34<2:53:28,  6.51s/it] 27%|██▋       | 583/2181 [1:04:40<2:53:18,  6.51s/it]                                                      {'loss': 2.1623, 'learning_rate': 0.0008596699001693256, 'epoch': 0.27}
 27%|██▋       | 583/2181 [1:04:41<2:53:18,  6.51s/it] 27%|██▋       | 584/2181 [1:04:47<2:55:35,  6.60s/it]                                                      {'loss': 2.2159, 'learning_rate': 0.000859153585459436, 'epoch': 0.27}
 27%|██▋       | 584/2181 [1:04:47<2:55:35,  6.60s/it] 27%|██▋       | 585/2181 [1:04:54<2:56:12,  6.62s/it]                                                      {'loss': 2.1408, 'learning_rate': 0.0008586364783227949, 'epoch': 0.27}
 27%|██▋       | 585/2181 [1:04:54<2:56:12,  6.62s/it] 27%|██▋       | 586/2181 [1:05:00<2:54:40,  6.57s/it]                                                      {'loss': 2.2152, 'learning_rate': 0.0008581185799003332, 'epoch': 0.27}
 27%|██▋       | 586/2181 [1:05:00<2:54:40,  6.57s/it] 27%|██▋       | 587/2181 [1:05:07<2:53:50,  6.54s/it]                                                      {'loss': 2.1881, 'learning_rate': 0.0008575998913347283, 'epoch': 0.27}
 27%|██▋       | 587/2181 [1:05:07<2:53:50,  6.54s/it] 27%|██▋       | 588/2181 [1:05:13<2:54:00,  6.55s/it]                                                      {'loss': 2.1226, 'learning_rate': 0.0008570804137704004, 'epoch': 0.27}
 27%|██▋       | 588/2181 [1:05:13<2:54:00,  6.55s/it] 27%|██▋       | 589/2181 [1:05:20<2:53:39,  6.55s/it]                                                      {'loss': 2.1192, 'learning_rate': 0.0008565601483535108, 'epoch': 0.27}
 27%|██▋       | 589/2181 [1:05:20<2:53:39,  6.55s/it] 27%|██▋       | 590/2181 [1:05:26<2:52:38,  6.51s/it]                                                      {'loss': 2.1433, 'learning_rate': 0.0008560390962319591, 'epoch': 0.27}
 27%|██▋       | 590/2181 [1:05:26<2:52:38,  6.51s/it] 27%|██▋       | 591/2181 [1:05:33<2:54:16,  6.58s/it]                                                      {'loss': 2.2152, 'learning_rate': 0.0008555172585553804, 'epoch': 0.27}
 27%|██▋       | 591/2181 [1:05:33<2:54:16,  6.58s/it] 27%|██▋       | 592/2181 [1:05:40<2:55:04,  6.61s/it]                                                      {'loss': 2.1776, 'learning_rate': 0.0008549946364751435, 'epoch': 0.27}
 27%|██▋       | 592/2181 [1:05:40<2:55:04,  6.61s/it] 27%|██▋       | 593/2181 [1:05:46<2:55:21,  6.63s/it]                                                      {'loss': 2.2276, 'learning_rate': 0.0008544712311443475, 'epoch': 0.27}
 27%|██▋       | 593/2181 [1:05:47<2:55:21,  6.63s/it] 27%|██▋       | 594/2181 [1:05:53<2:55:21,  6.63s/it]                                                      {'loss': 2.2019, 'learning_rate': 0.0008539470437178196, 'epoch': 0.27}
 27%|██▋       | 594/2181 [1:05:53<2:55:21,  6.63s/it] 27%|██▋       | 595/2181 [1:06:00<2:53:48,  6.58s/it]                                                      {'loss': 2.2235, 'learning_rate': 0.000853422075352113, 'epoch': 0.27}
 27%|██▋       | 595/2181 [1:06:00<2:53:48,  6.58s/it] 27%|██▋       | 596/2181 [1:06:06<2:53:02,  6.55s/it]                                                      {'loss': 2.1929, 'learning_rate': 0.0008528963272055035, 'epoch': 0.27}
 27%|██▋       | 596/2181 [1:06:06<2:53:02,  6.55s/it] 27%|██▋       | 597/2181 [1:06:13<2:52:34,  6.54s/it]                                                      {'loss': 2.193, 'learning_rate': 0.0008523698004379877, 'epoch': 0.27}
 27%|██▋       | 597/2181 [1:06:13<2:52:34,  6.54s/it] 27%|██▋       | 598/2181 [1:06:19<2:52:51,  6.55s/it]                                                      {'loss': 2.3037, 'learning_rate': 0.00085184249621128, 'epoch': 0.27}
 27%|██▋       | 598/2181 [1:06:19<2:52:51,  6.55s/it] 27%|██▋       | 599/2181 [1:06:26<2:54:31,  6.62s/it]                                                      {'loss': 2.2812, 'learning_rate': 0.0008513144156888101, 'epoch': 0.27}
 27%|██▋       | 599/2181 [1:06:26<2:54:31,  6.62s/it] 28%|██▊       | 600/2181 [1:06:33<2:55:17,  6.65s/it]                                                      {'loss': 2.2679, 'learning_rate': 0.0008507855600357207, 'epoch': 0.28}
 28%|██▊       | 600/2181 [1:06:33<2:55:17,  6.65s/it] 28%|██▊       | 601/2181 [1:06:39<2:55:34,  6.67s/it]                                                      {'loss': 2.2301, 'learning_rate': 0.0008502559304188644, 'epoch': 0.28}
 28%|██▊       | 601/2181 [1:06:39<2:55:34,  6.67s/it] 28%|██▊       | 602/2181 [1:06:46<2:54:35,  6.63s/it]                                                      {'loss': 2.2334, 'learning_rate': 0.0008497255280068019, 'epoch': 0.28}
 28%|██▊       | 602/2181 [1:06:46<2:54:35,  6.63s/it] 28%|██▊       | 603/2181 [1:06:52<2:53:24,  6.59s/it]                                                      {'loss': 2.135, 'learning_rate': 0.0008491943539697986, 'epoch': 0.28}
 28%|██▊       | 603/2181 [1:06:53<2:53:24,  6.59s/it] 28%|██▊       | 604/2181 [1:06:59<2:52:25,  6.56s/it]                                                      {'loss': 2.1743, 'learning_rate': 0.0008486624094798226, 'epoch': 0.28}
 28%|██▊       | 604/2181 [1:06:59<2:52:25,  6.56s/it] 28%|██▊       | 605/2181 [1:07:05<2:51:20,  6.52s/it]                                                      {'loss': 2.2275, 'learning_rate': 0.0008481296957105417, 'epoch': 0.28}
 28%|██▊       | 605/2181 [1:07:05<2:51:20,  6.52s/it] 28%|██▊       | 606/2181 [1:07:12<2:51:46,  6.54s/it]                                                      {'loss': 2.1937, 'learning_rate': 0.0008475962138373213, 'epoch': 0.28}
 28%|██▊       | 606/2181 [1:07:12<2:51:46,  6.54s/it] 28%|██▊       | 607/2181 [1:07:19<2:52:57,  6.59s/it]                                                      {'loss': 2.1927, 'learning_rate': 0.0008470619650372211, 'epoch': 0.28}
 28%|██▊       | 607/2181 [1:07:19<2:52:57,  6.59s/it] 28%|██▊       | 608/2181 [1:07:25<2:53:58,  6.64s/it]                                                      {'loss': 2.2299, 'learning_rate': 0.0008465269504889934, 'epoch': 0.28}
 28%|██▊       | 608/2181 [1:07:25<2:53:58,  6.64s/it] 28%|██▊       | 609/2181 [1:07:32<2:52:53,  6.60s/it]                                                      {'loss': 2.1869, 'learning_rate': 0.0008459911713730799, 'epoch': 0.28}
 28%|██▊       | 609/2181 [1:07:32<2:52:53,  6.60s/it] 28%|██▊       | 610/2181 [1:07:38<2:52:18,  6.58s/it]                                                      {'loss': 2.3076, 'learning_rate': 0.0008454546288716089, 'epoch': 0.28}
 28%|██▊       | 610/2181 [1:07:38<2:52:18,  6.58s/it] 28%|██▊       | 611/2181 [1:07:45<2:55:13,  6.70s/it]                                                      {'loss': 2.1893, 'learning_rate': 0.0008449173241683935, 'epoch': 0.28}
 28%|██▊       | 611/2181 [1:07:45<2:55:13,  6.70s/it] 28%|██▊       | 612/2181 [1:07:52<2:53:41,  6.64s/it]                                                      {'loss': 2.2269, 'learning_rate': 0.0008443792584489281, 'epoch': 0.28}
 28%|██▊       | 612/2181 [1:07:52<2:53:41,  6.64s/it] 28%|██▊       | 613/2181 [1:07:58<2:51:43,  6.57s/it]                                                      {'loss': 2.2151, 'learning_rate': 0.0008438404329003863, 'epoch': 0.28}
 28%|██▊       | 613/2181 [1:07:58<2:51:43,  6.57s/it] 28%|██▊       | 614/2181 [1:08:05<2:51:51,  6.58s/it]                                                      {'loss': 2.236, 'learning_rate': 0.0008433008487116183, 'epoch': 0.28}
 28%|██▊       | 614/2181 [1:08:05<2:51:51,  6.58s/it] 28%|██▊       | 615/2181 [1:08:12<2:53:03,  6.63s/it]                                                      {'loss': 2.1368, 'learning_rate': 0.0008427605070731481, 'epoch': 0.28}
 28%|██▊       | 615/2181 [1:08:12<2:53:03,  6.63s/it] 28%|██▊       | 616/2181 [1:08:18<2:52:42,  6.62s/it]                                                      {'loss': 2.2388, 'learning_rate': 0.0008422194091771708, 'epoch': 0.28}
 28%|██▊       | 616/2181 [1:08:18<2:52:42,  6.62s/it] 28%|██▊       | 617/2181 [1:08:25<2:51:38,  6.58s/it]                                                      {'loss': 2.0927, 'learning_rate': 0.0008416775562175503, 'epoch': 0.28}
 28%|██▊       | 617/2181 [1:08:25<2:51:38,  6.58s/it] 28%|██▊       | 618/2181 [1:08:31<2:52:36,  6.63s/it]                                                      {'loss': 2.2448, 'learning_rate': 0.000841134949389816, 'epoch': 0.28}
 28%|██▊       | 618/2181 [1:08:32<2:52:36,  6.63s/it] 28%|██▊       | 619/2181 [1:08:38<2:51:53,  6.60s/it]                                                      {'loss': 2.1924, 'learning_rate': 0.0008405915898911611, 'epoch': 0.28}
 28%|██▊       | 619/2181 [1:08:38<2:51:53,  6.60s/it] 28%|██▊       | 620/2181 [1:08:44<2:50:48,  6.57s/it]                                                      {'loss': 2.1832, 'learning_rate': 0.0008400474789204396, 'epoch': 0.28}
 28%|██▊       | 620/2181 [1:08:45<2:50:48,  6.57s/it] 28%|██▊       | 621/2181 [1:08:51<2:49:39,  6.53s/it]                                                      {'loss': 2.2034, 'learning_rate': 0.0008395026176781626, 'epoch': 0.28}
 28%|██▊       | 621/2181 [1:08:51<2:49:39,  6.53s/it] 29%|██▊       | 622/2181 [1:08:58<2:50:34,  6.56s/it]                                                      {'loss': 2.208, 'learning_rate': 0.0008389570073664976, 'epoch': 0.29}
 29%|██▊       | 622/2181 [1:08:58<2:50:34,  6.56s/it] 29%|██▊       | 623/2181 [1:09:04<2:51:10,  6.59s/it]                                                      {'loss': 2.1619, 'learning_rate': 0.0008384106491892642, 'epoch': 0.29}
 29%|██▊       | 623/2181 [1:09:04<2:51:10,  6.59s/it] 29%|██▊       | 624/2181 [1:09:11<2:50:14,  6.56s/it]                                                      {'loss': 2.1882, 'learning_rate': 0.0008378635443519327, 'epoch': 0.29}
 29%|██▊       | 624/2181 [1:09:11<2:50:14,  6.56s/it] 29%|██▊       | 625/2181 [1:09:17<2:49:48,  6.55s/it]                                                      {'loss': 2.2581, 'learning_rate': 0.0008373156940616199, 'epoch': 0.29}
 29%|██▊       | 625/2181 [1:09:17<2:49:48,  6.55s/it] 29%|██▊       | 626/2181 [1:09:24<2:49:40,  6.55s/it]                                                      {'loss': 2.2474, 'learning_rate': 0.0008367670995270882, 'epoch': 0.29}
 29%|██▊       | 626/2181 [1:09:24<2:49:40,  6.55s/it] 29%|██▊       | 627/2181 [1:09:30<2:49:03,  6.53s/it]                                                      {'loss': 2.1882, 'learning_rate': 0.0008362177619587416, 'epoch': 0.29}
 29%|██▊       | 627/2181 [1:09:30<2:49:03,  6.53s/it] 29%|██▉       | 628/2181 [1:09:37<2:50:07,  6.57s/it]                                                      {'loss': 2.2101, 'learning_rate': 0.0008356676825686238, 'epoch': 0.29}
 29%|██▉       | 628/2181 [1:09:37<2:50:07,  6.57s/it] 29%|██▉       | 629/2181 [1:09:44<2:50:45,  6.60s/it]                                                      {'loss': 2.2101, 'learning_rate': 0.0008351168625704147, 'epoch': 0.29}
 29%|██▉       | 629/2181 [1:09:44<2:50:45,  6.60s/it] 29%|██▉       | 630/2181 [1:09:50<2:52:04,  6.66s/it]                                                      {'loss': 2.2269, 'learning_rate': 0.0008345653031794292, 'epoch': 0.29}
 29%|██▉       | 630/2181 [1:09:50<2:52:04,  6.66s/it] 29%|██▉       | 631/2181 [1:09:57<2:51:03,  6.62s/it]                                                      {'loss': 2.2356, 'learning_rate': 0.0008340130056126125, 'epoch': 0.29}
 29%|██▉       | 631/2181 [1:09:57<2:51:03,  6.62s/it] 29%|██▉       | 632/2181 [1:10:03<2:49:31,  6.57s/it]                                                      {'loss': 2.1688, 'learning_rate': 0.0008334599710885394, 'epoch': 0.29}
 29%|██▉       | 632/2181 [1:10:03<2:49:31,  6.57s/it] 29%|██▉       | 633/2181 [1:10:10<2:48:53,  6.55s/it]                                                      {'loss': 2.1386, 'learning_rate': 0.0008329062008274098, 'epoch': 0.29}
 29%|██▉       | 633/2181 [1:10:10<2:48:53,  6.55s/it] 29%|██▉       | 634/2181 [1:10:16<2:49:08,  6.56s/it]                                                      {'loss': 2.2607, 'learning_rate': 0.000832351696051048, 'epoch': 0.29}
 29%|██▉       | 634/2181 [1:10:17<2:49:08,  6.56s/it] 29%|██▉       | 635/2181 [1:10:23<2:48:40,  6.55s/it]                                                      {'loss': 2.2398, 'learning_rate': 0.000831796457982898, 'epoch': 0.29}
 29%|██▉       | 635/2181 [1:10:23<2:48:40,  6.55s/it] 29%|██▉       | 636/2181 [1:10:30<2:48:49,  6.56s/it]                                                      {'loss': 2.2087, 'learning_rate': 0.0008312404878480222, 'epoch': 0.29}
 29%|██▉       | 636/2181 [1:10:30<2:48:49,  6.56s/it] 29%|██▉       | 637/2181 [1:10:36<2:50:11,  6.61s/it]                                                      {'loss': 2.2471, 'learning_rate': 0.0008306837868730979, 'epoch': 0.29}
 29%|██▉       | 637/2181 [1:10:36<2:50:11,  6.61s/it] 29%|██▉       | 638/2181 [1:10:43<2:51:04,  6.65s/it]                                                      {'loss': 2.1785, 'learning_rate': 0.0008301263562864152, 'epoch': 0.29}
 29%|██▉       | 638/2181 [1:10:43<2:51:04,  6.65s/it] 29%|██▉       | 639/2181 [1:10:50<2:50:22,  6.63s/it]                                                      {'loss': 2.1604, 'learning_rate': 0.0008295681973178737, 'epoch': 0.29}
 29%|██▉       | 639/2181 [1:10:50<2:50:22,  6.63s/it] 29%|██▉       | 640/2181 [1:10:56<2:49:30,  6.60s/it]                                                      {'loss': 2.2476, 'learning_rate': 0.0008290093111989804, 'epoch': 0.29}
 29%|██▉       | 640/2181 [1:10:56<2:49:30,  6.60s/it] 29%|██▉       | 641/2181 [1:11:03<2:48:06,  6.55s/it]                                                      {'loss': 2.172, 'learning_rate': 0.0008284496991628465, 'epoch': 0.29}
 29%|██▉       | 641/2181 [1:11:03<2:48:06,  6.55s/it] 29%|██▉       | 642/2181 [1:11:09<2:46:59,  6.51s/it]                                                      {'loss': 2.1541, 'learning_rate': 0.0008278893624441847, 'epoch': 0.29}
 29%|██▉       | 642/2181 [1:11:09<2:46:59,  6.51s/it] 29%|██▉       | 643/2181 [1:11:15<2:46:32,  6.50s/it]                                                      {'loss': 2.2493, 'learning_rate': 0.000827328302279307, 'epoch': 0.29}
 29%|██▉       | 643/2181 [1:11:16<2:46:32,  6.50s/it] 30%|██▉       | 644/2181 [1:11:23<2:55:05,  6.83s/it]                                                      {'loss': 2.0424, 'learning_rate': 0.0008267665199061211, 'epoch': 0.3}
 30%|██▉       | 644/2181 [1:11:23<2:55:05,  6.83s/it] 30%|██▉       | 645/2181 [1:11:30<2:54:27,  6.81s/it]                                                      {'loss': 2.2279, 'learning_rate': 0.0008262040165641288, 'epoch': 0.3}
 30%|██▉       | 645/2181 [1:11:30<2:54:27,  6.81s/it] 30%|██▉       | 646/2181 [1:11:37<2:53:05,  6.77s/it]                                                      {'loss': 2.2092, 'learning_rate': 0.0008256407934944219, 'epoch': 0.3}
 30%|██▉       | 646/2181 [1:11:37<2:53:05,  6.77s/it] 30%|██▉       | 647/2181 [1:11:43<2:50:55,  6.69s/it]                                                      {'loss': 2.2318, 'learning_rate': 0.0008250768519396807, 'epoch': 0.3}
 30%|██▉       | 647/2181 [1:11:43<2:50:55,  6.69s/it] 30%|██▉       | 648/2181 [1:11:49<2:49:17,  6.63s/it]                                                      {'loss': 2.1871, 'learning_rate': 0.0008245121931441706, 'epoch': 0.3}
 30%|██▉       | 648/2181 [1:11:50<2:49:17,  6.63s/it] 30%|██▉       | 649/2181 [1:11:56<2:48:17,  6.59s/it]                                                      {'loss': 2.2214, 'learning_rate': 0.0008239468183537393, 'epoch': 0.3}
 30%|██▉       | 649/2181 [1:11:56<2:48:17,  6.59s/it] 30%|██▉       | 650/2181 [1:12:02<2:46:46,  6.54s/it]                                                      {'loss': 2.1513, 'learning_rate': 0.0008233807288158146, 'epoch': 0.3}
 30%|██▉       | 650/2181 [1:12:02<2:46:46,  6.54s/it] 30%|██▉       | 651/2181 [1:12:09<2:47:42,  6.58s/it]                                                      {'loss': 2.2157, 'learning_rate': 0.0008228139257794012, 'epoch': 0.3}
 30%|██▉       | 651/2181 [1:12:09<2:47:42,  6.58s/it] 30%|██▉       | 652/2181 [1:12:16<2:48:13,  6.60s/it]                                                      {'loss': 2.1171, 'learning_rate': 0.0008222464104950778, 'epoch': 0.3}
 30%|██▉       | 652/2181 [1:12:16<2:48:13,  6.60s/it] 30%|██▉       | 653/2181 [1:12:22<2:48:57,  6.63s/it]                                                      {'loss': 2.1858, 'learning_rate': 0.000821678184214995, 'epoch': 0.3}
 30%|██▉       | 653/2181 [1:12:23<2:48:57,  6.63s/it] 30%|██▉       | 654/2181 [1:12:29<2:47:29,  6.58s/it]                                                      {'loss': 2.1481, 'learning_rate': 0.0008211092481928716, 'epoch': 0.3}
 30%|██▉       | 654/2181 [1:12:29<2:47:29,  6.58s/it] 30%|███       | 655/2181 [1:12:35<2:46:26,  6.54s/it]                                                      {'loss': 2.2259, 'learning_rate': 0.0008205396036839927, 'epoch': 0.3}
 30%|███       | 655/2181 [1:12:35<2:46:26,  6.54s/it] 30%|███       | 656/2181 [1:12:42<2:47:00,  6.57s/it]                                                      {'loss': 2.2347, 'learning_rate': 0.0008199692519452069, 'epoch': 0.3}
 30%|███       | 656/2181 [1:12:42<2:47:00,  6.57s/it] 30%|███       | 657/2181 [1:12:48<2:46:07,  6.54s/it]                                                      {'loss': 2.1954, 'learning_rate': 0.0008193981942349224, 'epoch': 0.3}
 30%|███       | 657/2181 [1:12:48<2:46:07,  6.54s/it] 30%|███       | 658/2181 [1:12:55<2:45:27,  6.52s/it]                                                      {'loss': 2.2129, 'learning_rate': 0.0008188264318131056, 'epoch': 0.3}
 30%|███       | 658/2181 [1:12:55<2:45:27,  6.52s/it] 30%|███       | 659/2181 [1:13:02<2:46:22,  6.56s/it]                                                      {'loss': 2.1274, 'learning_rate': 0.0008182539659412776, 'epoch': 0.3}
 30%|███       | 659/2181 [1:13:02<2:46:22,  6.56s/it] 30%|███       | 660/2181 [1:13:08<2:46:19,  6.56s/it]                                                      {'loss': 2.1301, 'learning_rate': 0.0008176807978825118, 'epoch': 0.3}
 30%|███       | 660/2181 [1:13:08<2:46:19,  6.56s/it] 30%|███       | 661/2181 [1:13:15<2:45:32,  6.53s/it]                                                      {'loss': 2.2449, 'learning_rate': 0.0008171069289014306, 'epoch': 0.3}
 30%|███       | 661/2181 [1:13:15<2:45:32,  6.53s/it] 30%|███       | 662/2181 [1:13:21<2:44:54,  6.51s/it]                                                      {'loss': 2.2286, 'learning_rate': 0.0008165323602642028, 'epoch': 0.3}
 30%|███       | 662/2181 [1:13:21<2:44:54,  6.51s/it] 30%|███       | 663/2181 [1:13:28<2:45:25,  6.54s/it]                                                      {'loss': 2.1292, 'learning_rate': 0.0008159570932385414, 'epoch': 0.3}
 30%|███       | 663/2181 [1:13:28<2:45:25,  6.54s/it] 30%|███       | 664/2181 [1:13:34<2:45:31,  6.55s/it]                                                      {'loss': 2.1405, 'learning_rate': 0.0008153811290936999, 'epoch': 0.3}
 30%|███       | 664/2181 [1:13:34<2:45:31,  6.55s/it] 30%|███       | 665/2181 [1:13:41<2:44:43,  6.52s/it]                                                      {'loss': 2.1326, 'learning_rate': 0.0008148044691004698, 'epoch': 0.3}
 30%|███       | 665/2181 [1:13:41<2:44:43,  6.52s/it] 31%|███       | 666/2181 [1:13:47<2:44:08,  6.50s/it]                                                      {'loss': 2.1511, 'learning_rate': 0.0008142271145311783, 'epoch': 0.31}
 31%|███       | 666/2181 [1:13:47<2:44:08,  6.50s/it] 31%|███       | 667/2181 [1:13:54<2:46:13,  6.59s/it]                                                      {'loss': 2.2235, 'learning_rate': 0.000813649066659685, 'epoch': 0.31}
 31%|███       | 667/2181 [1:13:54<2:46:13,  6.59s/it] 31%|███       | 668/2181 [1:14:01<2:47:05,  6.63s/it]                                                      {'loss': 2.2102, 'learning_rate': 0.0008130703267613787, 'epoch': 0.31}
 31%|███       | 668/2181 [1:14:01<2:47:05,  6.63s/it] 31%|███       | 669/2181 [1:14:07<2:45:31,  6.57s/it]                                                      {'loss': 2.1829, 'learning_rate': 0.0008124908961131759, 'epoch': 0.31}
 31%|███       | 669/2181 [1:14:07<2:45:31,  6.57s/it] 31%|███       | 670/2181 [1:14:13<2:44:00,  6.51s/it]                                                      {'loss': 2.0738, 'learning_rate': 0.0008119107759935163, 'epoch': 0.31}
 31%|███       | 670/2181 [1:14:14<2:44:00,  6.51s/it] 31%|███       | 671/2181 [1:14:20<2:43:30,  6.50s/it]                                                      {'loss': 2.182, 'learning_rate': 0.0008113299676823615, 'epoch': 0.31}
 31%|███       | 671/2181 [1:14:20<2:43:30,  6.50s/it] 31%|███       | 672/2181 [1:14:26<2:43:02,  6.48s/it]                                                      {'loss': 2.274, 'learning_rate': 0.0008107484724611911, 'epoch': 0.31}
 31%|███       | 672/2181 [1:14:26<2:43:02,  6.48s/it] 31%|███       | 673/2181 [1:14:33<2:43:17,  6.50s/it]                                                      {'loss': 2.2, 'learning_rate': 0.0008101662916130006, 'epoch': 0.31}
 31%|███       | 673/2181 [1:14:33<2:43:17,  6.50s/it] 31%|███       | 674/2181 [1:14:40<2:44:03,  6.53s/it]                                                      {'loss': 2.18, 'learning_rate': 0.0008095834264222979, 'epoch': 0.31}
 31%|███       | 674/2181 [1:14:40<2:44:03,  6.53s/it] 31%|███       | 675/2181 [1:14:46<2:45:38,  6.60s/it]                                                      {'loss': 2.2763, 'learning_rate': 0.0008089998781751009, 'epoch': 0.31}
 31%|███       | 675/2181 [1:14:46<2:45:38,  6.60s/it] 31%|███       | 676/2181 [1:14:53<2:46:40,  6.64s/it]                                                      {'loss': 2.2212, 'learning_rate': 0.0008084156481589349, 'epoch': 0.31}
 31%|███       | 676/2181 [1:14:53<2:46:40,  6.64s/it] 31%|███       | 677/2181 [1:15:00<2:45:34,  6.61s/it]                                                      {'loss': 2.2317, 'learning_rate': 0.0008078307376628291, 'epoch': 0.31}
 31%|███       | 677/2181 [1:15:00<2:45:34,  6.61s/it]