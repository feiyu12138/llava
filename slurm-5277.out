[2024-03-23 15:45:53,010] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-23 15:45:56,565] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7: setting --include=localhost:0,1,2,3,4,5,6,7
[2024-03-23 15:45:56,565] [INFO] [runner.py:571:main] cmd = /home/jchen293/.conda/envs/llava_git/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /datasets/jchen293/data/llava_datasets/LLaVA-Tuning/llava_v1_5_mix665k.json --image_folder /datasets/jchen293/data/llava_datasets/LLaVA-Tuning --vision_tower openai/clip-vit-large-patch14-336 --pretrain_mm_mlp_adapter /datasets/jchen293/weights/llava/checkpoint/llava-v1.5-7b-pretrain-stride-8-layer-16-grouping-avgpool1d/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /datasets/jchen293/weights/llava/checkpoint/llava-v1.5-7b-stride-8-layer-16-grouping-avgpool1d --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 50000 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb --stride 8 --layer 16 --grouping avgpool1d
[2024-03-23 15:45:59,055] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-23 15:46:00,942] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-03-23 15:46:00,942] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-03-23 15:46:00,942] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-03-23 15:46:00,942] [INFO] [launch.py:163:main] dist_world_size=8
[2024-03-23 15:46:00,942] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2024-03-23 15:46:16,096] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-23 15:46:16,104] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-23 15:46:16,113] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-23 15:46:16,177] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-23 15:46:16,214] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-23 15:46:16,263] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-23 15:46:16,265] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-23 15:46:16,294] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-23 15:46:22,102] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-23 15:46:22,104] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-23 15:46:22,107] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-23 15:46:22,108] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-23 15:46:22,109] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-23 15:46:22,116] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-23 15:46:22,117] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-23 15:46:22,117] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-23 15:46:22,117] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
[2024-03-23 15:46:42,201] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 707, num_elems = 15.36B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:41<00:41, 41.13s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:41<00:41, 41.37s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:41<00:41, 41.02s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:41<00:41, 41.36s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:41<00:41, 41.19s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:40<00:40, 40.46s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:41<00:41, 41.39s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:51<00:00, 23.23s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:51<00:00, 25.95s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:51<00:00, 23.12s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:51<00:00, 25.80s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:51<00:00, 23.24s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:51<00:00, 25.97s/it]
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:51<00:00, 23.28s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:51<00:00, 25.99s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:51<00:00, 22.90s/it]/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:51<00:00, 25.54s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:51<00:00, 23.22s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:51<00:00, 25.91s/it]
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:51<00:00, 23.24s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:51<00:00, 25.94s/it]
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:53<00:53, 53.39s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:03<00:00, 27.78s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:03<00:00, 31.62s/it]
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
[2024-03-23 15:47:47,025] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1098, num_elems = 15.67B
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Formatting inputs...Skip in lazy mode
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Parameter Offload: Total persistent parameters: 599040 in 312 params
wandb: Currently logged in as: jienengchen01. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /home/jchen293/code/llava_git/llava/wandb/run-20240323_155011-921otds0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glad-flower-73
wandb: â­ï¸ View project at https://wandb.ai/jienengchen01/huggingface
wandb: ðŸš€ View run at https://wandb.ai/jienengchen01/huggingface/runs/921otds0
  0%|          | 0/5198 [00:00<?, ?it/s]/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/jchen293/.conda/envs/llava_git/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
  0%|          | 1/5198 [01:11<103:46:12, 71.88s/it]                                                    {'loss': 1.3701, 'learning_rate': 1.282051282051282e-07, 'epoch': 0.0}
  0%|          | 1/5198 [01:11<103:46:12, 71.88s/it]  0%|          | 2/5198 [01:27<56:11:40, 38.93s/it]                                                    {'loss': 1.3814, 'learning_rate': 2.564102564102564e-07, 'epoch': 0.0}
  0%|          | 2/5198 [01:27<56:11:40, 38.93s/it]  0%|          | 3/5198 [01:40<38:44:56, 26.85s/it]                                                   {'loss': 1.3726, 'learning_rate': 3.846153846153847e-07, 'epoch': 0.0}
  0%|          | 3/5198 [01:40<38:44:56, 26.85s/it]  0%|          | 4/5198 [01:52<30:24:46, 21.08s/it]                                                   {'loss': 1.4132, 'learning_rate': 5.128205128205128e-07, 'epoch': 0.0}
  0%|          | 4/5198 [01:52<30:24:46, 21.08s/it]  0%|          | 5/5198 [02:04<25:39:44, 17.79s/it]                                                   {'loss': 1.3707, 'learning_rate': 6.41025641025641e-07, 'epoch': 0.0}
  0%|          | 5/5198 [02:04<25:39:44, 17.79s/it]  0%|          | 6/5198 [02:18<23:42:44, 16.44s/it]                                                   {'loss': 1.4053, 'learning_rate': 7.692307692307694e-07, 'epoch': 0.0}
  0%|          | 6/5198 [02:18<23:42:44, 16.44s/it]  0%|          | 7/5198 [02:29<21:21:04, 14.81s/it]                                                   {'loss': 1.3906, 'learning_rate': 8.974358974358975e-07, 'epoch': 0.0}
  0%|          | 7/5198 [02:29<21:21:04, 14.81s/it]  0%|          | 8/5198 [02:41<20:11:45, 14.01s/it]                                                   {'loss': 1.3488, 'learning_rate': 1.0256410256410257e-06, 'epoch': 0.0}
  0%|          | 8/5198 [02:42<20:11:45, 14.01s/it]  0%|          | 9/5198 [02:53<19:14:00, 13.34s/it]                                                   {'loss': 1.3895, 'learning_rate': 1.153846153846154e-06, 'epoch': 0.0}
  0%|          | 9/5198 [02:53<19:14:00, 13.34s/it]  0%|          | 10/5198 [03:05<18:26:32, 12.80s/it]                                                    {'loss': 1.3203, 'learning_rate': 1.282051282051282e-06, 'epoch': 0.0}
  0%|          | 10/5198 [03:05<18:26:32, 12.80s/it]  0%|          | 11/5198 [03:17<17:58:53, 12.48s/it]                                                    {'loss': 1.3389, 'learning_rate': 1.4102564102564104e-06, 'epoch': 0.0}
  0%|          | 11/5198 [03:17<17:58:53, 12.48s/it]  0%|          | 12/5198 [03:29<17:46:11, 12.34s/it]                                                    {'loss': 1.3275, 'learning_rate': 1.5384615384615387e-06, 'epoch': 0.0}
  0%|          | 12/5198 [03:29<17:46:11, 12.34s/it]  0%|          | 13/5198 [03:41<17:54:37, 12.44s/it]                                                    {'loss': 1.2557, 'learning_rate': 1.6666666666666667e-06, 'epoch': 0.0}
  0%|          | 13/5198 [03:41<17:54:37, 12.44s/it]  0%|          | 14/5198 [03:54<17:52:55, 12.42s/it]                                                    {'loss': 1.1996, 'learning_rate': 1.794871794871795e-06, 'epoch': 0.0}
  0%|          | 14/5198 [03:54<17:52:55, 12.42s/it]  0%|          | 15/5198 [04:05<17:33:48, 12.20s/it]                                                    {'loss': 1.214, 'learning_rate': 1.9230769230769234e-06, 'epoch': 0.0}
  0%|          | 15/5198 [04:05<17:33:48, 12.20s/it]  0%|          | 16/5198 [04:17<17:27:35, 12.13s/it]                                                    {'loss': 1.2576, 'learning_rate': 2.0512820512820513e-06, 'epoch': 0.0}
  0%|          | 16/5198 [04:18<17:27:35, 12.13s/it]  0%|          | 17/5198 [04:30<17:44:40, 12.33s/it]                                                    {'loss': 1.1653, 'learning_rate': 2.1794871794871797e-06, 'epoch': 0.0}
  0%|          | 17/5198 [04:30<17:44:40, 12.33s/it]  0%|          | 18/5198 [04:42<17:23:44, 12.09s/it]                                                    {'loss': 1.134, 'learning_rate': 2.307692307692308e-06, 'epoch': 0.0}
  0%|          | 18/5198 [04:42<17:23:44, 12.09s/it]  0%|          | 19/5198 [04:54<17:35:06, 12.22s/it]                                                    {'loss': 1.1945, 'learning_rate': 2.435897435897436e-06, 'epoch': 0.0}
  0%|          | 19/5198 [04:54<17:35:06, 12.22s/it]  0%|          | 20/5198 [05:07<17:55:20, 12.46s/it]                                                    {'loss': 1.0922, 'learning_rate': 2.564102564102564e-06, 'epoch': 0.0}
  0%|          | 20/5198 [05:07<17:55:20, 12.46s/it]  0%|          | 21/5198 [05:19<17:32:42, 12.20s/it]                                                    {'loss': 1.1828, 'learning_rate': 2.6923076923076923e-06, 'epoch': 0.0}
  0%|          | 21/5198 [05:19<17:32:42, 12.20s/it]  0%|          | 22/5198 [05:31<17:37:20, 12.26s/it]                                                    {'loss': 1.1381, 'learning_rate': 2.8205128205128207e-06, 'epoch': 0.0}
  0%|          | 22/5198 [05:31<17:37:20, 12.26s/it]  0%|          | 23/5198 [05:43<17:22:32, 12.09s/it]                                                    {'loss': 1.0611, 'learning_rate': 2.948717948717949e-06, 'epoch': 0.0}
  0%|          | 23/5198 [05:43<17:22:32, 12.09s/it]  0%|          | 24/5198 [05:55<17:22:42, 12.09s/it]                                                    {'loss': 1.1615, 'learning_rate': 3.0769230769230774e-06, 'epoch': 0.0}
  0%|          | 24/5198 [05:55<17:22:42, 12.09s/it]  0%|          | 25/5198 [06:07<17:28:35, 12.16s/it]                                                    {'loss': 1.1123, 'learning_rate': 3.205128205128206e-06, 'epoch': 0.0}
  0%|          | 25/5198 [06:08<17:28:35, 12.16s/it]  1%|          | 26/5198 [06:20<17:39:42, 12.29s/it]                                                    {'loss': 1.0989, 'learning_rate': 3.3333333333333333e-06, 'epoch': 0.01}
  1%|          | 26/5198 [06:20<17:39:42, 12.29s/it]  1%|          | 27/5198 [06:32<17:39:29, 12.29s/it]                                                    {'loss': 1.1025, 'learning_rate': 3.4615384615384617e-06, 'epoch': 0.01}
  1%|          | 27/5198 [06:32<17:39:29, 12.29s/it]  1%|          | 28/5198 [06:44<17:12:16, 11.98s/it]                                                    {'loss': 1.1018, 'learning_rate': 3.58974358974359e-06, 'epoch': 0.01}
  1%|          | 28/5198 [06:44<17:12:16, 11.98s/it]  1%|          | 29/5198 [06:58<18:24:41, 12.82s/it]                                                    {'loss': 1.0568, 'learning_rate': 3.7179487179487184e-06, 'epoch': 0.01}
  1%|          | 29/5198 [06:58<18:24:41, 12.82s/it]  1%|          | 30/5198 [07:10<17:51:13, 12.44s/it]                                                    {'loss': 1.0555, 'learning_rate': 3.846153846153847e-06, 'epoch': 0.01}
  1%|          | 30/5198 [07:10<17:51:13, 12.44s/it]  1%|          | 31/5198 [07:22<17:35:57, 12.26s/it]                                                    {'loss': 1.1005, 'learning_rate': 3.974358974358974e-06, 'epoch': 0.01}
  1%|          | 31/5198 [07:22<17:35:57, 12.26s/it]  1%|          | 32/5198 [07:34<17:34:14, 12.24s/it]                                                    {'loss': 1.0277, 'learning_rate': 4.102564102564103e-06, 'epoch': 0.01}
  1%|          | 32/5198 [07:34<17:34:14, 12.24s/it]  1%|          | 33/5198 [07:47<17:59:15, 12.54s/it]                                                    {'loss': 1.0523, 'learning_rate': 4.230769230769231e-06, 'epoch': 0.01}
  1%|          | 33/5198 [07:47<17:59:15, 12.54s/it]  1%|          | 34/5198 [07:59<17:42:24, 12.34s/it]                                                    {'loss': 1.0352, 'learning_rate': 4.358974358974359e-06, 'epoch': 0.01}
  1%|          | 34/5198 [07:59<17:42:24, 12.34s/it]  1%|          | 35/5198 [08:13<18:25:03, 12.84s/it]                                                    {'loss': 1.0065, 'learning_rate': 4.487179487179488e-06, 'epoch': 0.01}
  1%|          | 35/5198 [08:13<18:25:03, 12.84s/it]  1%|          | 36/5198 [08:24<17:47:30, 12.41s/it]                                                    {'loss': 1.0416, 'learning_rate': 4.615384615384616e-06, 'epoch': 0.01}
  1%|          | 36/5198 [08:24<17:47:30, 12.41s/it][2024-03-23 15:59:09,776] [WARNING] [stage3.py:1991:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  1%|          | 37/5198 [08:41<19:46:46, 13.80s/it]                                                    {'loss': 0.3082, 'learning_rate': 4.743589743589744e-06, 'epoch': 0.01}
  1%|          | 37/5198 [08:42<19:46:46, 13.80s/it]  1%|          | 38/5198 [08:57<20:37:15, 14.39s/it]                                                    {'loss': 1.0624, 'learning_rate': 4.871794871794872e-06, 'epoch': 0.01}
  1%|          | 38/5198 [08:57<20:37:15, 14.39s/it]  1%|          | 39/5198 [09:09<19:25:50, 13.56s/it]                                                    {'loss': 1.0545, 'learning_rate': 5e-06, 'epoch': 0.01}
  1%|          | 39/5198 [09:09<19:25:50, 13.56s/it]  1%|          | 40/5198 [09:21<18:41:18, 13.04s/it]                                                    {'loss': 1.0279, 'learning_rate': 5.128205128205128e-06, 'epoch': 0.01}
  1%|          | 40/5198 [09:21<18:41:18, 13.04s/it]  1%|          | 41/5198 [09:33<18:17:29, 12.77s/it]                                                    {'loss': 1.0242, 'learning_rate': 5.256410256410257e-06, 'epoch': 0.01}
  1%|          | 41/5198 [09:33<18:17:29, 12.77s/it]  1%|          | 42/5198 [09:45<18:02:53, 12.60s/it]                                                    {'loss': 1.0797, 'learning_rate': 5.384615384615385e-06, 'epoch': 0.01}
  1%|          | 42/5198 [09:45<18:02:53, 12.60s/it]  1%|          | 43/5198 [09:57<17:37:24, 12.31s/it]                                                    {'loss': 0.9773, 'learning_rate': 5.512820512820514e-06, 'epoch': 0.01}
  1%|          | 43/5198 [09:57<17:37:24, 12.31s/it]  1%|          | 44/5198 [10:09<17:43:38, 12.38s/it]                                                    {'loss': 1.0396, 'learning_rate': 5.641025641025641e-06, 'epoch': 0.01}
  1%|          | 44/5198 [10:09<17:43:38, 12.38s/it]  1%|          | 45/5198 [10:21<17:29:07, 12.22s/it]                                                    {'loss': 1.0451, 'learning_rate': 5.769230769230769e-06, 'epoch': 0.01}
  1%|          | 45/5198 [10:21<17:29:07, 12.22s/it]  1%|          | 46/5198 [10:33<17:35:23, 12.29s/it]                                                    {'loss': 1.0021, 'learning_rate': 5.897435897435898e-06, 'epoch': 0.01}
  1%|          | 46/5198 [10:34<17:35:23, 12.29s/it]  1%|          | 47/5198 [10:45<17:24:40, 12.17s/it]                                                    {'loss': 0.9823, 'learning_rate': 6.025641025641026e-06, 'epoch': 0.01}
  1%|          | 47/5198 [10:45<17:24:40, 12.17s/it]  1%|          | 48/5198 [10:57<17:02:03, 11.91s/it]                                                    {'loss': 0.9812, 'learning_rate': 6.153846153846155e-06, 'epoch': 0.01}
  1%|          | 48/5198 [10:57<17:02:03, 11.91s/it]  1%|          | 49/5198 [11:09<17:13:39, 12.04s/it]                                                    {'loss': 0.9823, 'learning_rate': 6.282051282051282e-06, 'epoch': 0.01}
  1%|          | 49/5198 [11:09<17:13:39, 12.04s/it]  1%|          | 50/5198 [11:22<17:46:29, 12.43s/it]                                                    {'loss': 0.9748, 'learning_rate': 6.410256410256412e-06, 'epoch': 0.01}
  1%|          | 50/5198 [11:22<17:46:29, 12.43s/it]  1%|          | 51/5198 [11:34<17:34:49, 12.30s/it]                                                    {'loss': 1.0165, 'learning_rate': 6.538461538461539e-06, 'epoch': 0.01}
  1%|          | 51/5198 [11:34<17:34:49, 12.30s/it]  1%|          | 52/5198 [11:46<17:28:45, 12.23s/it]                                                    {'loss': 1.0109, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.01}
  1%|          | 52/5198 [11:46<17:28:45, 12.23s/it]  1%|          | 53/5198 [11:59<17:26:55, 12.21s/it]                                                    {'loss': 0.9945, 'learning_rate': 6.794871794871796e-06, 'epoch': 0.01}
  1%|          | 53/5198 [11:59<17:26:55, 12.21s/it]  1%|          | 54/5198 [12:11<17:21:39, 12.15s/it]                                                    {'loss': 0.9803, 'learning_rate': 6.923076923076923e-06, 'epoch': 0.01}
  1%|          | 54/5198 [12:11<17:21:39, 12.15s/it]  1%|          | 55/5198 [12:22<17:12:56, 12.05s/it]                                                    {'loss': 1.0122, 'learning_rate': 7.051282051282053e-06, 'epoch': 0.01}
  1%|          | 55/5198 [12:22<17:12:56, 12.05s/it][2024-03-23 16:03:08,573] [WARNING] [stage3.py:1991:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  1%|          | 56/5198 [12:40<19:41:18, 13.78s/it]                                                    {'loss': 0.3198, 'learning_rate': 7.17948717948718e-06, 'epoch': 0.01}
  1%|          | 56/5198 [12:40<19:41:18, 13.78s/it]  1%|          | 57/5198 [12:53<19:04:29, 13.36s/it]                                                    {'loss': 0.9811, 'learning_rate': 7.307692307692308e-06, 'epoch': 0.01}
  1%|          | 57/5198 [12:53<19:04:29, 13.36s/it]  1%|          | 58/5198 [13:04<18:16:03, 12.79s/it]                                                    {'loss': 1.0165, 'learning_rate': 7.435897435897437e-06, 'epoch': 0.01}
  1%|          | 58/5198 [13:04<18:16:03, 12.79s/it]  1%|          | 59/5198 [13:16<17:43:31, 12.42s/it]                                                    {'loss': 1.0327, 'learning_rate': 7.564102564102564e-06, 'epoch': 0.01}
  1%|          | 59/5198 [13:16<17:43:31, 12.42s/it]  1%|          | 60/5198 [13:29<18:06:24, 12.69s/it]                                                    {'loss': 1.0008, 'learning_rate': 7.692307692307694e-06, 'epoch': 0.01}
  1%|          | 60/5198 [13:29<18:06:24, 12.69s/it]  1%|          | 61/5198 [13:42<18:21:51, 12.87s/it]                                                    {'loss': 0.9604, 'learning_rate': 7.820512820512822e-06, 'epoch': 0.01}
  1%|          | 61/5198 [13:42<18:21:51, 12.87s/it]  1%|          | 62/5198 [13:54<17:54:04, 12.55s/it]                                                    {'loss': 0.9777, 'learning_rate': 7.948717948717949e-06, 'epoch': 0.01}
  1%|          | 62/5198 [13:54<17:54:04, 12.55s/it]  1%|          | 63/5198 [14:07<18:00:55, 12.63s/it]                                                    {'loss': 0.9209, 'learning_rate': 8.076923076923077e-06, 'epoch': 0.01}
  1%|          | 63/5198 [14:07<18:00:55, 12.63s/it]  1%|          | 64/5198 [14:19<17:57:19, 12.59s/it]                                                    {'loss': 0.998, 'learning_rate': 8.205128205128205e-06, 'epoch': 0.01}
  1%|          | 64/5198 [14:20<17:57:19, 12.59s/it]  1%|â–         | 65/5198 [14:33<18:34:06, 13.02s/it]                                                    {'loss': 0.9858, 'learning_rate': 8.333333333333334e-06, 'epoch': 0.01}
  1%|â–         | 65/5198 [14:34<18:34:06, 13.02s/it]  1%|â–         | 66/5198 [14:47<18:40:16, 13.10s/it]                                                    {'loss': 0.9633, 'learning_rate': 8.461538461538462e-06, 'epoch': 0.01}
  1%|â–         | 66/5198 [14:47<18:40:16, 13.10s/it]  1%|â–         | 67/5198 [14:59<18:17:55, 12.84s/it]                                                    {'loss': 0.8938, 'learning_rate': 8.58974358974359e-06, 'epoch': 0.01}
  1%|â–         | 67/5198 [14:59<18:17:55, 12.84s/it]  1%|â–         | 68/5198 [15:11<18:02:40, 12.66s/it]                                                    {'loss': 0.9814, 'learning_rate': 8.717948717948719e-06, 'epoch': 0.01}
  1%|â–         | 68/5198 [15:11<18:02:40, 12.66s/it]  1%|â–         | 69/5198 [15:23<17:48:05, 12.49s/it]                                                    {'loss': 0.9737, 'learning_rate': 8.846153846153847e-06, 'epoch': 0.01}
  1%|â–         | 69/5198 [15:23<17:48:05, 12.49s/it]  1%|â–         | 70/5198 [15:35<17:39:27, 12.40s/it]                                                    {'loss': 0.9669, 'learning_rate': 8.974358974358976e-06, 'epoch': 0.01}
  1%|â–         | 70/5198 [15:35<17:39:27, 12.40s/it]  1%|â–         | 71/5198 [15:48<17:48:33, 12.51s/it]                                                    {'loss': 0.9473, 'learning_rate': 9.102564102564104e-06, 'epoch': 0.01}
  1%|â–         | 71/5198 [15:48<17:48:33, 12.51s/it]  1%|â–         | 72/5198 [16:01<17:57:23, 12.61s/it]                                                    {'loss': 0.9139, 'learning_rate': 9.230769230769232e-06, 'epoch': 0.01}
  1%|â–         | 72/5198 [16:01<17:57:23, 12.61s/it]  1%|â–         | 73/5198 [16:13<17:35:18, 12.35s/it]                                                    {'loss': 0.992, 'learning_rate': 9.358974358974359e-06, 'epoch': 0.01}
  1%|â–         | 73/5198 [16:13<17:35:18, 12.35s/it]  1%|â–         | 74/5198 [16:24<17:07:38, 12.03s/it]                                                    {'loss': 0.9473, 'learning_rate': 9.487179487179487e-06, 'epoch': 0.01}
  1%|â–         | 74/5198 [16:24<17:07:38, 12.03s/it]  1%|â–         | 75/5198 [16:36<17:16:55, 12.14s/it]                                                    {'loss': 0.9332, 'learning_rate': 9.615384615384616e-06, 'epoch': 0.01}
  1%|â–         | 75/5198 [16:37<17:16:55, 12.14s/it]  1%|â–         | 76/5198 [16:48<17:05:41, 12.02s/it]                                                    {'loss': 0.9407, 'learning_rate': 9.743589743589744e-06, 'epoch': 0.01}
  1%|â–         | 76/5198 [16:48<17:05:41, 12.02s/it]  1%|â–         | 77/5198 [17:00<17:09:46, 12.07s/it]                                                    {'loss': 0.9078, 'learning_rate': 9.871794871794872e-06, 'epoch': 0.01}
  1%|â–         | 77/5198 [17:00<17:09:46, 12.07s/it]  2%|â–         | 78/5198 [17:12<17:05:12, 12.01s/it]                                                    {'loss': 0.9793, 'learning_rate': 1e-05, 'epoch': 0.02}
  2%|â–         | 78/5198 [17:12<17:05:12, 12.01s/it]  2%|â–         | 79/5198 [17:24<17:09:52, 12.07s/it]                                                    {'loss': 1.0201, 'learning_rate': 1.012820512820513e-05, 'epoch': 0.02}
  2%|â–         | 79/5198 [17:25<17:09:52, 12.07s/it]  2%|â–         | 80/5198 [17:36<17:04:18, 12.01s/it]                                                    {'loss': 0.9602, 'learning_rate': 1.0256410256410256e-05, 'epoch': 0.02}
  2%|â–         | 80/5198 [17:36<17:04:18, 12.01s/it]  2%|â–         | 81/5198 [17:48<16:55:00, 11.90s/it]                                                    {'loss': 1.0101, 'learning_rate': 1.0384615384615386e-05, 'epoch': 0.02}
  2%|â–         | 81/5198 [17:48<16:55:00, 11.90s/it]  2%|â–         | 82/5198 [18:00<16:49:29, 11.84s/it]                                                    {'loss': 1.0334, 'learning_rate': 1.0512820512820514e-05, 'epoch': 0.02}
  2%|â–         | 82/5198 [18:00<16:49:29, 11.84s/it]  2%|â–         | 83/5198 [18:12<16:50:40, 11.86s/it]                                                    {'loss': 1.021, 'learning_rate': 1.0641025641025643e-05, 'epoch': 0.02}
  2%|â–         | 83/5198 [18:12<16:50:40, 11.86s/it]  2%|â–         | 84/5198 [18:24<17:06:14, 12.04s/it]                                                    {'loss': 0.9727, 'learning_rate': 1.076923076923077e-05, 'epoch': 0.02}
  2%|â–         | 84/5198 [18:24<17:06:14, 12.04s/it]  2%|â–         | 85/5198 [18:36<16:55:06, 11.91s/it]                                                    {'loss': 0.948, 'learning_rate': 1.0897435897435898e-05, 'epoch': 0.02}
  2%|â–         | 85/5198 [18:36<16:55:06, 11.91s/it]  2%|â–         | 86/5198 [18:47<16:27:36, 11.59s/it]                                                    {'loss': 0.9438, 'learning_rate': 1.1025641025641028e-05, 'epoch': 0.02}
  2%|â–         | 86/5198 [18:47<16:27:36, 11.59s/it]  2%|â–         | 87/5198 [18:59<16:44:22, 11.79s/it]                                                    {'loss': 0.9991, 'learning_rate': 1.1153846153846154e-05, 'epoch': 0.02}
  2%|â–         | 87/5198 [18:59<16:44:22, 11.79s/it]  2%|â–         | 88/5198 [19:13<17:45:13, 12.51s/it]                                                    {'loss': 0.9683, 'learning_rate': 1.1282051282051283e-05, 'epoch': 0.02}
  2%|â–         | 88/5198 [19:13<17:45:13, 12.51s/it]  2%|â–         | 89/5198 [19:30<19:49:20, 13.97s/it]                                                    {'loss': 0.2953, 'learning_rate': 1.1410256410256411e-05, 'epoch': 0.02}
  2%|â–         | 89/5198 [19:30<19:49:20, 13.97s/it]  2%|â–         | 90/5198 [19:42<18:52:32, 13.30s/it]                                                    {'loss': 0.9142, 'learning_rate': 1.1538461538461538e-05, 'epoch': 0.02}
  2%|â–         | 90/5198 [19:42<18:52:32, 13.30s/it]  2%|â–         | 91/5198 [19:54<18:16:47, 12.89s/it]                                                    {'loss': 0.9773, 'learning_rate': 1.1666666666666668e-05, 'epoch': 0.02}
  2%|â–         | 91/5198 [19:54<18:16:47, 12.89s/it]  2%|â–         | 92/5198 [20:06<17:44:15, 12.51s/it]                                                    {'loss': 0.9847, 'learning_rate': 1.1794871794871796e-05, 'epoch': 0.02}
  2%|â–         | 92/5198 [20:06<17:44:15, 12.51s/it]  2%|â–         | 93/5198 [20:18<17:42:49, 12.49s/it]                                                    {'loss': 0.9908, 'learning_rate': 1.1923076923076925e-05, 'epoch': 0.02}
  2%|â–         | 93/5198 [20:18<17:42:49, 12.49s/it]  2%|â–         | 94/5198 [20:30<17:29:29, 12.34s/it]                                                    {'loss': 0.9425, 'learning_rate': 1.2051282051282051e-05, 'epoch': 0.02}
  2%|â–         | 94/5198 [20:30<17:29:29, 12.34s/it]  2%|â–         | 95/5198 [20:47<19:29:11, 13.75s/it]                                                    {'loss': 0.308, 'learning_rate': 1.217948717948718e-05, 'epoch': 0.02}
  2%|â–         | 95/5198 [20:47<19:29:11, 13.75s/it]  2%|â–         | 96/5198 [20:59<18:49:25, 13.28s/it]                                                    {'loss': 0.9524, 'learning_rate': 1.230769230769231e-05, 'epoch': 0.02}
  2%|â–         | 96/5198 [20:59<18:49:25, 13.28s/it]  2%|â–         | 97/5198 [21:12<18:35:47, 13.12s/it]                                                    {'loss': 0.9553, 'learning_rate': 1.2435897435897436e-05, 'epoch': 0.02}
  2%|â–         | 97/5198 [21:12<18:35:47, 13.12s/it]  2%|â–         | 98/5198 [21:26<19:00:58, 13.42s/it]                                                    {'loss': 0.9815, 'learning_rate': 1.2564102564102565e-05, 'epoch': 0.02}
  2%|â–         | 98/5198 [21:26<19:00:58, 13.42s/it]  2%|â–         | 99/5198 [21:40<18:59:57, 13.41s/it]                                                    {'loss': 0.8906, 'learning_rate': 1.2692307692307693e-05, 'epoch': 0.02}
  2%|â–         | 99/5198 [21:40<18:59:57, 13.41s/it]  2%|â–         | 100/5198 [21:57<20:33:39, 14.52s/it]                                                     {'loss': 0.2938, 'learning_rate': 1.2820512820512823e-05, 'epoch': 0.02}
  2%|â–         | 100/5198 [21:57<20:33:39, 14.52s/it]  2%|â–         | 101/5198 [22:09<19:35:39, 13.84s/it]                                                     {'loss': 0.9388, 'learning_rate': 1.294871794871795e-05, 'epoch': 0.02}
  2%|â–         | 101/5198 [22:09<19:35:39, 13.84s/it]  2%|â–         | 102/5198 [22:20<18:28:00, 13.05s/it]                                                     {'loss': 1.0488, 'learning_rate': 1.3076923076923078e-05, 'epoch': 0.02}
  2%|â–         | 102/5198 [22:20<18:28:00, 13.05s/it]  2%|â–         | 103/5198 [22:34<18:44:30, 13.24s/it]                                                     {'loss': 0.98, 'learning_rate': 1.3205128205128207e-05, 'epoch': 0.02}
  2%|â–         | 103/5198 [22:34<18:44:30, 13.24s/it]  2%|â–         | 104/5198 [22:49<19:31:55, 13.80s/it]                                                     {'loss': 0.967, 'learning_rate': 1.3333333333333333e-05, 'epoch': 0.02}
  2%|â–         | 104/5198 [22:49<19:31:55, 13.80s/it]  2%|â–         | 105/5198 [23:06<20:58:27, 14.83s/it]                                                     {'loss': 0.2655, 'learning_rate': 1.3461538461538463e-05, 'epoch': 0.02}
  2%|â–         | 105/5198 [23:06<20:58:27, 14.83s/it]  2%|â–         | 106/5198 [23:18<19:43:36, 13.95s/it]                                                     {'loss': 0.9482, 'learning_rate': 1.3589743589743592e-05, 'epoch': 0.02}
  2%|â–         | 106/5198 [23:18<19:43:36, 13.95s/it]  2%|â–         | 107/5198 [23:29<18:40:05, 13.20s/it]                                                     {'loss': 1.0179, 'learning_rate': 1.3717948717948718e-05, 'epoch': 0.02}
  2%|â–         | 107/5198 [23:30<18:40:05, 13.20s/it]  2%|â–         | 108/5198 [23:41<18:06:46, 12.81s/it]                                                     {'loss': 0.9437, 'learning_rate': 1.3846153846153847e-05, 'epoch': 0.02}
  2%|â–         | 108/5198 [23:41<18:06:46, 12.81s/it]  2%|â–         | 109/5198 [23:53<17:32:34, 12.41s/it]                                                     {'loss': 0.9769, 'learning_rate': 1.3974358974358975e-05, 'epoch': 0.02}
  2%|â–         | 109/5198 [23:53<17:32:34, 12.41s/it]  2%|â–         | 110/5198 [24:05<17:19:36, 12.26s/it]                                                     {'loss': 0.966, 'learning_rate': 1.4102564102564105e-05, 'epoch': 0.02}
  2%|â–         | 110/5198 [24:05<17:19:36, 12.26s/it]  2%|â–         | 111/5198 [24:17<17:12:59, 12.18s/it]                                                     {'loss': 0.9476, 'learning_rate': 1.4230769230769232e-05, 'epoch': 0.02}
  2%|â–         | 111/5198 [24:17<17:12:59, 12.18s/it]  2%|â–         | 112/5198 [24:29<17:11:26, 12.17s/it]                                                     {'loss': 1.0301, 'learning_rate': 1.435897435897436e-05, 'epoch': 0.02}
  2%|â–         | 112/5198 [24:29<17:11:26, 12.17s/it]  2%|â–         | 113/5198 [24:41<17:19:51, 12.27s/it]                                                     {'loss': 0.9374, 'learning_rate': 1.4487179487179489e-05, 'epoch': 0.02}
  2%|â–         | 113/5198 [24:41<17:19:51, 12.27s/it]  2%|â–         | 114/5198 [24:53<17:07:40, 12.13s/it]                                                     {'loss': 0.9659, 'learning_rate': 1.4615384615384615e-05, 'epoch': 0.02}
  2%|â–         | 114/5198 [24:53<17:07:40, 12.13s/it]  2%|â–         | 115/5198 [25:06<17:16:13, 12.23s/it]                                                     {'loss': 0.9616, 'learning_rate': 1.4743589743589745e-05, 'epoch': 0.02}
  2%|â–         | 115/5198 [25:06<17:16:13, 12.23s/it]  2%|â–         | 116/5198 [25:22<18:50:56, 13.35s/it]                                                     {'loss': 0.2955, 'learning_rate': 1.4871794871794874e-05, 'epoch': 0.02}
  2%|â–         | 116/5198 [25:22<18:50:56, 13.35s/it]  2%|â–         | 117/5198 [25:34<18:38:03, 13.20s/it]                                                     {'loss': 0.9837, 'learning_rate': 1.5000000000000002e-05, 'epoch': 0.02}
  2%|â–         | 117/5198 [25:35<18:38:03, 13.20s/it]  2%|â–         | 118/5198 [25:46<17:57:51, 12.73s/it]                                                     {'loss': 0.9486, 'learning_rate': 1.5128205128205129e-05, 'epoch': 0.02}
  2%|â–         | 118/5198 [25:46<17:57:51, 12.73s/it]  2%|â–         | 119/5198 [25:58<17:32:26, 12.43s/it]                                                     {'loss': 1.0566, 'learning_rate': 1.5256410256410257e-05, 'epoch': 0.02}
  2%|â–         | 119/5198 [25:58<17:32:26, 12.43s/it]  2%|â–         | 120/5198 [26:12<18:05:32, 12.83s/it]                                                     {'loss': 0.9361, 'learning_rate': 1.5384615384615387e-05, 'epoch': 0.02}
  2%|â–         | 120/5198 [26:12<18:05:32, 12.83s/it]  2%|â–         | 121/5198 [26:25<18:12:53, 12.92s/it]                                                     {'loss': 0.9947, 'learning_rate': 1.5512820512820516e-05, 'epoch': 0.02}
  2%|â–         | 121/5198 [26:25<18:12:53, 12.92s/it]  2%|â–         | 122/5198 [26:38<18:16:23, 12.96s/it]                                                     {'loss': 1.0173, 'learning_rate': 1.5641025641025644e-05, 'epoch': 0.02}
  2%|â–         | 122/5198 [26:38<18:16:23, 12.96s/it]  2%|â–         | 123/5198 [26:51<18:23:54, 13.05s/it]                                                     {'loss': 0.9077, 'learning_rate': 1.576923076923077e-05, 'epoch': 0.02}
  2%|â–         | 123/5198 [26:51<18:23:54, 13.05s/it]  2%|â–         | 124/5198 [27:04<18:12:02, 12.91s/it]                                                     {'loss': 0.9464, 'learning_rate': 1.5897435897435897e-05, 'epoch': 0.02}
  2%|â–         | 124/5198 [27:04<18:12:02, 12.91s/it]  2%|â–         | 125/5198 [27:16<17:53:51, 12.70s/it]                                                     {'loss': 0.9686, 'learning_rate': 1.602564102564103e-05, 'epoch': 0.02}
  2%|â–         | 125/5198 [27:16<17:53:51, 12.70s/it]  2%|â–         | 126/5198 [27:28<17:34:46, 12.48s/it]                                                     {'loss': 0.9514, 'learning_rate': 1.6153846153846154e-05, 'epoch': 0.02}
  2%|â–         | 126/5198 [27:28<17:34:46, 12.48s/it]  2%|â–         | 127/5198 [27:39<17:03:14, 12.11s/it]                                                     {'loss': 1.0012, 'learning_rate': 1.6282051282051282e-05, 'epoch': 0.02}
  2%|â–         | 127/5198 [27:39<17:03:14, 12.11s/it]  2%|â–         | 128/5198 [27:51<17:07:56, 12.17s/it]                                                     {'loss': 0.8572, 'learning_rate': 1.641025641025641e-05, 'epoch': 0.02}
  2%|â–         | 128/5198 [27:51<17:07:56, 12.17s/it]  2%|â–         | 129/5198 [28:04<17:20:36, 12.32s/it]                                                     {'loss': 0.9287, 'learning_rate': 1.653846153846154e-05, 'epoch': 0.02}
  2%|â–         | 129/5198 [28:04<17:20:36, 12.32s/it]  3%|â–Ž         | 130/5198 [28:16<17:05:41, 12.14s/it]                                                     {'loss': 0.9538, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.03}
  3%|â–Ž         | 130/5198 [28:16<17:05:41, 12.14s/it]  3%|â–Ž         | 131/5198 [28:28<17:01:56, 12.10s/it]                                                     {'loss': 0.9723, 'learning_rate': 1.6794871794871796e-05, 'epoch': 0.03}
  3%|â–Ž         | 131/5198 [28:28<17:01:56, 12.10s/it]  3%|â–Ž         | 132/5198 [28:41<17:38:02, 12.53s/it]                                                     {'loss': 0.9145, 'learning_rate': 1.6923076923076924e-05, 'epoch': 0.03}
  3%|â–Ž         | 132/5198 [28:41<17:38:02, 12.53s/it]  3%|â–Ž         | 133/5198 [28:54<17:44:22, 12.61s/it]                                                     {'loss': 0.9104, 'learning_rate': 1.7051282051282053e-05, 'epoch': 0.03}
  3%|â–Ž         | 133/5198 [28:54<17:44:22, 12.61s/it]  3%|â–Ž         | 134/5198 [29:06<17:28:51, 12.43s/it]                                                     {'loss': 0.9424, 'learning_rate': 1.717948717948718e-05, 'epoch': 0.03}
  3%|â–Ž         | 134/5198 [29:06<17:28:51, 12.43s/it]  3%|â–Ž         | 135/5198 [29:17<17:02:29, 12.12s/it]                                                     {'loss': 0.93, 'learning_rate': 1.730769230769231e-05, 'epoch': 0.03}
  3%|â–Ž         | 135/5198 [29:18<17:02:29, 12.12s/it]  3%|â–Ž         | 136/5198 [29:32<18:09:43, 12.92s/it]                                                     {'loss': 0.9047, 'learning_rate': 1.7435897435897438e-05, 'epoch': 0.03}
  3%|â–Ž         | 136/5198 [29:32<18:09:43, 12.92s/it]  3%|â–Ž         | 137/5198 [29:44<17:51:47, 12.71s/it]                                                     {'loss': 0.9883, 'learning_rate': 1.7564102564102566e-05, 'epoch': 0.03}
  3%|â–Ž         | 137/5198 [29:45<17:51:47, 12.71s/it]  3%|â–Ž         | 138/5198 [29:57<17:42:14, 12.60s/it]                                                     {'loss': 1.0013, 'learning_rate': 1.7692307692307694e-05, 'epoch': 0.03}
  3%|â–Ž         | 138/5198 [29:57<17:42:14, 12.60s/it]  3%|â–Ž         | 139/5198 [30:09<17:31:02, 12.47s/it]                                                     {'loss': 0.9289, 'learning_rate': 1.7820512820512823e-05, 'epoch': 0.03}
  3%|â–Ž         | 139/5198 [30:09<17:31:02, 12.47s/it]  3%|â–Ž         | 140/5198 [30:21<17:08:06, 12.20s/it]                                                     {'loss': 1.0026, 'learning_rate': 1.794871794871795e-05, 'epoch': 0.03}
  3%|â–Ž         | 140/5198 [30:21<17:08:06, 12.20s/it]  3%|â–Ž         | 141/5198 [30:33<17:18:39, 12.32s/it]                                                     {'loss': 0.9686, 'learning_rate': 1.807692307692308e-05, 'epoch': 0.03}
  3%|â–Ž         | 141/5198 [30:33<17:18:39, 12.32s/it]  3%|â–Ž         | 142/5198 [30:46<17:31:39, 12.48s/it]                                                     {'loss': 0.9698, 'learning_rate': 1.8205128205128208e-05, 'epoch': 0.03}
  3%|â–Ž         | 142/5198 [30:46<17:31:39, 12.48s/it]  3%|â–Ž         | 143/5198 [30:57<16:58:09, 12.08s/it]                                                     {'loss': 0.9957, 'learning_rate': 1.8333333333333333e-05, 'epoch': 0.03}
  3%|â–Ž         | 143/5198 [30:57<16:58:09, 12.08s/it]  3%|â–Ž         | 144/5198 [31:10<17:11:00, 12.24s/it]                                                     {'loss': 0.9759, 'learning_rate': 1.8461538461538465e-05, 'epoch': 0.03}
  3%|â–Ž         | 144/5198 [31:10<17:11:00, 12.24s/it]  3%|â–Ž         | 145/5198 [31:22<17:01:10, 12.13s/it]                                                     {'loss': 0.9558, 'learning_rate': 1.8589743589743593e-05, 'epoch': 0.03}
  3%|â–Ž         | 145/5198 [31:22<17:01:10, 12.13s/it]  3%|â–Ž         | 146/5198 [31:34<17:08:29, 12.21s/it]                                                     {'loss': 0.9599, 'learning_rate': 1.8717948717948718e-05, 'epoch': 0.03}
  3%|â–Ž         | 146/5198 [31:34<17:08:29, 12.21s/it]  3%|â–Ž         | 147/5198 [31:46<17:01:59, 12.14s/it]                                                     {'loss': 0.9257, 'learning_rate': 1.8846153846153846e-05, 'epoch': 0.03}
  3%|â–Ž         | 147/5198 [31:46<17:01:59, 12.14s/it]  3%|â–Ž         | 148/5198 [31:58<17:02:29, 12.15s/it]                                                     {'loss': 0.9332, 'learning_rate': 1.8974358974358975e-05, 'epoch': 0.03}
  3%|â–Ž         | 148/5198 [31:58<17:02:29, 12.15s/it]  3%|â–Ž         | 149/5198 [32:10<16:59:04, 12.11s/it]                                                     {'loss': 0.942, 'learning_rate': 1.9102564102564106e-05, 'epoch': 0.03}
  3%|â–Ž         | 149/5198 [32:10<16:59:04, 12.11s/it]  3%|â–Ž         | 150/5198 [32:22<16:54:41, 12.06s/it]                                                     {'loss': 0.9376, 'learning_rate': 1.923076923076923e-05, 'epoch': 0.03}
  3%|â–Ž         | 150/5198 [32:22<16:54:41, 12.06s/it]  3%|â–Ž         | 151/5198 [32:36<17:33:41, 12.53s/it]                                                     {'loss': 0.9864, 'learning_rate': 1.935897435897436e-05, 'epoch': 0.03}
  3%|â–Ž         | 151/5198 [32:36<17:33:41, 12.53s/it]  3%|â–Ž         | 152/5198 [32:53<19:41:29, 14.05s/it]                                                     {'loss': 0.2928, 'learning_rate': 1.9487179487179488e-05, 'epoch': 0.03}
  3%|â–Ž         | 152/5198 [32:53<19:41:29, 14.05s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (2090 > 2048). Running this sequence through the model will result in indexing errors
  3%|â–Ž         | 153/5198 [33:05<18:44:02, 13.37s/it]                                                     {'loss': 0.9871, 'learning_rate': 1.9615384615384617e-05, 'epoch': 0.03}
  3%|â–Ž         | 153/5198 [33:05<18:44:02, 13.37s/it]  3%|â–Ž         | 154/5198 [33:17<18:01:27, 12.86s/it]                                                     {'loss': 0.9034, 'learning_rate': 1.9743589743589745e-05, 'epoch': 0.03}
  3%|â–Ž         | 154/5198 [33:17<18:01:27, 12.86s/it]  3%|â–Ž         | 155/5198 [33:29<17:44:08, 12.66s/it]                                                     {'loss': 0.9026, 'learning_rate': 1.9871794871794873e-05, 'epoch': 0.03}
  3%|â–Ž         | 155/5198 [33:29<17:44:08, 12.66s/it]  3%|â–Ž         | 156/5198 [33:41<17:24:40, 12.43s/it]                                                     {'loss': 1.0158, 'learning_rate': 2e-05, 'epoch': 0.03}
  3%|â–Ž         | 156/5198 [33:41<17:24:40, 12.43s/it]  3%|â–Ž         | 157/5198 [33:53<17:04:47, 12.20s/it]                                                     {'loss': 0.9671, 'learning_rate': 1.9999998058827844e-05, 'epoch': 0.03}
  3%|â–Ž         | 157/5198 [33:53<17:04:47, 12.20s/it]  3%|â–Ž         | 158/5198 [34:05<16:58:05, 12.12s/it]                                                     {'loss': 0.9864, 'learning_rate': 1.9999992235312136e-05, 'epoch': 0.03}
  3%|â–Ž         | 158/5198 [34:05<16:58:05, 12.12s/it]  3%|â–Ž         | 159/5198 [34:17<17:17:38, 12.36s/it]                                                     {'loss': 0.9362, 'learning_rate': 1.9999982529455127e-05, 'epoch': 0.03}
  3%|â–Ž         | 159/5198 [34:17<17:17:38, 12.36s/it]  3%|â–Ž         | 160/5198 [34:30<17:14:16, 12.32s/it]                                                     {'loss': 0.9452, 'learning_rate': 1.9999968941260596e-05, 'epoch': 0.03}
  3%|â–Ž         | 160/5198 [34:30<17:14:16, 12.32s/it]  3%|â–Ž         | 161/5198 [34:42<17:09:51, 12.27s/it]                                                     {'loss': 0.9648, 'learning_rate': 1.9999951470733808e-05, 'epoch': 0.03}
  3%|â–Ž         | 161/5198 [34:42<17:09:51, 12.27s/it]  3%|â–Ž         | 162/5198 [34:57<18:19:01, 13.09s/it]                                                     {'loss': 0.9879, 'learning_rate': 1.9999930117881548e-05, 'epoch': 0.03}
  3%|â–Ž         | 162/5198 [34:57<18:19:01, 13.09s/it]  3%|â–Ž         | 163/5198 [35:10<18:12:42, 13.02s/it]                                                     {'loss': 0.9618, 'learning_rate': 1.9999904882712115e-05, 'epoch': 0.03}
  3%|â–Ž         | 163/5198 [35:10<18:12:42, 13.02s/it]  3%|â–Ž         | 164/5198 [35:27<20:04:52, 14.36s/it]                                                     {'loss': 0.3373, 'learning_rate': 1.99998757652353e-05, 'epoch': 0.03}
  3%|â–Ž         | 164/5198 [35:27<20:04:52, 14.36s/it]  3%|â–Ž         | 165/5198 [35:39<19:00:35, 13.60s/it]                                                     {'loss': 0.9582, 'learning_rate': 1.9999842765462403e-05, 'epoch': 0.03}
  3%|â–Ž         | 165/5198 [35:39<19:00:35, 13.60s/it]  3%|â–Ž         | 166/5198 [35:52<18:37:08, 13.32s/it]                                                     {'loss': 0.9661, 'learning_rate': 1.999980588340624e-05, 'epoch': 0.03}
  3%|â–Ž         | 166/5198 [35:52<18:37:08, 13.32s/it]  3%|â–Ž         | 167/5198 [36:04<18:04:14, 12.93s/it]                                                     {'loss': 0.9775, 'learning_rate': 1.9999765119081132e-05, 'epoch': 0.03}
  3%|â–Ž         | 167/5198 [36:04<18:04:14, 12.93s/it]  3%|â–Ž         | 168/5198 [36:17<18:05:51, 12.95s/it]                                                     {'loss': 0.9381, 'learning_rate': 1.9999720472502902e-05, 'epoch': 0.03}
  3%|â–Ž         | 168/5198 [36:17<18:05:51, 12.95s/it]  3%|â–Ž         | 169/5198 [36:30<18:10:27, 13.01s/it]                                                     {'loss': 0.9514, 'learning_rate': 1.9999671943688885e-05, 'epoch': 0.03}
  3%|â–Ž         | 169/5198 [36:30<18:10:27, 13.01s/it]  3%|â–Ž         | 170/5198 [36:42<17:51:09, 12.78s/it]                                                     {'loss': 0.9268, 'learning_rate': 1.9999619532657915e-05, 'epoch': 0.03}
  3%|â–Ž         | 170/5198 [36:42<17:51:09, 12.78s/it]  3%|â–Ž         | 171/5198 [36:54<17:19:32, 12.41s/it]                                                     {'loss': 0.8377, 'learning_rate': 1.9999563239430352e-05, 'epoch': 0.03}
  3%|â–Ž         | 171/5198 [36:54<17:19:32, 12.41s/it]  3%|â–Ž         | 172/5198 [37:06<17:15:37, 12.36s/it]                                                     {'loss': 0.9826, 'learning_rate': 1.9999503064028043e-05, 'epoch': 0.03}
  3%|â–Ž         | 172/5198 [37:06<17:15:37, 12.36s/it]  3%|â–Ž         | 173/5198 [37:18<17:11:45, 12.32s/it]                                                     {'loss': 0.937, 'learning_rate': 1.999943900647435e-05, 'epoch': 0.03}
  3%|â–Ž         | 173/5198 [37:18<17:11:45, 12.32s/it]  3%|â–Ž         | 174/5198 [37:31<17:15:15, 12.36s/it]                                                     {'loss': 0.9651, 'learning_rate': 1.9999371066794146e-05, 'epoch': 0.03}
  3%|â–Ž         | 174/5198 [37:31<17:15:15, 12.36s/it]  3%|â–Ž         | 175/5198 [37:42<16:53:26, 12.11s/it]                                                     {'loss': 0.967, 'learning_rate': 1.9999299245013805e-05, 'epoch': 0.03}
  3%|â–Ž         | 175/5198 [37:42<16:53:26, 12.11s/it]  3%|â–Ž         | 176/5198 [37:54<16:43:23, 11.99s/it]                                                     {'loss': 0.9328, 'learning_rate': 1.999922354116121e-05, 'epoch': 0.03}
  3%|â–Ž         | 176/5198 [37:54<16:43:23, 11.99s/it]  3%|â–Ž         | 177/5198 [38:06<16:54:23, 12.12s/it]                                                     {'loss': 0.9359, 'learning_rate': 1.999914395526575e-05, 'epoch': 0.03}
  3%|â–Ž         | 177/5198 [38:06<16:54:23, 12.12s/it]  3%|â–Ž         | 178/5198 [38:18<16:52:24, 12.10s/it]                                                     {'loss': 0.899, 'learning_rate': 1.9999060487358333e-05, 'epoch': 0.03}
  3%|â–Ž         | 178/5198 [38:18<16:52:24, 12.10s/it]  3%|â–Ž         | 179/5198 [38:30<16:39:39, 11.95s/it]                                                     {'loss': 0.9744, 'learning_rate': 1.9998973137471352e-05, 'epoch': 0.03}
  3%|â–Ž         | 179/5198 [38:30<16:39:39, 11.95s/it]  3%|â–Ž         | 180/5198 [38:47<18:42:13, 13.42s/it]                                                     {'loss': 0.3043, 'learning_rate': 1.9998881905638727e-05, 'epoch': 0.03}
  3%|â–Ž         | 180/5198 [38:47<18:42:13, 13.42s/it]  3%|â–Ž         | 181/5198 [39:00<18:29:40, 13.27s/it]                                                     {'loss': 0.9378, 'learning_rate': 1.9998786791895874e-05, 'epoch': 0.03}
  3%|â–Ž         | 181/5198 [39:00<18:29:40, 13.27s/it]  4%|â–Ž         | 182/5198 [39:12<18:17:42, 13.13s/it]                                                     {'loss': 1.0093, 'learning_rate': 1.999868779627972e-05, 'epoch': 0.04}
  4%|â–Ž         | 182/5198 [39:13<18:17:42, 13.13s/it]  4%|â–Ž         | 183/5198 [39:27<19:01:10, 13.65s/it]                                                     {'loss': 0.9305, 'learning_rate': 1.9998584918828695e-05, 'epoch': 0.04}
  4%|â–Ž         | 183/5198 [39:27<19:01:10, 13.65s/it]  4%|â–Ž         | 184/5198 [39:39<18:23:46, 13.21s/it]                                                     {'loss': 0.9356, 'learning_rate': 1.9998478159582747e-05, 'epoch': 0.04}
  4%|â–Ž         | 184/5198 [39:40<18:23:46, 13.21s/it]  4%|â–Ž         | 185/5198 [39:51<17:41:45, 12.71s/it]                                                     {'loss': 0.99, 'learning_rate': 1.999836751858332e-05, 'epoch': 0.04}
  4%|â–Ž         | 185/5198 [39:51<17:41:45, 12.71s/it]  4%|â–Ž         | 186/5198 [40:03<17:21:50, 12.47s/it]                                                     {'loss': 0.9725, 'learning_rate': 1.9998252995873367e-05, 'epoch': 0.04}
  4%|â–Ž         | 186/5198 [40:03<17:21:50, 12.47s/it]  4%|â–Ž         | 187/5198 [40:21<19:40:32, 14.14s/it]                                                     {'loss': 0.3367, 'learning_rate': 1.999813459149735e-05, 'epoch': 0.04}
  4%|â–Ž         | 187/5198 [40:21<19:40:32, 14.14s/it]  4%|â–Ž         | 188/5198 [40:33<18:45:36, 13.48s/it]                                                     {'loss': 0.9196, 'learning_rate': 1.9998012305501243e-05, 'epoch': 0.04}
  4%|â–Ž         | 188/5198 [40:33<18:45:36, 13.48s/it]  4%|â–Ž         | 189/5198 [40:46<18:36:50, 13.38s/it]                                                     {'loss': 0.8689, 'learning_rate': 1.999788613793251e-05, 'epoch': 0.04}
  4%|â–Ž         | 189/5198 [40:46<18:36:50, 13.38s/it]  4%|â–Ž         | 190/5198 [41:02<19:43:38, 14.18s/it]                                                     {'loss': 0.2969, 'learning_rate': 1.999775608884015e-05, 'epoch': 0.04}
  4%|â–Ž         | 190/5198 [41:02<19:43:38, 14.18s/it]  4%|â–Ž         | 191/5198 [41:15<18:59:21, 13.65s/it]                                                     {'loss': 0.9307, 'learning_rate': 1.9997622158274635e-05, 'epoch': 0.04}
  4%|â–Ž         | 191/5198 [41:15<18:59:21, 13.65s/it]  4%|â–Ž         | 192/5198 [41:26<18:06:48, 13.03s/it]                                                     {'loss': 0.9381, 'learning_rate': 1.9997484346287973e-05, 'epoch': 0.04}
  4%|â–Ž         | 192/5198 [41:26<18:06:48, 13.03s/it]  4%|â–Ž         | 193/5198 [41:38<17:38:06, 12.68s/it]                                                     {'loss': 1.0292, 'learning_rate': 1.9997342652933668e-05, 'epoch': 0.04}
  4%|â–Ž         | 193/5198 [41:38<17:38:06, 12.68s/it]  4%|â–Ž         | 194/5198 [41:52<18:07:12, 13.04s/it]                                                     {'loss': 0.9721, 'learning_rate': 1.9997197078266723e-05, 'epoch': 0.04}
  4%|â–Ž         | 194/5198 [41:52<18:07:12, 13.04s/it]  4%|â–         | 195/5198 [42:04<17:52:30, 12.86s/it]                                                     {'loss': 0.9125, 'learning_rate': 1.999704762234366e-05, 'epoch': 0.04}
  4%|â–         | 195/5198 [42:04<17:52:30, 12.86s/it]  4%|â–         | 196/5198 [42:19<18:26:56, 13.28s/it]                                                     {'loss': 0.9808, 'learning_rate': 1.99968942852225e-05, 'epoch': 0.04}
  4%|â–         | 196/5198 [42:19<18:26:56, 13.28s/it]  4%|â–         | 197/5198 [42:31<18:11:51, 13.10s/it]                                                     {'loss': 0.9883, 'learning_rate': 1.9996737066962778e-05, 'epoch': 0.04}
  4%|â–         | 197/5198 [42:31<18:11:51, 13.10s/it]  4%|â–         | 198/5198 [42:44<17:53:50, 12.89s/it]                                                     {'loss': 0.967, 'learning_rate': 1.9996575967625525e-05, 'epoch': 0.04}
  4%|â–         | 198/5198 [42:44<17:53:50, 12.89s/it]  4%|â–         | 199/5198 [42:55<17:20:54, 12.49s/it]                                                     {'loss': 0.9154, 'learning_rate': 1.999641098727329e-05, 'epoch': 0.04}
  4%|â–         | 199/5198 [42:55<17:20:54, 12.49s/it]  4%|â–         | 200/5198 [43:08<17:36:26, 12.68s/it]                                                     {'loss': 0.9416, 'learning_rate': 1.999624212597013e-05, 'epoch': 0.04}
  4%|â–         | 200/5198 [43:08<17:36:26, 12.68s/it]  4%|â–         | 201/5198 [43:20<17:20:10, 12.49s/it]                                                     {'loss': 0.9952, 'learning_rate': 1.9996069383781587e-05, 'epoch': 0.04}
  4%|â–         | 201/5198 [43:20<17:20:10, 12.49s/it]  4%|â–         | 202/5198 [43:32<17:01:22, 12.27s/it]                                                     {'loss': 0.9959, 'learning_rate': 1.9995892760774738e-05, 'epoch': 0.04}
  4%|â–         | 202/5198 [43:32<17:01:22, 12.27s/it]  4%|â–         | 203/5198 [43:44<16:44:59, 12.07s/it]                                                     {'loss': 0.975, 'learning_rate': 1.9995712257018153e-05, 'epoch': 0.04}
  4%|â–         | 203/5198 [43:44<16:44:59, 12.07s/it]  4%|â–         | 204/5198 [43:56<16:42:17, 12.04s/it]                                                     {'loss': 0.9134, 'learning_rate': 1.9995527872581903e-05, 'epoch': 0.04}
  4%|â–         | 204/5198 [43:56<16:42:17, 12.04s/it]  4%|â–         | 205/5198 [44:13<18:42:34, 13.49s/it]                                                     {'loss': 0.3353, 'learning_rate': 1.9995339607537578e-05, 'epoch': 0.04}
  4%|â–         | 205/5198 [44:13<18:42:34, 13.49s/it]  4%|â–         | 206/5198 [44:24<17:44:55, 12.80s/it]                                                     {'loss': 0.9346, 'learning_rate': 1.9995147461958267e-05, 'epoch': 0.04}
  4%|â–         | 206/5198 [44:24<17:44:55, 12.80s/it]  4%|â–         | 207/5198 [44:35<17:17:20, 12.47s/it]                                                     {'loss': 0.942, 'learning_rate': 1.999495143591857e-05, 'epoch': 0.04}
  4%|â–         | 207/5198 [44:36<17:17:20, 12.47s/it]  4%|â–         | 208/5198 [44:50<17:59:52, 12.98s/it]                                                     {'loss': 0.9444, 'learning_rate': 1.999475152949459e-05, 'epoch': 0.04}
  4%|â–         | 208/5198 [44:50<17:59:52, 12.98s/it]  4%|â–         | 209/5198 [45:02<17:56:28, 12.95s/it]                                                     {'loss': 0.9878, 'learning_rate': 1.9994547742763935e-05, 'epoch': 0.04}
  4%|â–         | 209/5198 [45:03<17:56:28, 12.95s/it]  4%|â–         | 210/5198 [45:16<18:08:57, 13.10s/it]                                                     {'loss': 0.9769, 'learning_rate': 1.9994340075805724e-05, 'epoch': 0.04}
  4%|â–         | 210/5198 [45:16<18:08:57, 13.10s/it]  4%|â–         | 211/5198 [45:28<17:43:37, 12.80s/it]                                                     {'loss': 0.9546, 'learning_rate': 1.9994128528700583e-05, 'epoch': 0.04}
  4%|â–         | 211/5198 [45:28<17:43:37, 12.80s/it]  4%|â–         | 212/5198 [45:40<17:28:33, 12.62s/it]                                                     {'loss': 0.8826, 'learning_rate': 1.9993913101530635e-05, 'epoch': 0.04}
  4%|â–         | 212/5198 [45:40<17:28:33, 12.62s/it]  4%|â–         | 213/5198 [45:52<17:07:32, 12.37s/it]                                                     {'loss': 0.9215, 'learning_rate': 1.9993693794379525e-05, 'epoch': 0.04}
  4%|â–         | 213/5198 [45:52<17:07:32, 12.37s/it]  4%|â–         | 214/5198 [46:04<16:59:44, 12.28s/it]                                                     {'loss': 0.9447, 'learning_rate': 1.9993470607332387e-05, 'epoch': 0.04}
  4%|â–         | 214/5198 [46:04<16:59:44, 12.28s/it]  4%|â–         | 215/5198 [46:16<16:49:17, 12.15s/it]                                                     {'loss': 0.9561, 'learning_rate': 1.999324354047588e-05, 'epoch': 0.04}
  4%|â–         | 215/5198 [46:16<16:49:17, 12.15s/it]  4%|â–         | 216/5198 [46:28<16:51:16, 12.18s/it]                                                     {'loss': 0.9691, 'learning_rate': 1.9993012593898146e-05, 'epoch': 0.04}
  4%|â–         | 216/5198 [46:28<16:51:16, 12.18s/it]  4%|â–         | 217/5198 [46:45<18:42:51, 13.53s/it]                                                     {'loss': 0.3087, 'learning_rate': 1.9992777767688857e-05, 'epoch': 0.04}
  4%|â–         | 217/5198 [46:45<18:42:51, 13.53s/it]  4%|â–         | 218/5198 [47:01<19:58:33, 14.44s/it]                                                     {'loss': 0.3359, 'learning_rate': 1.9992539061939175e-05, 'epoch': 0.04}
  4%|â–         | 218/5198 [47:01<19:58:33, 14.44s/it]  4%|â–         | 219/5198 [47:14<19:11:45, 13.88s/it]                                                     {'loss': 1.0012, 'learning_rate': 1.999229647674178e-05, 'epoch': 0.04}
  4%|â–         | 219/5198 [47:14<19:11:45, 13.88s/it]  4%|â–         | 220/5198 [47:30<20:14:23, 14.64s/it]                                                     {'loss': 0.299, 'learning_rate': 1.9992050012190845e-05, 'epoch': 0.04}
  4%|â–         | 220/5198 [47:31<20:14:23, 14.64s/it]  4%|â–         | 221/5198 [47:45<20:04:39, 14.52s/it]                                                     {'loss': 0.9492, 'learning_rate': 1.9991799668382058e-05, 'epoch': 0.04}
  4%|â–         | 221/5198 [47:45<20:04:39, 14.52s/it]  4%|â–         | 222/5198 [47:57<19:14:38, 13.92s/it]                                                     {'loss': 0.9444, 'learning_rate': 1.9991545445412614e-05, 'epoch': 0.04}
  4%|â–         | 222/5198 [47:57<19:14:38, 13.92s/it]  4%|â–         | 223/5198 [48:12<19:40:00, 14.23s/it]                                                     {'loss': 1.02, 'learning_rate': 1.9991287343381208e-05, 'epoch': 0.04}
  4%|â–         | 223/5198 [48:12<19:40:00, 14.23s/it]  4%|â–         | 224/5198 [48:24<18:29:39, 13.39s/it]                                                     {'loss': 0.9768, 'learning_rate': 1.9991025362388044e-05, 'epoch': 0.04}
  4%|â–         | 224/5198 [48:24<18:29:39, 13.39s/it]  4%|â–         | 225/5198 [48:35<17:45:49, 12.86s/it]                                                     {'loss': 0.9362, 'learning_rate': 1.9990759502534835e-05, 'epoch': 0.04}
  4%|â–         | 225/5198 [48:35<17:45:49, 12.86s/it]  4%|â–         | 226/5198 [48:52<19:18:50, 13.98s/it]                                                     {'loss': 0.2838, 'learning_rate': 1.9990489763924796e-05, 'epoch': 0.04}
  4%|â–         | 226/5198 [48:52<19:18:50, 13.98s/it]  4%|â–         | 227/5198 [49:03<18:19:36, 13.27s/it]                                                     {'loss': 0.905, 'learning_rate': 1.9990216146662648e-05, 'epoch': 0.04}
  4%|â–         | 227/5198 [49:03<18:19:36, 13.27s/it]  4%|â–         | 228/5198 [49:15<17:33:30, 12.72s/it]                                                     {'loss': 0.9789, 'learning_rate': 1.9989938650854618e-05, 'epoch': 0.04}
  4%|â–         | 228/5198 [49:15<17:33:30, 12.72s/it]  4%|â–         | 229/5198 [49:27<17:17:42, 12.53s/it]                                                     {'loss': 0.9194, 'learning_rate': 1.998965727660844e-05, 'epoch': 0.04}
  4%|â–         | 229/5198 [49:27<17:17:42, 12.53s/it]  4%|â–         | 230/5198 [49:39<17:02:10, 12.35s/it]                                                     {'loss': 0.9158, 'learning_rate': 1.9989372024033352e-05, 'epoch': 0.04}
  4%|â–         | 230/5198 [49:39<17:02:10, 12.35s/it]  4%|â–         | 231/5198 [49:50<16:36:56, 12.04s/it]                                                     {'loss': 0.9233, 'learning_rate': 1.99890828932401e-05, 'epoch': 0.04}
  4%|â–         | 231/5198 [49:50<16:36:56, 12.04s/it]  4%|â–         | 232/5198 [50:03<16:48:27, 12.18s/it]                                                     {'loss': 0.9235, 'learning_rate': 1.9988789884340938e-05, 'epoch': 0.04}
  4%|â–         | 232/5198 [50:03<16:48:27, 12.18s/it]  4%|â–         | 233/5198 [50:15<16:44:14, 12.14s/it]                                                     {'loss': 0.9462, 'learning_rate': 1.9988492997449615e-05, 'epoch': 0.04}
  4%|â–         | 233/5198 [50:15<16:44:14, 12.14s/it]  5%|â–         | 234/5198 [50:27<16:39:14, 12.08s/it]                                                     {'loss': 0.93, 'learning_rate': 1.9988192232681398e-05, 'epoch': 0.05}
  5%|â–         | 234/5198 [50:27<16:39:14, 12.08s/it]  5%|â–         | 235/5198 [50:38<16:20:00, 11.85s/it]                                                     {'loss': 0.9339, 'learning_rate': 1.9987887590153055e-05, 'epoch': 0.05}
  5%|â–         | 235/5198 [50:38<16:20:00, 11.85s/it]  5%|â–         | 236/5198 [50:51<17:01:39, 12.35s/it]                                                     {'loss': 0.993, 'learning_rate': 1.9987579069982856e-05, 'epoch': 0.05}
  5%|â–         | 236/5198 [50:52<17:01:39, 12.35s/it]  5%|â–         | 237/5198 [51:04<17:03:52, 12.38s/it]                                                     {'loss': 0.974, 'learning_rate': 1.9987266672290577e-05, 'epoch': 0.05}
  5%|â–         | 237/5198 [51:04<17:03:52, 12.38s/it]  5%|â–         | 238/5198 [51:16<16:52:42, 12.25s/it]                                                     {'loss': 0.9762, 'learning_rate': 1.9986950397197503e-05, 'epoch': 0.05}
  5%|â–         | 238/5198 [51:16<16:52:42, 12.25s/it]  5%|â–         | 239/5198 [51:29<17:09:17, 12.45s/it]                                                     {'loss': 0.9472, 'learning_rate': 1.9986630244826425e-05, 'epoch': 0.05}
  5%|â–         | 239/5198 [51:29<17:09:17, 12.45s/it]  5%|â–         | 240/5198 [51:40<16:48:53, 12.21s/it]                                                     {'loss': 0.9881, 'learning_rate': 1.998630621530164e-05, 'epoch': 0.05}
  5%|â–         | 240/5198 [51:41<16:48:53, 12.21s/it]  5%|â–         | 241/5198 [51:53<16:49:05, 12.21s/it]                                                     {'loss': 0.9605, 'learning_rate': 1.998597830874894e-05, 'epoch': 0.05}
  5%|â–         | 241/5198 [51:53<16:49:05, 12.21s/it]  5%|â–         | 242/5198 [52:04<16:35:14, 12.05s/it]                                                     {'loss': 0.8691, 'learning_rate': 1.9985646525295634e-05, 'epoch': 0.05}
  5%|â–         | 242/5198 [52:04<16:35:14, 12.05s/it]  5%|â–         | 243/5198 [52:16<16:32:45, 12.02s/it]                                                     {'loss': 0.9077, 'learning_rate': 1.998531086507053e-05, 'epoch': 0.05}
  5%|â–         | 243/5198 [52:16<16:32:45, 12.02s/it]  5%|â–         | 244/5198 [52:28<16:25:22, 11.93s/it]                                                     {'loss': 0.9321, 'learning_rate': 1.9984971328203945e-05, 'epoch': 0.05}
  5%|â–         | 244/5198 [52:28<16:25:22, 11.93s/it]  5%|â–         | 245/5198 [52:41<16:41:44, 12.13s/it]                                                     {'loss': 0.9463, 'learning_rate': 1.9984627914827698e-05, 'epoch': 0.05}
  5%|â–         | 245/5198 [52:41<16:41:44, 12.13s/it]  5%|â–         | 246/5198 [52:52<16:24:31, 11.93s/it]                                                     {'loss': 1.0008, 'learning_rate': 1.9984280625075115e-05, 'epoch': 0.05}
  5%|â–         | 246/5198 [52:52<16:24:31, 11.93s/it]  5%|â–         | 247/5198 [53:04<16:21:49, 11.90s/it]                                                     {'loss': 0.9338, 'learning_rate': 1.9983929459081022e-05, 'epoch': 0.05}
  5%|â–         | 247/5198 [53:04<16:21:49, 11.90s/it]  5%|â–         | 248/5198 [53:16<16:36:52, 12.08s/it]                                                     {'loss': 0.9416, 'learning_rate': 1.998357441698176e-05, 'epoch': 0.05}
  5%|â–         | 248/5198 [53:17<16:36:52, 12.08s/it]  5%|â–         | 249/5198 [53:30<17:22:06, 12.63s/it]                                                     {'loss': 0.9102, 'learning_rate': 1.998321549891516e-05, 'epoch': 0.05}
  5%|â–         | 249/5198 [53:30<17:22:06, 12.63s/it]  5%|â–         | 250/5198 [53:43<17:30:01, 12.73s/it]                                                     {'loss': 0.9005, 'learning_rate': 1.9982852705020572e-05, 'epoch': 0.05}
  5%|â–         | 250/5198 [53:43<17:30:01, 12.73s/it]  5%|â–         | 251/5198 [54:00<19:06:55, 13.91s/it]                                                     {'loss': 0.3204, 'learning_rate': 1.9982486035438848e-05, 'epoch': 0.05}
  5%|â–         | 251/5198 [54:00<19:06:55, 13.91s/it]  5%|â–         | 252/5198 [54:12<18:17:38, 13.32s/it]                                                     {'loss': 0.9384, 'learning_rate': 1.9982115490312334e-05, 'epoch': 0.05}
  5%|â–         | 252/5198 [54:12<18:17:38, 13.32s/it]  5%|â–         | 253/5198 [54:24<17:46:46, 12.94s/it]                                                     {'loss': 0.9469, 'learning_rate': 1.9981741069784894e-05, 'epoch': 0.05}
  5%|â–         | 253/5198 [54:24<17:46:46, 12.94s/it]  5%|â–         | 254/5198 [54:37<17:40:10, 12.87s/it]                                                     {'loss': 0.9309, 'learning_rate': 1.9981362774001886e-05, 'epoch': 0.05}
  5%|â–         | 254/5198 [54:37<17:40:10, 12.87s/it]  5%|â–         | 255/5198 [54:49<17:23:27, 12.67s/it]                                                     {'loss': 0.9687, 'learning_rate': 1.9980980603110185e-05, 'epoch': 0.05}
  5%|â–         | 255/5198 [54:49<17:23:27, 12.67s/it]  5%|â–         | 256/5198 [55:01<17:01:48, 12.41s/it]                                                     {'loss': 0.9011, 'learning_rate': 1.9980594557258158e-05, 'epoch': 0.05}
  5%|â–         | 256/5198 [55:01<17:01:48, 12.41s/it]  5%|â–         | 257/5198 [55:13<17:05:38, 12.45s/it]                                                     {'loss': 0.8675, 'learning_rate': 1.9980204636595682e-05, 'epoch': 0.05}
  5%|â–         | 257/5198 [55:13<17:05:38, 12.45s/it]  5%|â–         | 258/5198 [55:25<17:00:06, 12.39s/it]                                                     {'loss': 0.9477, 'learning_rate': 1.9979810841274135e-05, 'epoch': 0.05}
  5%|â–         | 258/5198 [55:26<17:00:06, 12.39s/it]  5%|â–         | 259/5198 [55:39<17:35:26, 12.82s/it]                                                     {'loss': 0.9813, 'learning_rate': 1.9979413171446403e-05, 'epoch': 0.05}
  5%|â–         | 259/5198 [55:39<17:35:26, 12.82s/it]  5%|â–Œ         | 260/5198 [55:51<17:15:46, 12.59s/it]                                                     {'loss': 0.9452, 'learning_rate': 1.9979011627266884e-05, 'epoch': 0.05}
  5%|â–Œ         | 260/5198 [55:51<17:15:46, 12.59s/it]  5%|â–Œ         | 261/5198 [56:03<16:49:09, 12.26s/it]                                                     {'loss': 0.9548, 'learning_rate': 1.997860620889146e-05, 'epoch': 0.05}
  5%|â–Œ         | 261/5198 [56:03<16:49:09, 12.26s/it]  5%|â–Œ         | 262/5198 [56:16<17:14:11, 12.57s/it]                                                     {'loss': 0.8105, 'learning_rate': 1.997819691647753e-05, 'epoch': 0.05}
  5%|â–Œ         | 262/5198 [56:16<17:14:11, 12.57s/it]  5%|â–Œ         | 263/5198 [56:28<16:59:18, 12.39s/it]                                                     {'loss': 0.9601, 'learning_rate': 1.9977783750184e-05, 'epoch': 0.05}
  5%|â–Œ         | 263/5198 [56:28<16:59:18, 12.39s/it]  5%|â–Œ         | 264/5198 [56:41<17:07:01, 12.49s/it]                                                     {'loss': 0.9403, 'learning_rate': 1.9977366710171274e-05, 'epoch': 0.05}
  5%|â–Œ         | 264/5198 [56:41<17:07:01, 12.49s/it]  5%|â–Œ         | 265/5198 [56:52<16:44:16, 12.22s/it]                                                     {'loss': 0.9429, 'learning_rate': 1.9976945796601258e-05, 'epoch': 0.05}
  5%|â–Œ         | 265/5198 [56:52<16:44:16, 12.22s/it]  5%|â–Œ         | 266/5198 [57:04<16:36:38, 12.12s/it]                                                     {'loss': 0.9377, 'learning_rate': 1.9976521009637366e-05, 'epoch': 0.05}
  5%|â–Œ         | 266/5198 [57:04<16:36:38, 12.12s/it]  5%|â–Œ         | 267/5198 [57:16<16:36:47, 12.13s/it]                                                     {'loss': 0.9438, 'learning_rate': 1.997609234944452e-05, 'epoch': 0.05}
  5%|â–Œ         | 267/5198 [57:17<16:36:47, 12.13s/it]  5%|â–Œ         | 268/5198 [57:28<16:31:39, 12.07s/it]                                                     {'loss': 1.0126, 'learning_rate': 1.9975659816189137e-05, 'epoch': 0.05}
  5%|â–Œ         | 268/5198 [57:28<16:31:39, 12.07s/it]  5%|â–Œ         | 269/5198 [57:43<17:41:11, 12.92s/it]                                                     {'loss': 0.8633, 'learning_rate': 1.997522341003914e-05, 'epoch': 0.05}
  5%|â–Œ         | 269/5198 [57:43<17:41:11, 12.92s/it]  5%|â–Œ         | 270/5198 [57:55<17:08:13, 12.52s/it]                                                     {'loss': 0.9296, 'learning_rate': 1.9974783131163957e-05, 'epoch': 0.05}
  5%|â–Œ         | 270/5198 [57:55<17:08:13, 12.52s/it]  5%|â–Œ         | 271/5198 [58:07<17:03:54, 12.47s/it]                                                     {'loss': 0.9499, 'learning_rate': 1.9974338979734523e-05, 'epoch': 0.05}
  5%|â–Œ         | 271/5198 [58:07<17:03:54, 12.47s/it]  5%|â–Œ         | 272/5198 [58:19<16:42:54, 12.22s/it]                                                     {'loss': 0.9683, 'learning_rate': 1.997389095592327e-05, 'epoch': 0.05}
  5%|â–Œ         | 272/5198 [58:19<16:42:54, 12.22s/it]  5%|â–Œ         | 273/5198 [58:30<16:21:38, 11.96s/it]                                                     {'loss': 0.9515, 'learning_rate': 1.9973439059904133e-05, 'epoch': 0.05}
  5%|â–Œ         | 273/5198 [58:30<16:21:38, 11.96s/it]  5%|â–Œ         | 274/5198 [58:43<16:41:03, 12.20s/it]                                                     {'loss': 0.9479, 'learning_rate': 1.9972983291852565e-05, 'epoch': 0.05}
  5%|â–Œ         | 274/5198 [58:43<16:41:03, 12.20s/it]  5%|â–Œ         | 275/5198 [58:55<16:36:16, 12.14s/it]                                                     {'loss': 0.9667, 'learning_rate': 1.9972523651945496e-05, 'epoch': 0.05}
  5%|â–Œ         | 275/5198 [58:55<16:36:16, 12.14s/it]  5%|â–Œ         | 276/5198 [59:07<16:22:40, 11.98s/it]                                                     {'loss': 0.9321, 'learning_rate': 1.9972060140361384e-05, 'epoch': 0.05}
  5%|â–Œ         | 276/5198 [59:07<16:22:40, 11.98s/it]  5%|â–Œ         | 277/5198 [59:19<16:22:19, 11.98s/it]                                                     {'loss': 0.9746, 'learning_rate': 1.997159275728018e-05, 'epoch': 0.05}
  5%|â–Œ         | 277/5198 [59:19<16:22:19, 11.98s/it]  5%|â–Œ         | 278/5198 [59:31<16:26:22, 12.03s/it]                                                     {'loss': 0.9605, 'learning_rate': 1.9971121502883332e-05, 'epoch': 0.05}
  5%|â–Œ         | 278/5198 [59:31<16:26:22, 12.03s/it]  5%|â–Œ         | 279/5198 [59:43<16:35:27, 12.14s/it]                                                     {'loss': 0.9615, 'learning_rate': 1.9970646377353802e-05, 'epoch': 0.05}
  5%|â–Œ         | 279/5198 [59:43<16:35:27, 12.14s/it]  5%|â–Œ         | 280/5198 [59:55<16:22:53, 11.99s/it]                                                     {'loss': 0.9627, 'learning_rate': 1.997016738087605e-05, 'epoch': 0.05}
  5%|â–Œ         | 280/5198 [59:55<16:22:53, 11.99s/it]  5%|â–Œ         | 281/5198 [1:00:07<16:20:51, 11.97s/it]                                                       {'loss': 0.9355, 'learning_rate': 1.9969684513636035e-05, 'epoch': 0.05}
  5%|â–Œ         | 281/5198 [1:00:07<16:20:51, 11.97s/it]  5%|â–Œ         | 282/5198 [1:00:20<16:53:37, 12.37s/it]                                                       {'loss': 0.9349, 'learning_rate': 1.9969197775821227e-05, 'epoch': 0.05}
  5%|â–Œ         | 282/5198 [1:00:20<16:53:37, 12.37s/it]  5%|â–Œ         | 283/5198 [1:00:32<16:49:05, 12.32s/it]                                                       {'loss': 0.9224, 'learning_rate': 1.9968707167620593e-05, 'epoch': 0.05}
  5%|â–Œ         | 283/5198 [1:00:32<16:49:05, 12.32s/it]  5%|â–Œ         | 284/5198 [1:00:45<16:55:23, 12.40s/it]                                                       {'loss': 0.9773, 'learning_rate': 1.9968212689224603e-05, 'epoch': 0.05}
  5%|â–Œ         | 284/5198 [1:00:45<16:55:23, 12.40s/it]  5%|â–Œ         | 285/5198 [1:00:57<16:44:26, 12.27s/it]                                                       {'loss': 0.8673, 'learning_rate': 1.996771434082523e-05, 'epoch': 0.05}
  5%|â–Œ         | 285/5198 [1:00:57<16:44:26, 12.27s/it]  6%|â–Œ         | 286/5198 [1:01:09<16:44:24, 12.27s/it]                                                       {'loss': 0.926, 'learning_rate': 1.9967212122615958e-05, 'epoch': 0.06}
  6%|â–Œ         | 286/5198 [1:01:09<16:44:24, 12.27s/it]  6%|â–Œ         | 287/5198 [1:01:21<16:39:35, 12.21s/it]                                                       {'loss': 0.8449, 'learning_rate': 1.9966706034791752e-05, 'epoch': 0.06}
  6%|â–Œ         | 287/5198 [1:01:21<16:39:35, 12.21s/it]  6%|â–Œ         | 288/5198 [1:01:33<16:39:21, 12.21s/it]                                                       {'loss': 0.8912, 'learning_rate': 1.9966196077549106e-05, 'epoch': 0.06}
  6%|â–Œ         | 288/5198 [1:01:33<16:39:21, 12.21s/it]  6%|â–Œ         | 289/5198 [1:01:45<16:39:55, 12.22s/it]                                                       {'loss': 0.9538, 'learning_rate': 1.996568225108599e-05, 'epoch': 0.06}
  6%|â–Œ         | 289/5198 [1:01:46<16:39:55, 12.22s/it]  6%|â–Œ         | 290/5198 [1:01:58<16:39:31, 12.22s/it]                                                       {'loss': 0.9341, 'learning_rate': 1.99651645556019e-05, 'epoch': 0.06}
  6%|â–Œ         | 290/5198 [1:01:58<16:39:31, 12.22s/it]  6%|â–Œ         | 291/5198 [1:02:09<16:23:01, 12.02s/it]                                                       {'loss': 0.9649, 'learning_rate': 1.9964642991297817e-05, 'epoch': 0.06}
  6%|â–Œ         | 291/5198 [1:02:09<16:23:01, 12.02s/it]  6%|â–Œ         | 292/5198 [1:02:21<16:06:22, 11.82s/it]                                                       {'loss': 0.9152, 'learning_rate': 1.996411755837623e-05, 'epoch': 0.06}
  6%|â–Œ         | 292/5198 [1:02:21<16:06:22, 11.82s/it]  6%|â–Œ         | 293/5198 [1:02:34<16:39:19, 12.22s/it]                                                       {'loss': 0.9102, 'learning_rate': 1.9963588257041137e-05, 'epoch': 0.06}
  6%|â–Œ         | 293/5198 [1:02:34<16:39:19, 12.22s/it]  6%|â–Œ         | 294/5198 [1:02:46<16:35:47, 12.18s/it]                                                       {'loss': 0.9716, 'learning_rate': 1.996305508749802e-05, 'epoch': 0.06}
  6%|â–Œ         | 294/5198 [1:02:46<16:35:47, 12.18s/it]  6%|â–Œ         | 295/5198 [1:02:59<17:02:18, 12.51s/it]                                                       {'loss': 0.9117, 'learning_rate': 1.9962518049953887e-05, 'epoch': 0.06}
  6%|â–Œ         | 295/5198 [1:02:59<17:02:18, 12.51s/it]  6%|â–Œ         | 296/5198 [1:03:11<16:38:09, 12.22s/it]                                                       {'loss': 0.9695, 'learning_rate': 1.9961977144617225e-05, 'epoch': 0.06}
  6%|â–Œ         | 296/5198 [1:03:11<16:38:09, 12.22s/it][2024-03-23 16:53:55,982] [WARNING] [stage3.py:1991:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  6%|â–Œ         | 297/5198 [1:03:28<18:34:50, 13.65s/it]                                                       {'loss': 0.3346, 'learning_rate': 1.996143237169803e-05, 'epoch': 0.06}
  6%|â–Œ         | 297/5198 [1:03:28<18:34:50, 13.65s/it]  6%|â–Œ         | 298/5198 [1:03:40<18:05:36, 13.29s/it]                                                       {'loss': 0.9301, 'learning_rate': 1.996088373140781e-05, 'epoch': 0.06}
  6%|â–Œ         | 298/5198 [1:03:40<18:05:36, 13.29s/it]  6%|â–Œ         | 299/5198 [1:03:51<17:15:10, 12.68s/it]                                                       {'loss': 0.9515, 'learning_rate': 1.9960331223959564e-05, 'epoch': 0.06}
  6%|â–Œ         | 299/5198 [1:03:51<17:15:10, 12.68s/it]  6%|â–Œ         | 300/5198 [1:04:03<16:59:11, 12.48s/it]                                                       {'loss': 0.8998, 'learning_rate': 1.995977484956779e-05, 'epoch': 0.06}
  6%|â–Œ         | 300/5198 [1:04:04<16:59:11, 12.48s/it]  6%|â–Œ         | 301/5198 [1:04:17<17:26:45, 12.83s/it]                                                       {'loss': 0.972, 'learning_rate': 1.9959214608448495e-05, 'epoch': 0.06}
  6%|â–Œ         | 301/5198 [1:04:17<17:26:45, 12.83s/it]  6%|â–Œ         | 302/5198 [1:04:30<17:28:10, 12.85s/it]                                                       {'loss': 0.9936, 'learning_rate': 1.9958650500819183e-05, 'epoch': 0.06}
  6%|â–Œ         | 302/5198 [1:04:30<17:28:10, 12.85s/it]  6%|â–Œ         | 303/5198 [1:04:41<16:56:46, 12.46s/it]                                                       {'loss': 0.9135, 'learning_rate': 1.995808252689886e-05, 'epoch': 0.06}
  6%|â–Œ         | 303/5198 [1:04:42<16:56:46, 12.46s/it]  6%|â–Œ         | 304/5198 [1:04:54<17:08:03, 12.60s/it]                                                       {'loss': 0.8864, 'learning_rate': 1.9957510686908034e-05, 'epoch': 0.06}
  6%|â–Œ         | 304/5198 [1:04:55<17:08:03, 12.60s/it]  6%|â–Œ         | 305/5198 [1:05:07<17:04:39, 12.56s/it]                                                       {'loss': 0.9459, 'learning_rate': 1.9956934981068713e-05, 'epoch': 0.06}
  6%|â–Œ         | 305/5198 [1:05:07<17:04:39, 12.56s/it]  6%|â–Œ         | 306/5198 [1:05:18<16:39:22, 12.26s/it]                                                       {'loss': 0.8798, 'learning_rate': 1.9956355409604402e-05, 'epoch': 0.06}
  6%|â–Œ         | 306/5198 [1:05:18<16:39:22, 12.26s/it]  6%|â–Œ         | 307/5198 [1:05:30<16:15:28, 11.97s/it]                                                       {'loss': 0.9756, 'learning_rate': 1.9955771972740118e-05, 'epoch': 0.06}
  6%|â–Œ         | 307/5198 [1:05:30<16:15:28, 11.97s/it]  6%|â–Œ         | 308/5198 [1:05:42<16:27:28, 12.12s/it]                                                       {'loss': 0.89, 'learning_rate': 1.9955184670702363e-05, 'epoch': 0.06}
  6%|â–Œ         | 308/5198 [1:05:42<16:27:28, 12.12s/it]  6%|â–Œ         | 309/5198 [1:05:54<16:11:49, 11.93s/it]                                                       {'loss': 0.9229, 'learning_rate': 1.995459350371915e-05, 'epoch': 0.06}
  6%|â–Œ         | 309/5198 [1:05:54<16:11:49, 11.93s/it]  6%|â–Œ         | 310/5198 [1:06:05<16:00:35, 11.79s/it]                                                       {'loss': 0.9386, 'learning_rate': 1.9953998472019996e-05, 'epoch': 0.06}
  6%|â–Œ         | 310/5198 [1:06:05<16:00:35, 11.79s/it]  6%|â–Œ         | 311/5198 [1:06:18<16:25:08, 12.09s/it]                                                       {'loss': 0.9153, 'learning_rate': 1.995339957583591e-05, 'epoch': 0.06}
  6%|â–Œ         | 311/5198 [1:06:18<16:25:08, 12.09s/it]  6%|â–Œ         | 312/5198 [1:06:31<16:38:37, 12.26s/it]                                                       {'loss': 0.869, 'learning_rate': 1.9952796815399403e-05, 'epoch': 0.06}
  6%|â–Œ         | 312/5198 [1:06:31<16:38:37, 12.26s/it]  6%|â–Œ         | 313/5198 [1:06:42<16:07:12, 11.88s/it]                                                       {'loss': 0.9663, 'learning_rate': 1.9952190190944484e-05, 'epoch': 0.06}
  6%|â–Œ         | 313/5198 [1:06:42<16:07:12, 11.88s/it]  6%|â–Œ         | 314/5198 [1:06:53<16:00:18, 11.80s/it]                                                       {'loss': 0.9965, 'learning_rate': 1.9951579702706668e-05, 'epoch': 0.06}
  6%|â–Œ         | 314/5198 [1:06:53<16:00:18, 11.80s/it]  6%|â–Œ         | 315/5198 [1:07:05<16:03:44, 11.84s/it]                                                       {'loss': 0.8769, 'learning_rate': 1.9950965350922975e-05, 'epoch': 0.06}
  6%|â–Œ         | 315/5198 [1:07:05<16:03:44, 11.84s/it]  6%|â–Œ         | 316/5198 [1:07:18<16:19:03, 12.03s/it]                                                       {'loss': 0.9374, 'learning_rate': 1.9950347135831907e-05, 'epoch': 0.06}
  6%|â–Œ         | 316/5198 [1:07:18<16:19:03, 12.03s/it]  6%|â–Œ         | 317/5198 [1:07:30<16:25:48, 12.12s/it]                                                       {'loss': 0.9568, 'learning_rate': 1.994972505767348e-05, 'epoch': 0.06}
  6%|â–Œ         | 317/5198 [1:07:30<16:25:48, 12.12s/it]  6%|â–Œ         | 318/5198 [1:07:42<16:16:46, 12.01s/it]                                                       {'loss': 0.9421, 'learning_rate': 1.994909911668921e-05, 'epoch': 0.06}
  6%|â–Œ         | 318/5198 [1:07:42<16:16:46, 12.01s/it]  6%|â–Œ         | 319/5198 [1:07:54<16:15:57, 12.00s/it]                                                       {'loss': 0.9295, 'learning_rate': 1.99484693131221e-05, 'epoch': 0.06}
  6%|â–Œ         | 319/5198 [1:07:54<16:15:57, 12.00s/it]  6%|â–Œ         | 320/5198 [1:08:07<16:56:03, 12.50s/it]                                                       {'loss': 0.9115, 'learning_rate': 1.994783564721667e-05, 'epoch': 0.06}
  6%|â–Œ         | 320/5198 [1:08:07<16:56:03, 12.50s/it]  6%|â–Œ         | 321/5198 [1:08:20<16:49:32, 12.42s/it]                                                       {'loss': 0.9022, 'learning_rate': 1.9947198119218924e-05, 'epoch': 0.06}
  6%|â–Œ         | 321/5198 [1:08:20<16:49:32, 12.42s/it]  6%|â–Œ         | 322/5198 [1:08:38<19:15:48, 14.22s/it]                                                       {'loss': 0.2805, 'learning_rate': 1.994655672937638e-05, 'epoch': 0.06}
  6%|â–Œ         | 322/5198 [1:08:38<19:15:48, 14.22s/it]  6%|â–Œ         | 323/5198 [1:08:51<18:35:47, 13.73s/it]                                                       {'loss': 1.0148, 'learning_rate': 1.9945911477938044e-05, 'epoch': 0.06}
  6%|â–Œ         | 323/5198 [1:08:51<18:35:47, 13.73s/it]  6%|â–Œ         | 324/5198 [1:09:02<17:42:13, 13.08s/it]                                                       {'loss': 0.9498, 'learning_rate': 1.994526236515442e-05, 'epoch': 0.06}
  6%|â–Œ         | 324/5198 [1:09:02<17:42:13, 13.08s/it]  6%|â–‹         | 325/5198 [1:09:14<17:03:00, 12.60s/it]                                                       {'loss': 0.9924, 'learning_rate': 1.994460939127753e-05, 'epoch': 0.06}
  6%|â–‹         | 325/5198 [1:09:14<17:03:00, 12.60s/it]  6%|â–‹         | 326/5198 [1:09:26<16:46:46, 12.40s/it]                                                       {'loss': 0.875, 'learning_rate': 1.9943952556560863e-05, 'epoch': 0.06}
  6%|â–‹         | 326/5198 [1:09:26<16:46:46, 12.40s/it]  6%|â–‹         | 327/5198 [1:09:37<16:34:02, 12.24s/it]                                                       {'loss': 0.9845, 'learning_rate': 1.9943291861259433e-05, 'epoch': 0.06}
  6%|â–‹         | 327/5198 [1:09:38<16:34:02, 12.24s/it]  6%|â–‹         | 328/5198 [1:09:52<17:27:01, 12.90s/it]                                                       {'loss': 0.9215, 'learning_rate': 1.9942627305629747e-05, 'epoch': 0.06}
  6%|â–‹         | 328/5198 [1:09:52<17:27:01, 12.90s/it]  6%|â–‹         | 329/5198 [1:10:03<16:48:48, 12.43s/it]                                                       {'loss': 0.9227, 'learning_rate': 1.9941958889929808e-05, 'epoch': 0.06}
  6%|â–‹         | 329/5198 [1:10:03<16:48:48, 12.43s/it]  6%|â–‹         | 330/5198 [1:10:16<17:03:36, 12.62s/it]                                                       {'loss': 0.8874, 'learning_rate': 1.9941286614419113e-05, 'epoch': 0.06}
  6%|â–‹         | 330/5198 [1:10:16<17:03:36, 12.62s/it]  6%|â–‹         | 331/5198 [1:10:29<17:08:34, 12.68s/it]                                                       {'loss': 0.9528, 'learning_rate': 1.994061047935867e-05, 'epoch': 0.06}
  6%|â–‹         | 331/5198 [1:10:29<17:08:34, 12.68s/it]  6%|â–‹         | 332/5198 [1:10:41<17:00:18, 12.58s/it]                                                       {'loss': 0.9335, 'learning_rate': 1.9939930485010968e-05, 'epoch': 0.06}
  6%|â–‹         | 332/5198 [1:10:41<17:00:18, 12.58s/it]  6%|â–‹         | 333/5198 [1:10:53<16:40:46, 12.34s/it]                                                       {'loss': 0.9213, 'learning_rate': 1.9939246631640014e-05, 'epoch': 0.06}
  6%|â–‹         | 333/5198 [1:10:53<16:40:46, 12.34s/it]  6%|â–‹         | 334/5198 [1:11:05<16:32:33, 12.24s/it]                                                       {'loss': 0.8656, 'learning_rate': 1.99385589195113e-05, 'epoch': 0.06}
  6%|â–‹         | 334/5198 [1:11:05<16:32:33, 12.24s/it]  6%|â–‹         | 335/5198 [1:11:17<16:15:16, 12.03s/it]                                                       {'loss': 0.9419, 'learning_rate': 1.9937867348891815e-05, 'epoch': 0.06}
  6%|â–‹         | 335/5198 [1:11:17<16:15:16, 12.03s/it]  6%|â–‹         | 336/5198 [1:11:30<16:37:36, 12.31s/it]                                                       {'loss': 0.9438, 'learning_rate': 1.9937171920050057e-05, 'epoch': 0.06}
  6%|â–‹         | 336/5198 [1:11:30<16:37:36, 12.31s/it]  6%|â–‹         | 337/5198 [1:11:41<16:16:01, 12.05s/it]                                                       {'loss': 0.918, 'learning_rate': 1.9936472633256012e-05, 'epoch': 0.06}
  6%|â–‹         | 337/5198 [1:11:41<16:16:01, 12.05s/it]  7%|â–‹         | 338/5198 [1:11:53<16:14:27, 12.03s/it]                                                       {'loss': 0.944, 'learning_rate': 1.9935769488781167e-05, 'epoch': 0.07}
  7%|â–‹         | 338/5198 [1:11:53<16:14:27, 12.03s/it]  7%|â–‹         | 339/5198 [1:12:04<15:57:17, 11.82s/it]                                                       {'loss': 0.9629, 'learning_rate': 1.993506248689851e-05, 'epoch': 0.07}
  7%|â–‹         | 339/5198 [1:12:05<15:57:17, 11.82s/it]  7%|â–‹         | 340/5198 [1:12:17<16:25:29, 12.17s/it]                                                       {'loss': 0.9233, 'learning_rate': 1.993435162788252e-05, 'epoch': 0.07}
  7%|â–‹         | 340/5198 [1:12:18<16:25:29, 12.17s/it]  7%|â–‹         | 341/5198 [1:12:29<16:12:16, 12.01s/it]                                                       {'loss': 0.9569, 'learning_rate': 1.993363691200918e-05, 'epoch': 0.07}
  7%|â–‹         | 341/5198 [1:12:29<16:12:16, 12.01s/it]  7%|â–‹         | 342/5198 [1:12:43<16:57:57, 12.58s/it]                                                       {'loss': 0.8741, 'learning_rate': 1.9932918339555965e-05, 'epoch': 0.07}
  7%|â–‹         | 342/5198 [1:12:43<16:57:57, 12.58s/it]  7%|â–‹         | 343/5198 [1:12:54<16:28:47, 12.22s/it]                                                       {'loss': 0.9788, 'learning_rate': 1.9932195910801848e-05, 'epoch': 0.07}
  7%|â–‹         | 343/5198 [1:12:54<16:28:47, 12.22s/it]  7%|â–‹         | 344/5198 [1:13:06<16:19:04, 12.10s/it]                                                       {'loss': 0.9107, 'learning_rate': 1.9931469626027305e-05, 'epoch': 0.07}
  7%|â–‹         | 344/5198 [1:13:06<16:19:04, 12.10s/it]  7%|â–‹         | 345/5198 [1:13:18<16:10:41, 12.00s/it]                                                       {'loss': 0.9166, 'learning_rate': 1.9930739485514304e-05, 'epoch': 0.07}
  7%|â–‹         | 345/5198 [1:13:18<16:10:41, 12.00s/it]  7%|â–‹         | 346/5198 [1:13:30<16:12:32, 12.03s/it]                                                       {'loss': 0.931, 'learning_rate': 1.9930005489546308e-05, 'epoch': 0.07}
  7%|â–‹         | 346/5198 [1:13:30<16:12:32, 12.03s/it]  7%|â–‹         | 347/5198 [1:13:42<16:09:26, 11.99s/it]                                                       {'loss': 0.9633, 'learning_rate': 1.9929267638408277e-05, 'epoch': 0.07}
  7%|â–‹         | 347/5198 [1:13:42<16:09:26, 11.99s/it]  7%|â–‹         | 348/5198 [1:13:53<15:57:33, 11.85s/it]                                                       {'loss': 0.9272, 'learning_rate': 1.9928525932386678e-05, 'epoch': 0.07}
  7%|â–‹         | 348/5198 [1:13:54<15:57:33, 11.85s/it]  7%|â–‹         | 349/5198 [1:14:06<16:22:27, 12.16s/it]                                                       {'loss': 0.9508, 'learning_rate': 1.9927780371769463e-05, 'epoch': 0.07}
  7%|â–‹         | 349/5198 [1:14:06<16:22:27, 12.16s/it]  7%|â–‹         | 350/5198 [1:14:19<16:41:07, 12.39s/it]                                                       {'loss': 0.88, 'learning_rate': 1.9927030956846083e-05, 'epoch': 0.07}
  7%|â–‹         | 350/5198 [1:14:19<16:41:07, 12.39s/it]  7%|â–‹         | 351/5198 [1:14:31<16:22:47, 12.17s/it]                                                       {'loss': 0.9213, 'learning_rate': 1.992627768790749e-05, 'epoch': 0.07}
  7%|â–‹         | 351/5198 [1:14:31<16:22:47, 12.17s/it]  7%|â–‹         | 352/5198 [1:14:48<18:15:48, 13.57s/it]                                                       {'loss': 0.3238, 'learning_rate': 1.9925520565246125e-05, 'epoch': 0.07}
  7%|â–‹         | 352/5198 [1:14:48<18:15:48, 13.57s/it]  7%|â–‹         | 353/5198 [1:15:00<17:47:29, 13.22s/it]                                                       {'loss': 0.9049, 'learning_rate': 1.9924759589155932e-05, 'epoch': 0.07}
  7%|â–‹         | 353/5198 [1:15:00<17:47:29, 13.22s/it]  7%|â–‹         | 354/5198 [1:15:12<17:23:01, 12.92s/it]                                                       {'loss': 0.9769, 'learning_rate': 1.9923994759932344e-05, 'epoch': 0.07}
  7%|â–‹         | 354/5198 [1:15:13<17:23:01, 12.92s/it]  7%|â–‹         | 355/5198 [1:15:24<16:50:40, 12.52s/it]                                                       {'loss': 0.906, 'learning_rate': 1.9923226077872296e-05, 'epoch': 0.07}
  7%|â–‹         | 355/5198 [1:15:24<16:50:40, 12.52s/it]  7%|â–‹         | 356/5198 [1:15:37<17:08:25, 12.74s/it]                                                       {'loss': 0.9141, 'learning_rate': 1.9922453543274223e-05, 'epoch': 0.07}
  7%|â–‹         | 356/5198 [1:15:37<17:08:25, 12.74s/it]  7%|â–‹         | 357/5198 [1:15:50<17:01:39, 12.66s/it]                                                       {'loss': 0.9155, 'learning_rate': 1.9921677156438044e-05, 'epoch': 0.07}
  7%|â–‹         | 357/5198 [1:15:50<17:01:39, 12.66s/it]  7%|â–‹         | 358/5198 [1:16:03<17:14:02, 12.82s/it]                                                       {'loss': 0.9836, 'learning_rate': 1.9920896917665178e-05, 'epoch': 0.07}
  7%|â–‹         | 358/5198 [1:16:03<17:14:02, 12.82s/it]  7%|â–‹         | 359/5198 [1:16:15<16:54:20, 12.58s/it]                                                       {'loss': 0.9896, 'learning_rate': 1.992011282725854e-05, 'epoch': 0.07}
  7%|â–‹         | 359/5198 [1:16:15<16:54:20, 12.58s/it]  7%|â–‹         | 360/5198 [1:16:27<16:34:21, 12.33s/it]                                                       {'loss': 0.9294, 'learning_rate': 1.9919324885522548e-05, 'epoch': 0.07}
  7%|â–‹         | 360/5198 [1:16:27<16:34:21, 12.33s/it]  7%|â–‹         | 361/5198 [1:16:39<16:37:55, 12.38s/it]                                                       {'loss': 0.9307, 'learning_rate': 1.99185330927631e-05, 'epoch': 0.07}
  7%|â–‹         | 361/5198 [1:16:39<16:37:55, 12.38s/it]  7%|â–‹         | 362/5198 [1:16:51<16:31:26, 12.30s/it]                                                       {'loss': 0.8997, 'learning_rate': 1.99177374492876e-05, 'epoch': 0.07}
  7%|â–‹         | 362/5198 [1:16:51<16:31:26, 12.30s/it]  7%|â–‹         | 363/5198 [1:17:03<16:10:25, 12.04s/it]                                                       {'loss': 0.9275, 'learning_rate': 1.991693795540494e-05, 'epoch': 0.07}
  7%|â–‹         | 363/5198 [1:17:03<16:10:25, 12.04s/it]  7%|â–‹         | 364/5198 [1:17:17<17:06:11, 12.74s/it]                                                       {'loss': 0.9073, 'learning_rate': 1.9916134611425522e-05, 'epoch': 0.07}
  7%|â–‹         | 364/5198 [1:17:17<17:06:11, 12.74s/it]  7%|â–‹         | 365/5198 [1:17:31<17:28:48, 13.02s/it]                                                       {'loss': 0.908, 'learning_rate': 1.9915327417661226e-05, 'epoch': 0.07}
  7%|â–‹         | 365/5198 [1:17:31<17:28:48, 13.02s/it]  7%|â–‹         | 366/5198 [1:17:43<16:58:49, 12.65s/it]                                                       {'loss': 0.9393, 'learning_rate': 1.991451637442543e-05, 'epoch': 0.07}
  7%|â–‹         | 366/5198 [1:17:43<16:58:49, 12.65s/it]  7%|â–‹         | 367/5198 [1:17:58<17:58:18, 13.39s/it]                                                       {'loss': 0.9582, 'learning_rate': 1.9913701482033008e-05, 'epoch': 0.07}
  7%|â–‹         | 367/5198 [1:17:58<17:58:18, 13.39s/it]  7%|â–‹         | 368/5198 [1:18:10<17:40:36, 13.18s/it]                                                       {'loss': 0.9098, 'learning_rate': 1.9912882740800336e-05, 'epoch': 0.07}
  7%|â–‹         | 368/5198 [1:18:10<17:40:36, 13.18s/it]  7%|â–‹         | 369/5198 [1:18:23<17:30:51, 13.06s/it]                                                       {'loss': 0.9269, 'learning_rate': 1.9912060151045273e-05, 'epoch': 0.07}
  7%|â–‹         | 369/5198 [1:18:23<17:30:51, 13.06s/it]  7%|â–‹         | 370/5198 [1:18:37<17:51:28, 13.32s/it]                                                       {'loss': 0.9646, 'learning_rate': 1.9911233713087172e-05, 'epoch': 0.07}
  7%|â–‹         | 370/5198 [1:18:37<17:51:28, 13.32s/it]  7%|â–‹         | 371/5198 [1:18:50<17:43:28, 13.22s/it]                                                       {'loss': 0.9365, 'learning_rate': 1.9910403427246895e-05, 'epoch': 0.07}
  7%|â–‹         | 371/5198 [1:18:50<17:43:28, 13.22s/it]  7%|â–‹         | 372/5198 [1:19:02<17:08:23, 12.79s/it]                                                       {'loss': 0.9339, 'learning_rate': 1.990956929384678e-05, 'epoch': 0.07}
  7%|â–‹         | 372/5198 [1:19:02<17:08:23, 12.79s/it]  7%|â–‹         | 373/5198 [1:19:14<16:48:50, 12.55s/it]                                                       {'loss': 0.9083, 'learning_rate': 1.990873131321067e-05, 'epoch': 0.07}
  7%|â–‹         | 373/5198 [1:19:14<16:48:50, 12.55s/it]  7%|â–‹         | 374/5198 [1:19:27<16:54:17, 12.62s/it]                                                       {'loss': 0.9452, 'learning_rate': 1.9907889485663897e-05, 'epoch': 0.07}
  7%|â–‹         | 374/5198 [1:19:27<16:54:17, 12.62s/it]  7%|â–‹         | 375/5198 [1:19:39<16:57:20, 12.66s/it]                                                       {'loss': 0.9064, 'learning_rate': 1.9907043811533283e-05, 'epoch': 0.07}
  7%|â–‹         | 375/5198 [1:19:39<16:57:20, 12.66s/it]  7%|â–‹         | 376/5198 [1:19:51<16:38:12, 12.42s/it]                                                       {'loss': 0.8983, 'learning_rate': 1.9906194291147155e-05, 'epoch': 0.07}
  7%|â–‹         | 376/5198 [1:19:51<16:38:12, 12.42s/it]  7%|â–‹         | 377/5198 [1:20:03<16:14:35, 12.13s/it]                                                       {'loss': 0.973, 'learning_rate': 1.9905340924835322e-05, 'epoch': 0.07}
  7%|â–‹         | 377/5198 [1:20:03<16:14:35, 12.13s/it]  7%|â–‹         | 378/5198 [1:20:15<16:13:28, 12.12s/it]                                                       {'loss': 0.8877, 'learning_rate': 1.9904483712929094e-05, 'epoch': 0.07}
  7%|â–‹         | 378/5198 [1:20:15<16:13:28, 12.12s/it]  7%|â–‹         | 379/5198 [1:20:27<16:15:32, 12.15s/it]                                                       {'loss': 0.9208, 'learning_rate': 1.9903622655761267e-05, 'epoch': 0.07}
  7%|â–‹         | 379/5198 [1:20:27<16:15:32, 12.15s/it]  7%|â–‹         | 380/5198 [1:20:39<16:01:52, 11.98s/it]                                                       {'loss': 0.9096, 'learning_rate': 1.990275775366613e-05, 'epoch': 0.07}
  7%|â–‹         | 380/5198 [1:20:39<16:01:52, 11.98s/it]  7%|â–‹         | 381/5198 [1:20:50<15:49:14, 11.82s/it]                                                       {'loss': 0.9334, 'learning_rate': 1.9901889006979473e-05, 'epoch': 0.07}
  7%|â–‹         | 381/5198 [1:20:50<15:49:14, 11.82s/it]  7%|â–‹         | 382/5198 [1:21:01<15:40:59, 11.72s/it]                                                       {'loss': 0.9104, 'learning_rate': 1.990101641603857e-05, 'epoch': 0.07}
  7%|â–‹         | 382/5198 [1:21:02<15:40:59, 11.72s/it]  7%|â–‹         | 383/5198 [1:21:15<16:18:02, 12.19s/it]                                                       {'loss': 0.913, 'learning_rate': 1.9900139981182193e-05, 'epoch': 0.07}
  7%|â–‹         | 383/5198 [1:21:15<16:18:02, 12.19s/it]  7%|â–‹         | 384/5198 [1:21:26<16:05:20, 12.03s/it]                                                       {'loss': 0.9012, 'learning_rate': 1.9899259702750604e-05, 'epoch': 0.07}
  7%|â–‹         | 384/5198 [1:21:27<16:05:20, 12.03s/it]  7%|â–‹         | 385/5198 [1:21:39<16:07:49, 12.07s/it]                                                       {'loss': 0.914, 'learning_rate': 1.9898375581085555e-05, 'epoch': 0.07}
  7%|â–‹         | 385/5198 [1:21:39<16:07:49, 12.07s/it]  7%|â–‹         | 386/5198 [1:21:51<16:14:38, 12.15s/it]                                                       {'loss': 0.9366, 'learning_rate': 1.9897487616530296e-05, 'epoch': 0.07}
  7%|â–‹         | 386/5198 [1:21:51<16:14:38, 12.15s/it]  7%|â–‹         | 387/5198 [1:22:03<16:07:42, 12.07s/it]                                                       {'loss': 0.9397, 'learning_rate': 1.9896595809429565e-05, 'epoch': 0.07}
  7%|â–‹         | 387/5198 [1:22:03<16:07:42, 12.07s/it]  7%|â–‹         | 388/5198 [1:22:14<15:49:23, 11.84s/it]                                                       {'loss': 0.8955, 'learning_rate': 1.9895700160129593e-05, 'epoch': 0.07}
  7%|â–‹         | 388/5198 [1:22:14<15:49:23, 11.84s/it]  7%|â–‹         | 389/5198 [1:22:26<15:38:27, 11.71s/it]                                                       {'loss': 0.9404, 'learning_rate': 1.9894800668978095e-05, 'epoch': 0.07}
  7%|â–‹         | 389/5198 [1:22:26<15:38:27, 11.71s/it]  8%|â–Š         | 390/5198 [1:22:38<15:44:46, 11.79s/it]                                                       {'loss': 0.9409, 'learning_rate': 1.9893897336324292e-05, 'epoch': 0.08}
  8%|â–Š         | 390/5198 [1:22:38<15:44:46, 11.79s/it]  8%|â–Š         | 391/5198 [1:22:50<15:58:13, 11.96s/it]                                                       {'loss': 0.9459, 'learning_rate': 1.9892990162518884e-05, 'epoch': 0.08}
  8%|â–Š         | 391/5198 [1:22:50<15:58:13, 11.96s/it]  8%|â–Š         | 392/5198 [1:23:05<17:05:20, 12.80s/it]                                                       {'loss': 0.9678, 'learning_rate': 1.9892079147914072e-05, 'epoch': 0.08}
  8%|â–Š         | 392/5198 [1:23:05<17:05:20, 12.80s/it]  8%|â–Š         | 393/5198 [1:23:20<17:55:28, 13.43s/it]                                                       {'loss': 0.9204, 'learning_rate': 1.9891164292863537e-05, 'epoch': 0.08}
  8%|â–Š         | 393/5198 [1:23:20<17:55:28, 13.43s/it]  8%|â–Š         | 394/5198 [1:23:32<17:26:40, 13.07s/it]                                                       {'loss': 0.9586, 'learning_rate': 1.9890245597722465e-05, 'epoch': 0.08}
  8%|â–Š         | 394/5198 [1:23:32<17:26:40, 13.07s/it]  8%|â–Š         | 395/5198 [1:23:45<17:32:39, 13.15s/it]                                                       {'loss': 0.9297, 'learning_rate': 1.9889323062847516e-05, 'epoch': 0.08}
  8%|â–Š         | 395/5198 [1:23:45<17:32:39, 13.15s/it]  8%|â–Š         | 396/5198 [1:23:57<16:56:52, 12.71s/it]                                                       {'loss': 0.9629, 'learning_rate': 1.988839668859686e-05, 'epoch': 0.08}
  8%|â–Š         | 396/5198 [1:23:57<16:56:52, 12.71s/it]  8%|â–Š         | 397/5198 [1:24:09<16:39:53, 12.50s/it]                                                       {'loss': 0.8943, 'learning_rate': 1.988746647533014e-05, 'epoch': 0.08}
  8%|â–Š         | 397/5198 [1:24:09<16:39:53, 12.50s/it]  8%|â–Š         | 398/5198 [1:24:21<16:26:45, 12.33s/it]                                                       {'loss': 0.9646, 'learning_rate': 1.9886532423408495e-05, 'epoch': 0.08}
  8%|â–Š         | 398/5198 [1:24:21<16:26:45, 12.33s/it]  8%|â–Š         | 399/5198 [1:24:33<16:25:44, 12.32s/it]                                                       {'loss': 0.9431, 'learning_rate': 1.9885594533194564e-05, 'epoch': 0.08}
  8%|â–Š         | 399/5198 [1:24:33<16:25:44, 12.32s/it]  8%|â–Š         | 400/5198 [1:24:45<16:17:07, 12.22s/it]                                                       {'loss': 0.8803, 'learning_rate': 1.9884652805052465e-05, 'epoch': 0.08}
  8%|â–Š         | 400/5198 [1:24:45<16:17:07, 12.22s/it]  8%|â–Š         | 401/5198 [1:24:56<15:57:29, 11.98s/it]                                                       {'loss': 0.9005, 'learning_rate': 1.9883707239347804e-05, 'epoch': 0.08}
  8%|â–Š         | 401/5198 [1:24:56<15:57:29, 11.98s/it]  8%|â–Š         | 402/5198 [1:25:09<16:17:09, 12.22s/it]                                                       {'loss': 0.9208, 'learning_rate': 1.988275783644769e-05, 'epoch': 0.08}
  8%|â–Š         | 402/5198 [1:25:09<16:17:09, 12.22s/it]  8%|â–Š         | 403/5198 [1:25:21<16:10:20, 12.14s/it]                                                       {'loss': 0.9514, 'learning_rate': 1.988180459672071e-05, 'epoch': 0.08}
  8%|â–Š         | 403/5198 [1:25:21<16:10:20, 12.14s/it]  8%|â–Š         | 404/5198 [1:25:33<15:57:37, 11.99s/it]                                                       {'loss': 0.9384, 'learning_rate': 1.988084752053695e-05, 'epoch': 0.08}
  8%|â–Š         | 404/5198 [1:25:33<15:57:37, 11.99s/it]  8%|â–Š         | 405/5198 [1:25:45<16:09:02, 12.13s/it]                                                       {'loss': 0.9535, 'learning_rate': 1.9879886608267967e-05, 'epoch': 0.08}
  8%|â–Š         | 405/5198 [1:25:45<16:09:02, 12.13s/it]  8%|â–Š         | 406/5198 [1:25:57<16:02:28, 12.05s/it]                                                       {'loss': 0.9627, 'learning_rate': 1.9878921860286832e-05, 'epoch': 0.08}
  8%|â–Š         | 406/5198 [1:25:57<16:02:28, 12.05s/it]  8%|â–Š         | 407/5198 [1:26:09<15:56:14, 11.98s/it]                                                       {'loss': 0.9068, 'learning_rate': 1.9877953276968088e-05, 'epoch': 0.08}
  8%|â–Š         | 407/5198 [1:26:09<15:56:14, 11.98s/it]  8%|â–Š         | 408/5198 [1:26:21<16:02:49, 12.06s/it]                                                       {'loss': 0.8982, 'learning_rate': 1.9876980858687777e-05, 'epoch': 0.08}
  8%|â–Š         | 408/5198 [1:26:21<16:02:49, 12.06s/it]  8%|â–Š         | 409/5198 [1:26:38<17:46:23, 13.36s/it]                                                       {'loss': 0.2942, 'learning_rate': 1.9876004605823417e-05, 'epoch': 0.08}
  8%|â–Š         | 409/5198 [1:26:38<17:46:23, 13.36s/it]  8%|â–Š         | 410/5198 [1:26:54<19:05:41, 14.36s/it]                                                       {'loss': 0.3068, 'learning_rate': 1.987502451875403e-05, 'epoch': 0.08}
  8%|â–Š         | 410/5198 [1:26:54<19:05:41, 14.36s/it]  8%|â–Š         | 411/5198 [1:27:06<18:04:09, 13.59s/it]                                                       {'loss': 0.9017, 'learning_rate': 1.987404059786012e-05, 'epoch': 0.08}
  8%|â–Š         | 411/5198 [1:27:06<18:04:09, 13.59s/it]  8%|â–Š         | 412/5198 [1:27:18<17:27:45, 13.14s/it]                                                       {'loss': 0.9283, 'learning_rate': 1.9873052843523676e-05, 'epoch': 0.08}
  8%|â–Š         | 412/5198 [1:27:18<17:27:45, 13.14s/it]  8%|â–Š         | 413/5198 [1:27:30<16:50:47, 12.67s/it]                                                       {'loss': 0.9215, 'learning_rate': 1.987206125612818e-05, 'epoch': 0.08}
  8%|â–Š         | 413/5198 [1:27:30<16:50:47, 12.67s/it]  8%|â–Š         | 414/5198 [1:27:42<16:31:29, 12.44s/it]                                                       {'loss': 0.9304, 'learning_rate': 1.98710658360586e-05, 'epoch': 0.08}
  8%|â–Š         | 414/5198 [1:27:42<16:31:29, 12.44s/it]  8%|â–Š         | 415/5198 [1:27:53<16:17:28, 12.26s/it]                                                       {'loss': 0.9015, 'learning_rate': 1.987006658370139e-05, 'epoch': 0.08}
  8%|â–Š         | 415/5198 [1:27:54<16:17:28, 12.26s/it]  8%|â–Š         | 416/5198 [1:28:05<16:11:56, 12.20s/it]                                                       {'loss': 0.9526, 'learning_rate': 1.9869063499444495e-05, 'epoch': 0.08}
  8%|â–Š         | 416/5198 [1:28:06<16:11:56, 12.20s/it]  8%|â–Š         | 417/5198 [1:28:20<17:09:30, 12.92s/it]                                                       {'loss': 0.9591, 'learning_rate': 1.9868056583677346e-05, 'epoch': 0.08}
  8%|â–Š         | 417/5198 [1:28:20<17:09:30, 12.92s/it]  8%|â–Š         | 418/5198 [1:28:33<17:03:34, 12.85s/it]                                                       {'loss': 0.9033, 'learning_rate': 1.9867045836790867e-05, 'epoch': 0.08}
  8%|â–Š         | 418/5198 [1:28:33<17:03:34, 12.85s/it]  8%|â–Š         | 419/5198 [1:28:46<17:03:42, 12.85s/it]                                                       {'loss': 0.9467, 'learning_rate': 1.9866031259177463e-05, 'epoch': 0.08}
  8%|â–Š         | 419/5198 [1:28:46<17:03:42, 12.85s/it]  8%|â–Š         | 420/5198 [1:28:57<16:36:55, 12.52s/it]                                                       {'loss': 0.9435, 'learning_rate': 1.9865012851231022e-05, 'epoch': 0.08}
  8%|â–Š         | 420/5198 [1:28:57<16:36:55, 12.52s/it]  8%|â–Š         | 421/5198 [1:29:12<17:33:57, 13.24s/it]                                                       {'loss': 0.9628, 'learning_rate': 1.9863990613346936e-05, 'epoch': 0.08}
  8%|â–Š         | 421/5198 [1:29:12<17:33:57, 13.24s/it]  8%|â–Š         | 422/5198 [1:29:25<17:19:36, 13.06s/it]                                                       {'loss': 0.9278, 'learning_rate': 1.986296454592206e-05, 'epoch': 0.08}
  8%|â–Š         | 422/5198 [1:29:25<17:19:36, 13.06s/it]  8%|â–Š         | 423/5198 [1:29:37<16:54:54, 12.75s/it]                                                       {'loss': 0.9603, 'learning_rate': 1.9861934649354763e-05, 'epoch': 0.08}
  8%|â–Š         | 423/5198 [1:29:37<16:54:54, 12.75s/it]  8%|â–Š         | 424/5198 [1:29:51<17:30:10, 13.20s/it]                                                       {'loss': 0.9171, 'learning_rate': 1.9860900924044873e-05, 'epoch': 0.08}
  8%|â–Š         | 424/5198 [1:29:51<17:30:10, 13.20s/it]  8%|â–Š         | 425/5198 [1:30:03<16:52:20, 12.73s/it]                                                       {'loss': 0.8382, 'learning_rate': 1.9859863370393726e-05, 'epoch': 0.08}
  8%|â–Š         | 425/5198 [1:30:03<16:52:20, 12.73s/it]  8%|â–Š         | 426/5198 [1:30:14<16:12:46, 12.23s/it]                                                       {'loss': 0.9469, 'learning_rate': 1.9858821988804132e-05, 'epoch': 0.08}
  8%|â–Š         | 426/5198 [1:30:14<16:12:46, 12.23s/it]  8%|â–Š         | 427/5198 [1:30:27<16:25:07, 12.39s/it]                                                       {'loss': 0.9001, 'learning_rate': 1.9857776779680393e-05, 'epoch': 0.08}
  8%|â–Š         | 427/5198 [1:30:27<16:25:07, 12.39s/it]  8%|â–Š         | 428/5198 [1:30:40<16:53:51, 12.75s/it]                                                       {'loss': 0.9333, 'learning_rate': 1.98567277434283e-05, 'epoch': 0.08}
  8%|â–Š         | 428/5198 [1:30:40<16:53:51, 12.75s/it]  8%|â–Š         | 429/5198 [1:30:54<17:11:22, 12.98s/it]                                                       {'loss': 0.9189, 'learning_rate': 1.9855674880455115e-05, 'epoch': 0.08}
  8%|â–Š         | 429/5198 [1:30:54<17:11:22, 12.98s/it]  8%|â–Š         | 430/5198 [1:31:06<16:43:32, 12.63s/it]                                                       {'loss': 0.9776, 'learning_rate': 1.98546181911696e-05, 'epoch': 0.08}
  8%|â–Š         | 430/5198 [1:31:06<16:43:32, 12.63s/it]  8%|â–Š         | 431/5198 [1:31:18<16:42:53, 12.62s/it]                                                       {'loss': 0.9548, 'learning_rate': 1.9853557675982e-05, 'epoch': 0.08}
  8%|â–Š         | 431/5198 [1:31:18<16:42:53, 12.62s/it]  8%|â–Š         | 432/5198 [1:31:30<16:28:36, 12.45s/it]                                                       {'loss': 0.8772, 'learning_rate': 1.985249333530404e-05, 'epoch': 0.08}
  8%|â–Š         | 432/5198 [1:31:30<16:28:36, 12.45s/it]  8%|â–Š         | 433/5198 [1:31:45<17:28:03, 13.20s/it]                                                       {'loss': 0.8741, 'learning_rate': 1.9851425169548938e-05, 'epoch': 0.08}
  8%|â–Š         | 433/5198 [1:31:45<17:28:03, 13.20s/it]  8%|â–Š         | 434/5198 [1:32:02<18:56:56, 14.32s/it]                                                       {'loss': 0.3152, 'learning_rate': 1.9850353179131392e-05, 'epoch': 0.08}
  8%|â–Š         | 434/5198 [1:32:02<18:56:56, 14.32s/it]  8%|â–Š         | 435/5198 [1:32:18<19:33:09, 14.78s/it]                                                       {'loss': 0.3262, 'learning_rate': 1.9849277364467585e-05, 'epoch': 0.08}
  8%|â–Š         | 435/5198 [1:32:18<19:33:09, 14.78s/it]  8%|â–Š         | 436/5198 [1:32:29<18:12:03, 13.76s/it]                                                       {'loss': 0.9004, 'learning_rate': 1.984819772597518e-05, 'epoch': 0.08}
  8%|â–Š         | 436/5198 [1:32:29<18:12:03, 13.76s/it]  8%|â–Š         | 437/5198 [1:32:42<17:34:53, 13.29s/it]                                                       {'loss': 0.9249, 'learning_rate': 1.9847114264073336e-05, 'epoch': 0.08}
  8%|â–Š         | 437/5198 [1:32:42<17:34:53, 13.29s/it]  8%|â–Š         | 438/5198 [1:32:55<17:36:09, 13.31s/it]                                                       {'loss': 0.9363, 'learning_rate': 1.984602697918269e-05, 'epoch': 0.08}
  8%|â–Š         | 438/5198 [1:32:55<17:36:09, 13.31s/it]  8%|â–Š         | 439/5198 [1:33:06<16:49:48, 12.73s/it]                                                       {'loss': 0.9139, 'learning_rate': 1.9844935871725363e-05, 'epoch': 0.08}
  8%|â–Š         | 439/5198 [1:33:06<16:49:48, 12.73s/it]  8%|â–Š         | 440/5198 [1:33:18<16:34:48, 12.54s/it]                                                       {'loss': 0.8913, 'learning_rate': 1.9843840942124956e-05, 'epoch': 0.08}
  8%|â–Š         | 440/5198 [1:33:18<16:34:48, 12.54s/it]  8%|â–Š         | 441/5198 [1:33:30<16:20:37, 12.37s/it]                                                       {'loss': 0.9765, 'learning_rate': 1.9842742190806566e-05, 'epoch': 0.08}
  8%|â–Š         | 441/5198 [1:33:30<16:20:37, 12.37s/it]  9%|â–Š         | 442/5198 [1:33:42<16:08:53, 12.22s/it]                                                       {'loss': 0.9085, 'learning_rate': 1.984163961819676e-05, 'epoch': 0.09}
  9%|â–Š         | 442/5198 [1:33:42<16:08:53, 12.22s/it]  9%|â–Š         | 443/5198 [1:33:55<16:24:54, 12.43s/it]                                                       {'loss': 0.9262, 'learning_rate': 1.9840533224723595e-05, 'epoch': 0.09}
  9%|â–Š         | 443/5198 [1:33:55<16:24:54, 12.43s/it]  9%|â–Š         | 444/5198 [1:34:07<16:19:37, 12.36s/it]                                                       {'loss': 0.9238, 'learning_rate': 1.9839423010816616e-05, 'epoch': 0.09}
  9%|â–Š         | 444/5198 [1:34:07<16:19:37, 12.36s/it]  9%|â–Š         | 445/5198 [1:34:20<16:24:21, 12.43s/it]                                                       {'loss': 0.9325, 'learning_rate': 1.983830897690684e-05, 'epoch': 0.09}
  9%|â–Š         | 445/5198 [1:34:20<16:24:21, 12.43s/it]  9%|â–Š         | 446/5198 [1:34:33<16:41:55, 12.65s/it]                                                       {'loss': 0.9304, 'learning_rate': 1.9837191123426777e-05, 'epoch': 0.09}
  9%|â–Š         | 446/5198 [1:34:33<16:41:55, 12.65s/it]  9%|â–Š         | 447/5198 [1:34:45<16:27:38, 12.47s/it]                                                       {'loss': 0.946, 'learning_rate': 1.983606945081042e-05, 'epoch': 0.09}
  9%|â–Š         | 447/5198 [1:34:45<16:27:38, 12.47s/it]  9%|â–Š         | 448/5198 [1:34:58<16:24:58, 12.44s/it]                                                       {'loss': 0.9306, 'learning_rate': 1.983494395949323e-05, 'epoch': 0.09}
  9%|â–Š         | 448/5198 [1:34:58<16:24:58, 12.44s/it]  9%|â–Š         | 449/5198 [1:35:10<16:34:33, 12.57s/it]                                                       {'loss': 0.9329, 'learning_rate': 1.983381464991217e-05, 'epoch': 0.09}
  9%|â–Š         | 449/5198 [1:35:10<16:34:33, 12.57s/it]  9%|â–Š         | 450/5198 [1:35:23<16:26:13, 12.46s/it]                                                       {'loss': 0.865, 'learning_rate': 1.9832681522505676e-05, 'epoch': 0.09}
  9%|â–Š         | 450/5198 [1:35:23<16:26:13, 12.46s/it]  9%|â–Š         | 451/5198 [1:35:34<16:00:46, 12.14s/it]                                                       {'loss': 0.9816, 'learning_rate': 1.9831544577713663e-05, 'epoch': 0.09}
  9%|â–Š         | 451/5198 [1:35:34<16:00:46, 12.14s/it]  9%|â–Š         | 452/5198 [1:35:46<16:01:53, 12.16s/it]                                                       {'loss': 0.9471, 'learning_rate': 1.983040381597754e-05, 'epoch': 0.09}
  9%|â–Š         | 452/5198 [1:35:46<16:01:53, 12.16s/it]  9%|â–Š         | 453/5198 [1:35:58<15:50:24, 12.02s/it]                                                       {'loss': 0.8631, 'learning_rate': 1.982925923774018e-05, 'epoch': 0.09}
  9%|â–Š         | 453/5198 [1:35:58<15:50:24, 12.02s/it]  9%|â–Š         | 454/5198 [1:36:10<15:44:53, 11.95s/it]                                                       {'loss': 0.8883, 'learning_rate': 1.9828110843445954e-05, 'epoch': 0.09}
  9%|â–Š         | 454/5198 [1:36:10<15:44:53, 11.95s/it]  9%|â–‰         | 455/5198 [1:36:22<15:44:30, 11.95s/it]                                                       {'loss': 0.909, 'learning_rate': 1.982695863354071e-05, 'epoch': 0.09}
  9%|â–‰         | 455/5198 [1:36:22<15:44:30, 11.95s/it]  9%|â–‰         | 456/5198 [1:36:34<15:55:08, 12.09s/it]                                                       {'loss': 0.8891, 'learning_rate': 1.9825802608471767e-05, 'epoch': 0.09}
  9%|â–‰         | 456/5198 [1:36:34<15:55:08, 12.09s/it]  9%|â–‰         | 457/5198 [1:36:46<15:53:00, 12.06s/it]                                                       {'loss': 0.9588, 'learning_rate': 1.982464276868794e-05, 'epoch': 0.09}
  9%|â–‰         | 457/5198 [1:36:46<15:53:00, 12.06s/it]  9%|â–‰         | 458/5198 [1:36:59<16:23:07, 12.44s/it]                                                       {'loss': 0.9441, 'learning_rate': 1.982347911463952e-05, 'epoch': 0.09}
  9%|â–‰         | 458/5198 [1:36:59<16:23:07, 12.44s/it]  9%|â–‰         | 459/5198 [1:37:13<16:48:42, 12.77s/it]                                                       {'loss': 0.9098, 'learning_rate': 1.9822311646778277e-05, 'epoch': 0.09}
  9%|â–‰         | 459/5198 [1:37:13<16:48:42, 12.77s/it]  9%|â–‰         | 460/5198 [1:37:26<16:53:09, 12.83s/it]                                                       {'loss': 0.9122, 'learning_rate': 1.982114036555746e-05, 'epoch': 0.09}
  9%|â–‰         | 460/5198 [1:37:26<16:53:09, 12.83s/it]  9%|â–‰         | 461/5198 [1:37:38<16:34:18, 12.59s/it]                                                       {'loss': 0.8874, 'learning_rate': 1.9819965271431797e-05, 'epoch': 0.09}
  9%|â–‰         | 461/5198 [1:37:38<16:34:18, 12.59s/it]WARNING: tokenization mismatch: 1 vs. 70. (ignored)
  9%|â–‰         | 462/5198 [1:37:49<16:09:29, 12.28s/it]                                                       {'loss': 0.9192, 'learning_rate': 1.9818786364857506e-05, 'epoch': 0.09}
  9%|â–‰         | 462/5198 [1:37:50<16:09:29, 12.28s/it]  9%|â–‰         | 463/5198 [1:38:02<16:20:15, 12.42s/it]                                                       {'loss': 0.9343, 'learning_rate': 1.9817603646292278e-05, 'epoch': 0.09}
  9%|â–‰         | 463/5198 [1:38:02<16:20:15, 12.42s/it]  9%|â–‰         | 464/5198 [1:38:16<16:49:40, 12.80s/it]                                                       {'loss': 0.9004, 'learning_rate': 1.9816417116195287e-05, 'epoch': 0.09}
  9%|â–‰         | 464/5198 [1:38:16<16:49:40, 12.80s/it]  9%|â–‰         | 465/5198 [1:38:31<17:36:58, 13.40s/it]                                                       {'loss': 0.9272, 'learning_rate': 1.9815226775027182e-05, 'epoch': 0.09}
  9%|â–‰         | 465/5198 [1:38:31<17:36:58, 13.40s/it]  9%|â–‰         | 466/5198 [1:38:43<16:59:29, 12.93s/it]                                                       {'loss': 0.9878, 'learning_rate': 1.9814032623250093e-05, 'epoch': 0.09}
  9%|â–‰         | 466/5198 [1:38:43<16:59:29, 12.93s/it]  9%|â–‰         | 467/5198 [1:38:57<17:30:24, 13.32s/it]                                                       {'loss': 0.9002, 'learning_rate': 1.9812834661327632e-05, 'epoch': 0.09}
  9%|â–‰         | 467/5198 [1:38:57<17:30:24, 13.32s/it]  9%|â–‰         | 468/5198 [1:39:09<16:55:05, 12.88s/it]                                                       {'loss': 0.8682, 'learning_rate': 1.9811632889724888e-05, 'epoch': 0.09}
  9%|â–‰         | 468/5198 [1:39:09<16:55:05, 12.88s/it]  9%|â–‰         | 469/5198 [1:39:22<17:18:04, 13.17s/it]                                                       {'loss': 0.8455, 'learning_rate': 1.9810427308908437e-05, 'epoch': 0.09}
  9%|â–‰         | 469/5198 [1:39:23<17:18:04, 13.17s/it]  9%|â–‰         | 470/5198 [1:39:34<16:47:52, 12.79s/it]                                                       {'loss': 0.8479, 'learning_rate': 1.9809217919346318e-05, 'epoch': 0.09}
  9%|â–‰         | 470/5198 [1:39:34<16:47:52, 12.79s/it]  9%|â–‰         | 471/5198 [1:39:46<16:26:42, 12.52s/it]                                                       {'loss': 0.9833, 'learning_rate': 1.980800472150806e-05, 'epoch': 0.09}
  9%|â–‰         | 471/5198 [1:39:46<16:26:42, 12.52s/it]  9%|â–‰         | 472/5198 [1:39:58<15:59:34, 12.18s/it]                                                       {'loss': 0.9233, 'learning_rate': 1.9806787715864674e-05, 'epoch': 0.09}
  9%|â–‰         | 472/5198 [1:39:58<15:59:34, 12.18s/it]  9%|â–‰         | 473/5198 [1:40:09<15:42:34, 11.97s/it]                                                       {'loss': 0.9618, 'learning_rate': 1.9805566902888637e-05, 'epoch': 0.09}
  9%|â–‰         | 473/5198 [1:40:09<15:42:34, 11.97s/it]  9%|â–‰         | 474/5198 [1:40:21<15:48:55, 12.05s/it]                                                       {'loss': 0.9409, 'learning_rate': 1.9804342283053916e-05, 'epoch': 0.09}
  9%|â–‰         | 474/5198 [1:40:21<15:48:55, 12.05s/it]  9%|â–‰         | 475/5198 [1:40:33<15:43:09, 11.98s/it]                                                       {'loss': 0.9872, 'learning_rate': 1.980311385683594e-05, 'epoch': 0.09}
  9%|â–‰         | 475/5198 [1:40:33<15:43:09, 11.98s/it]  9%|â–‰         | 476/5198 [1:40:47<16:17:10, 12.42s/it]                                                       {'loss': 0.9394, 'learning_rate': 1.980188162471164e-05, 'epoch': 0.09}
  9%|â–‰         | 476/5198 [1:40:47<16:17:10, 12.42s/it]  9%|â–‰         | 477/5198 [1:40:58<15:54:16, 12.13s/it]                                                       {'loss': 0.9264, 'learning_rate': 1.98006455871594e-05, 'epoch': 0.09}
  9%|â–‰         | 477/5198 [1:40:58<15:54:16, 12.13s/it]  9%|â–‰         | 478/5198 [1:41:10<15:55:35, 12.15s/it]                                                       {'loss': 0.8727, 'learning_rate': 1.97994057446591e-05, 'epoch': 0.09}
  9%|â–‰         | 478/5198 [1:41:10<15:55:35, 12.15s/it]  9%|â–‰         | 479/5198 [1:41:23<16:00:57, 12.22s/it]                                                       {'loss': 0.9152, 'learning_rate': 1.979816209769209e-05, 'epoch': 0.09}
  9%|â–‰         | 479/5198 [1:41:23<16:00:57, 12.22s/it]  9%|â–‰         | 480/5198 [1:41:35<15:53:05, 12.12s/it]                                                       {'loss': 0.8649, 'learning_rate': 1.9796914646741187e-05, 'epoch': 0.09}
  9%|â–‰         | 480/5198 [1:41:35<15:53:05, 12.12s/it]  9%|â–‰         | 481/5198 [1:41:46<15:36:19, 11.91s/it]                                                       {'loss': 0.9098, 'learning_rate': 1.9795663392290702e-05, 'epoch': 0.09}
  9%|â–‰         | 481/5198 [1:41:46<15:36:19, 11.91s/it]  9%|â–‰         | 482/5198 [1:41:58<15:41:41, 11.98s/it]                                                       {'loss': 0.8576, 'learning_rate': 1.9794408334826415e-05, 'epoch': 0.09}
  9%|â–‰         | 482/5198 [1:41:58<15:41:41, 11.98s/it]  9%|â–‰         | 483/5198 [1:42:10<15:36:51, 11.92s/it]                                                       {'loss': 0.9252, 'learning_rate': 1.979314947483558e-05, 'epoch': 0.09}
  9%|â–‰         | 483/5198 [1:42:10<15:36:51, 11.92s/it]  9%|â–‰         | 484/5198 [1:42:22<15:29:29, 11.83s/it]                                                       {'loss': 0.9709, 'learning_rate': 1.9791886812806932e-05, 'epoch': 0.09}
  9%|â–‰         | 484/5198 [1:42:22<15:29:29, 11.83s/it]  9%|â–‰         | 485/5198 [1:42:33<15:22:41, 11.75s/it]                                                       {'loss': 0.8779, 'learning_rate': 1.9790620349230676e-05, 'epoch': 0.09}
  9%|â–‰         | 485/5198 [1:42:33<15:22:41, 11.75s/it]  9%|â–‰         | 486/5198 [1:42:46<15:49:16, 12.09s/it]                                                       {'loss': 0.9237, 'learning_rate': 1.9789350084598504e-05, 'epoch': 0.09}
  9%|â–‰         | 486/5198 [1:42:46<15:49:16, 12.09s/it]  9%|â–‰         | 487/5198 [1:42:57<15:24:35, 11.78s/it]                                                       {'loss': 0.8687, 'learning_rate': 1.9788076019403565e-05, 'epoch': 0.09}
  9%|â–‰         | 487/5198 [1:42:57<15:24:35, 11.78s/it]  9%|â–‰         | 488/5198 [1:43:09<15:26:06, 11.80s/it]                                                       {'loss': 0.9582, 'learning_rate': 1.9786798154140507e-05, 'epoch': 0.09}
  9%|â–‰         | 488/5198 [1:43:09<15:26:06, 11.80s/it]  9%|â–‰         | 489/5198 [1:43:26<17:20:27, 13.26s/it]                                                       {'loss': 0.3709, 'learning_rate': 1.9785516489305437e-05, 'epoch': 0.09}
  9%|â–‰         | 489/5198 [1:43:26<17:20:27, 13.26s/it]  9%|â–‰         | 490/5198 [1:43:38<16:50:34, 12.88s/it]                                                       {'loss': 0.9286, 'learning_rate': 1.9784231025395936e-05, 'epoch': 0.09}
  9%|â–‰         | 490/5198 [1:43:38<16:50:34, 12.88s/it]  9%|â–‰         | 491/5198 [1:43:50<16:44:24, 12.80s/it]                                                       {'loss': 0.9314, 'learning_rate': 1.9782941762911075e-05, 'epoch': 0.09}
  9%|â–‰         | 491/5198 [1:43:50<16:44:24, 12.80s/it]  9%|â–‰         | 492/5198 [1:44:03<16:41:24, 12.77s/it]                                                       {'loss': 0.938, 'learning_rate': 1.9781648702351383e-05, 'epoch': 0.09}
  9%|â–‰         | 492/5198 [1:44:03<16:41:24, 12.77s/it]  9%|â–‰         | 493/5198 [1:44:17<17:21:12, 13.28s/it]                                                       {'loss': 0.9388, 'learning_rate': 1.9780351844218874e-05, 'epoch': 0.09}
  9%|â–‰         | 493/5198 [1:44:17<17:21:12, 13.28s/it] 10%|â–‰         | 494/5198 [1:44:30<16:56:26, 12.96s/it]                                                       {'loss': 0.9058, 'learning_rate': 1.977905118901703e-05, 'epoch': 0.1}
 10%|â–‰         | 494/5198 [1:44:30<16:56:26, 12.96s/it] 10%|â–‰         | 495/5198 [1:44:41<16:28:04, 12.61s/it]                                                       {'loss': 0.888, 'learning_rate': 1.977774673725081e-05, 'epoch': 0.1}
 10%|â–‰         | 495/5198 [1:44:41<16:28:04, 12.61s/it] 10%|â–‰         | 496/5198 [1:44:59<18:26:34, 14.12s/it]                                                       {'loss': 0.3337, 'learning_rate': 1.977643848942665e-05, 'epoch': 0.1}
 10%|â–‰         | 496/5198 [1:44:59<18:26:34, 14.12s/it] 10%|â–‰         | 497/5198 [1:45:11<17:47:21, 13.62s/it]                                                       {'loss': 0.8936, 'learning_rate': 1.977512644605246e-05, 'epoch': 0.1}
 10%|â–‰         | 497/5198 [1:45:12<17:47:21, 13.62s/it] 10%|â–‰         | 498/5198 [1:45:23<17:05:01, 13.09s/it]                                                       {'loss': 0.8827, 'learning_rate': 1.9773810607637612e-05, 'epoch': 0.1}
 10%|â–‰         | 498/5198 [1:45:23<17:05:01, 13.09s/it] 10%|â–‰         | 499/5198 [1:45:35<16:32:35, 12.67s/it]                                                       {'loss': 0.9245, 'learning_rate': 1.9772490974692962e-05, 'epoch': 0.1}
 10%|â–‰         | 499/5198 [1:45:35<16:32:35, 12.67s/it] 10%|â–‰         | 500/5198 [1:45:47<16:22:35, 12.55s/it]                                                       {'loss': 0.9533, 'learning_rate': 1.9771167547730844e-05, 'epoch': 0.1}
 10%|â–‰         | 500/5198 [1:45:47<16:22:35, 12.55s/it] 10%|â–‰         | 501/5198 [1:45:59<16:13:53, 12.44s/it]                                                       {'loss': 0.9578, 'learning_rate': 1.976984032726505e-05, 'epoch': 0.1}
 10%|â–‰         | 501/5198 [1:45:59<16:13:53, 12.44s/it] 10%|â–‰         | 502/5198 [1:46:11<15:59:41, 12.26s/it]                                                       {'loss': 0.9514, 'learning_rate': 1.976850931381086e-05, 'epoch': 0.1}
 10%|â–‰         | 502/5198 [1:46:11<15:59:41, 12.26s/it] 10%|â–‰         | 503/5198 [1:46:22<15:29:00, 11.87s/it]                                                       {'loss': 0.9026, 'learning_rate': 1.976717450788501e-05, 'epoch': 0.1}
 10%|â–‰         | 503/5198 [1:46:22<15:29:00, 11.87s/it] 10%|â–‰         | 504/5198 [1:46:34<15:31:17, 11.90s/it]                                                       {'loss': 0.8795, 'learning_rate': 1.9765835910005726e-05, 'epoch': 0.1}
 10%|â–‰         | 504/5198 [1:46:34<15:31:17, 11.90s/it] 10%|â–‰         | 505/5198 [1:46:47<15:55:39, 12.22s/it]                                                       {'loss': 0.876, 'learning_rate': 1.9764493520692685e-05, 'epoch': 0.1}
 10%|â–‰         | 505/5198 [1:46:47<15:55:39, 12.22s/it] 10%|â–‰         | 506/5198 [1:46:59<15:41:29, 12.04s/it]                                                       {'loss': 0.9991, 'learning_rate': 1.9763147340467067e-05, 'epoch': 0.1}
 10%|â–‰         | 506/5198 [1:46:59<15:41:29, 12.04s/it] 10%|â–‰         | 507/5198 [1:47:10<15:30:15, 11.90s/it]                                                       {'loss': 0.873, 'learning_rate': 1.9761797369851498e-05, 'epoch': 0.1}
 10%|â–‰         | 507/5198 [1:47:10<15:30:15, 11.90s/it] 10%|â–‰         | 508/5198 [1:47:23<15:59:24, 12.27s/it]                                                       {'loss': 0.8956, 'learning_rate': 1.9760443609370074e-05, 'epoch': 0.1}
 10%|â–‰         | 508/5198 [1:47:24<15:59:24, 12.27s/it] 10%|â–‰         | 509/5198 [1:47:35<15:52:59, 12.19s/it]                                                       {'loss': 0.9119, 'learning_rate': 1.975908605954838e-05, 'epoch': 0.1}
 10%|â–‰         | 509/5198 [1:47:36<15:52:59, 12.19s/it] 10%|â–‰         | 510/5198 [1:47:48<15:56:47, 12.25s/it]                                                       {'loss': 0.9454, 'learning_rate': 1.9757724720913466e-05, 'epoch': 0.1}
 10%|â–‰         | 510/5198 [1:47:48<15:56:47, 12.25s/it] 10%|â–‰         | 511/5198 [1:47:59<15:41:31, 12.05s/it]                                                       {'loss': 0.9151, 'learning_rate': 1.9756359593993845e-05, 'epoch': 0.1}
 10%|â–‰         | 511/5198 [1:48:00<15:41:31, 12.05s/it] 10%|â–‰         | 512/5198 [1:48:16<17:19:44, 13.31s/it]                                                       {'loss': 0.3012, 'learning_rate': 1.975499067931951e-05, 'epoch': 0.1}
 10%|â–‰         | 512/5198 [1:48:16<17:19:44, 13.31s/it] 10%|â–‰         | 513/5198 [1:48:27<16:28:09, 12.66s/it]                                                       {'loss': 0.959, 'learning_rate': 1.975361797742192e-05, 'epoch': 0.1}
 10%|â–‰         | 513/5198 [1:48:27<16:28:09, 12.66s/it] 10%|â–‰         | 514/5198 [1:48:40<16:45:18, 12.88s/it]                                                       {'loss': 0.9331, 'learning_rate': 1.9752241488834002e-05, 'epoch': 0.1}
 10%|â–‰         | 514/5198 [1:48:40<16:45:18, 12.88s/it] 10%|â–‰         | 515/5198 [1:48:52<16:23:53, 12.61s/it]                                                       {'loss': 0.9247, 'learning_rate': 1.975086121409016e-05, 'epoch': 0.1}
 10%|â–‰         | 515/5198 [1:48:52<16:23:53, 12.61s/it] 10%|â–‰         | 516/5198 [1:49:06<16:43:52, 12.86s/it]                                                       {'loss': 0.8741, 'learning_rate': 1.974947715372626e-05, 'epoch': 0.1}
 10%|â–‰         | 516/5198 [1:49:06<16:43:52, 12.86s/it] 10%|â–‰         | 517/5198 [1:49:19<16:55:05, 13.01s/it]                                                       {'loss': 0.8651, 'learning_rate': 1.974808930827965e-05, 'epoch': 0.1}
 10%|â–‰         | 517/5198 [1:49:19<16:55:05, 13.01s/it] 10%|â–‰         | 518/5198 [1:49:31<16:22:21, 12.59s/it]                                                       {'loss': 0.9011, 'learning_rate': 1.9746697678289128e-05, 'epoch': 0.1}
 10%|â–‰         | 518/5198 [1:49:31<16:22:21, 12.59s/it] 10%|â–‰         | 519/5198 [1:49:43<16:13:09, 12.48s/it]                                                       {'loss': 0.9128, 'learning_rate': 1.9745302264294982e-05, 'epoch': 0.1}
 10%|â–‰         | 519/5198 [1:49:43<16:13:09, 12.48s/it] 10%|â–ˆ         | 520/5198 [1:49:55<15:56:47, 12.27s/it]                                                       {'loss': 0.9175, 'learning_rate': 1.9743903066838954e-05, 'epoch': 0.1}
 10%|â–ˆ         | 520/5198 [1:49:55<15:56:47, 12.27s/it] 10%|â–ˆ         | 521/5198 [1:50:07<16:00:08, 12.32s/it]                                                       {'loss': 0.9374, 'learning_rate': 1.9742500086464266e-05, 'epoch': 0.1}
 10%|â–ˆ         | 521/5198 [1:50:07<16:00:08, 12.32s/it] 10%|â–ˆ         | 522/5198 [1:50:23<17:29:51, 13.47s/it]                                                       {'loss': 0.3244, 'learning_rate': 1.9741093323715597e-05, 'epoch': 0.1}
 10%|â–ˆ         | 522/5198 [1:50:23<17:29:51, 13.47s/it] 10%|â–ˆ         | 523/5198 [1:50:35<16:50:28, 12.97s/it]                                                       {'loss': 0.9386, 'learning_rate': 1.9739682779139107e-05, 'epoch': 0.1}
 10%|â–ˆ         | 523/5198 [1:50:35<16:50:28, 12.97s/it] 10%|â–ˆ         | 524/5198 [1:50:47<16:35:16, 12.78s/it]                                                       {'loss': 0.9106, 'learning_rate': 1.9738268453282414e-05, 'epoch': 0.1}
 10%|â–ˆ         | 524/5198 [1:50:47<16:35:16, 12.78s/it] 10%|â–ˆ         | 525/5198 [1:50:59<16:05:02, 12.39s/it]                                                       {'loss': 0.934, 'learning_rate': 1.9736850346694608e-05, 'epoch': 0.1}
 10%|â–ˆ         | 525/5198 [1:50:59<16:05:02, 12.39s/it] 10%|â–ˆ         | 526/5198 [1:51:11<15:49:03, 12.19s/it]                                                       {'loss': 0.9475, 'learning_rate': 1.973542845992625e-05, 'epoch': 0.1}
 10%|â–ˆ         | 526/5198 [1:51:11<15:49:03, 12.19s/it] 10%|â–ˆ         | 527/5198 [1:51:23<15:47:26, 12.17s/it]                                                       {'loss': 0.8821, 'learning_rate': 1.9734002793529362e-05, 'epoch': 0.1}
 10%|â–ˆ         | 527/5198 [1:51:23<15:47:26, 12.17s/it] 10%|â–ˆ         | 528/5198 [1:51:37<16:47:29, 12.94s/it]                                                       {'loss': 0.8888, 'learning_rate': 1.9732573348057437e-05, 'epoch': 0.1}
 10%|â–ˆ         | 528/5198 [1:51:37<16:47:29, 12.94s/it] 10%|â–ˆ         | 529/5198 [1:51:49<16:21:04, 12.61s/it]                                                       {'loss': 0.9651, 'learning_rate': 1.973114012406544e-05, 'epoch': 0.1}
 10%|â–ˆ         | 529/5198 [1:51:49<16:21:04, 12.61s/it] 10%|â–ˆ         | 530/5198 [1:52:02<16:17:15, 12.56s/it]                                                       {'loss': 0.9137, 'learning_rate': 1.9729703122109788e-05, 'epoch': 0.1}
 10%|â–ˆ         | 530/5198 [1:52:02<16:17:15, 12.56s/it] 10%|â–ˆ         | 531/5198 [1:52:13<15:52:25, 12.24s/it]                                                       {'loss': 0.8637, 'learning_rate': 1.9728262342748384e-05, 'epoch': 0.1}
 10%|â–ˆ         | 531/5198 [1:52:13<15:52:25, 12.24s/it] 10%|â–ˆ         | 532/5198 [1:52:25<15:32:22, 11.99s/it]                                                       {'loss': 0.9281, 'learning_rate': 1.9726817786540584e-05, 'epoch': 0.1}
 10%|â–ˆ         | 532/5198 [1:52:25<15:32:22, 11.99s/it] 10%|â–ˆ         | 533/5198 [1:52:37<15:30:14, 11.96s/it]                                                       {'loss': 0.9315, 'learning_rate': 1.9725369454047215e-05, 'epoch': 0.1}
 10%|â–ˆ         | 533/5198 [1:52:37<15:30:14, 11.96s/it] 10%|â–ˆ         | 534/5198 [1:52:49<15:32:58, 12.00s/it]                                                       {'loss': 0.9606, 'learning_rate': 1.9723917345830568e-05, 'epoch': 0.1}
 10%|â–ˆ         | 534/5198 [1:52:49<15:32:58, 12.00s/it] 10%|â–ˆ         | 535/5198 [1:53:02<16:11:04, 12.49s/it]                                                       {'loss': 0.898, 'learning_rate': 1.9722461462454405e-05, 'epoch': 0.1}
 10%|â–ˆ         | 535/5198 [1:53:02<16:11:04, 12.49s/it] 10%|â–ˆ         | 536/5198 [1:53:15<16:14:11, 12.54s/it]                                                       {'loss': 0.8966, 'learning_rate': 1.9721001804483947e-05, 'epoch': 0.1}
 10%|â–ˆ         | 536/5198 [1:53:15<16:14:11, 12.54s/it] 10%|â–ˆ         | 537/5198 [1:53:27<16:03:03, 12.40s/it]                                                       {'loss': 0.9885, 'learning_rate': 1.9719538372485887e-05, 'epoch': 0.1}
 10%|â–ˆ         | 537/5198 [1:53:27<16:03:03, 12.40s/it] 10%|â–ˆ         | 538/5198 [1:53:39<15:52:08, 12.26s/it]                                                       {'loss': 0.8772, 'learning_rate': 1.9718071167028376e-05, 'epoch': 0.1}
 10%|â–ˆ         | 538/5198 [1:53:39<15:52:08, 12.26s/it] 10%|â–ˆ         | 539/5198 [1:53:50<15:32:30, 12.01s/it]                                                       {'loss': 0.9013, 'learning_rate': 1.9716600188681038e-05, 'epoch': 0.1}
 10%|â–ˆ         | 539/5198 [1:53:50<15:32:30, 12.01s/it] 10%|â–ˆ         | 540/5198 [1:54:04<16:06:20, 12.45s/it]                                                       {'loss': 0.9098, 'learning_rate': 1.971512543801495e-05, 'epoch': 0.1}
 10%|â–ˆ         | 540/5198 [1:54:04<16:06:20, 12.45s/it] 10%|â–ˆ         | 541/5198 [1:54:16<15:54:01, 12.29s/it]                                                       {'loss': 0.9317, 'learning_rate': 1.9713646915602663e-05, 'epoch': 0.1}
 10%|â–ˆ         | 541/5198 [1:54:16<15:54:01, 12.29s/it] 10%|â–ˆ         | 542/5198 [1:54:31<16:51:52, 13.04s/it]                                                       {'loss': 0.9137, 'learning_rate': 1.9712164622018197e-05, 'epoch': 0.1}
 10%|â–ˆ         | 542/5198 [1:54:31<16:51:52, 13.04s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (2316 > 2048). Running this sequence through the model will result in indexing errors
 10%|â–ˆ         | 543/5198 [1:54:43<16:34:37, 12.82s/it]                                                       {'loss': 0.8729, 'learning_rate': 1.9710678557837024e-05, 'epoch': 0.1}
 10%|â–ˆ         | 543/5198 [1:54:43<16:34:37, 12.82s/it] 10%|â–ˆ         | 544/5198 [1:54:54<15:58:43, 12.36s/it]                                                       {'loss': 0.9092, 'learning_rate': 1.9709188723636088e-05, 'epoch': 0.1}
 10%|â–ˆ         | 544/5198 [1:54:54<15:58:43, 12.36s/it] 10%|â–ˆ         | 545/5198 [1:55:07<16:21:06, 12.65s/it]                                                       {'loss': 0.9111, 'learning_rate': 1.970769511999379e-05, 'epoch': 0.1}
 10%|â–ˆ         | 545/5198 [1:55:08<16:21:06, 12.65s/it] 11%|â–ˆ         | 546/5198 [1:55:19<16:02:31, 12.41s/it]                                                       {'loss': 0.8986, 'learning_rate': 1.9706197747490004e-05, 'epoch': 0.11}
 11%|â–ˆ         | 546/5198 [1:55:19<16:02:31, 12.41s/it] 11%|â–ˆ         | 547/5198 [1:55:33<16:21:15, 12.66s/it]                                                       {'loss': 0.8986, 'learning_rate': 1.9704696606706055e-05, 'epoch': 0.11}
 11%|â–ˆ         | 547/5198 [1:55:33<16:21:15, 12.66s/it] 11%|â–ˆ         | 548/5198 [1:55:47<17:06:47, 13.25s/it]                                                       {'loss': 0.9389, 'learning_rate': 1.9703191698224742e-05, 'epoch': 0.11}
 11%|â–ˆ         | 548/5198 [1:55:47<17:06:47, 13.25s/it] 11%|â–ˆ         | 549/5198 [1:55:59<16:39:25, 12.90s/it]                                                       {'loss': 0.9414, 'learning_rate': 1.9701683022630323e-05, 'epoch': 0.11}
 11%|â–ˆ         | 549/5198 [1:55:59<16:39:25, 12.90s/it] 11%|â–ˆ         | 550/5198 [1:56:11<16:02:39, 12.43s/it]                                                       {'loss': 0.9558, 'learning_rate': 1.9700170580508514e-05, 'epoch': 0.11}
 11%|â–ˆ         | 550/5198 [1:56:11<16:02:39, 12.43s/it] 11%|â–ˆ         | 551/5198 [1:56:22<15:48:31, 12.25s/it]                                                       {'loss': 0.9278, 'learning_rate': 1.9698654372446495e-05, 'epoch': 0.11}
 11%|â–ˆ         | 551/5198 [1:56:22<15:48:31, 12.25s/it] 11%|â–ˆ         | 552/5198 [1:56:37<16:49:56, 13.04s/it]                                                       {'loss': 0.9141, 'learning_rate': 1.969713439903292e-05, 'epoch': 0.11}
 11%|â–ˆ         | 552/5198 [1:56:37<16:49:56, 13.04s/it] 11%|â–ˆ         | 553/5198 [1:56:49<16:26:16, 12.74s/it]                                                       {'loss': 0.9025, 'learning_rate': 1.9695610660857886e-05, 'epoch': 0.11}
 11%|â–ˆ         | 553/5198 [1:56:49<16:26:16, 12.74s/it] 11%|â–ˆ         | 554/5198 [1:57:01<16:04:08, 12.46s/it]                                                       {'loss': 0.9513, 'learning_rate': 1.9694083158512965e-05, 'epoch': 0.11}
 11%|â–ˆ         | 554/5198 [1:57:01<16:04:08, 12.46s/it] 11%|â–ˆ         | 555/5198 [1:57:14<16:14:00, 12.59s/it]                                                       {'loss': 0.9548, 'learning_rate': 1.9692551892591185e-05, 'epoch': 0.11}
 11%|â–ˆ         | 555/5198 [1:57:14<16:14:00, 12.59s/it] 11%|â–ˆ         | 556/5198 [1:57:26<16:06:44, 12.50s/it]                                                       {'loss': 0.9466, 'learning_rate': 1.9691016863687037e-05, 'epoch': 0.11}
 11%|â–ˆ         | 556/5198 [1:57:26<16:06:44, 12.50s/it] 11%|â–ˆ         | 557/5198 [1:57:39<16:01:27, 12.43s/it]                                                       {'loss': 0.8725, 'learning_rate': 1.968947807239647e-05, 'epoch': 0.11}
 11%|â–ˆ         | 557/5198 [1:57:39<16:01:27, 12.43s/it] 11%|â–ˆ         | 558/5198 [1:57:50<15:43:54, 12.21s/it]                                                       {'loss': 0.8873, 'learning_rate': 1.9687935519316897e-05, 'epoch': 0.11}
 11%|â–ˆ         | 558/5198 [1:57:50<15:43:54, 12.21s/it] 11%|â–ˆ         | 559/5198 [1:58:03<16:01:49, 12.44s/it]                                                       {'loss': 0.8965, 'learning_rate': 1.9686389205047186e-05, 'epoch': 0.11}
 11%|â–ˆ         | 559/5198 [1:58:03<16:01:49, 12.44s/it] 11%|â–ˆ         | 560/5198 [1:58:15<15:48:07, 12.27s/it]                                                       {'loss': 0.9403, 'learning_rate': 1.9684839130187678e-05, 'epoch': 0.11}
 11%|â–ˆ         | 560/5198 [1:58:15<15:48:07, 12.27s/it]